This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*, .cursorrules, .cursor/rules/*, .clinerules, CLAUDE.md
- Files matching these patterns are excluded: .*.*, **/*.pbxproj, **/node_modules/**, **/dist/**, **/build/**, **/compile/**, **/*.spec.*, **/*.pyc, **/.env, **/.env.*, **/*.env, **/*.env.*, **/*.lock, **/*.lockb, **/package-lock.*, **/pnpm-lock.*, **/*.tsbuildinfo, **/certdata.txt
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
cache/
frontend/src/components/common/
frontend/src/components/layout/
frontend/src/components/ui/
frontend/src/lib/
.gitignore
api/__init__.py
api/main.py
api/models/__init__.py
api/models/requests.py
api/models/responses.py
api/routers/__init__.py
api/routers/competitors.py
api/routers/dashboard.py
api/routers/listings.py
api/routers/niches.py
api/routers/stress.py
api/routers/trends.py
config/logging.conf
config/settings.py
example_usage.md
frontend/package.json
frontend/public/index.html
frontend/src/App.css
frontend/src/App.js
frontend/src/index.js
frontend/src/pages/Dashboard.jsx
frontend/src/reportWebVitals.js
frontend/src/services/api.js
frontend/src/utils/chartUtils.js
frontend/tailwind.config.js
kdp-strategist.mcp.json
README.md
requirements.txt
run_server.py
setup.py
src/kdp_strategist/__init__.py
src/kdp_strategist/agent/__init__.py
src/kdp_strategist/agent/kdp_strategist_agent.py
src/kdp_strategist/agent/tools/__init__.py
src/kdp_strategist/agent/tools/competitor_analysis.py
src/kdp_strategist/agent/tools/listing_generation.py
src/kdp_strategist/agent/tools/niche_discovery.py
src/kdp_strategist/agent/tools/stress_testing.py
src/kdp_strategist/agent/tools/trend_validation.py
src/kdp_strategist/data/__init__.py
src/kdp_strategist/data/cache_manager.py
src/kdp_strategist/data/keepa_client.py
src/kdp_strategist/data/trends_client.py
src/kdp_strategist/main.py
src/kdp_strategist/models/__init__.py
src/kdp_strategist/models/listing_model.py
src/kdp_strategist/models/niche_model.py
src/kdp_strategist/models/trend_model.py
start_frontend.py
start_kdp_strategist.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="example_usage.md">
#!/usr/bin/env python3
"""Basic Usage Example for KDP Strategist AI Agent.

This example demonstrates how to use the KDP Strategist AI Agent
programmatically to discover profitable niches, analyze competitors,
generate listings, validate trends, and perform stress testing.

Usage:
    python examples/basic_usage.py
"""

import asyncio
import logging
import sys
from pathlib import Path

# Add the src directory to the Python path
project_root = Path(__file__).parent.parent
src_dir = project_root / "src"
sys.path.insert(0, str(src_dir))

from config.settings import load_settings
from agent.kdp_strategist_agent import KDPStrategistAgent

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def demonstrate_niche_discovery(agent: KDPStrategistAgent):
    """Demonstrate niche discovery functionality."""
    print("\n" + "="*60)
    print("üîç NICHE DISCOVERY DEMONSTRATION")
    print("="*60)
    
    try:
        # Example: Find profitable niches in the cooking space
        result = await agent.discover_niches("find_profitable_niches", {
            "base_keywords": ["healthy cooking", "meal prep", "quick recipes"],
            "max_niches": 3,
            "min_profitability": 60,
            "include_seasonal": True
        })
        
        if "error" in result:
            print(f"‚ùå Error: {result['error']}")
            return
        
        discovered_niches = result.get("discovered_niches", {})
        analysis_metadata = result.get("analysis_metadata", {})
        
        print(f"‚úÖ Found {len(discovered_niches)} profitable niches")
        print(f"üìä Analysis completed in {analysis_metadata.get('processing_time_seconds', 'N/A')} seconds")
        
        for i, (keyword, niche_data) in enumerate(discovered_niches.items(), 1):
            print(f"\n{i}. {keyword.title()}")
            print(f"   Overall Score: {niche_data.get('overall_score', 'N/A')}/100")
            print(f"   Profitability: {niche_data.get('profitability_score', 'N/A')}/100")
            print(f"   Competition: {niche_data.get('competition_score', 'N/A')}/100")
            print(f"   Market Size: {niche_data.get('market_size_score', 'N/A')}/100")
            
            related_keywords = niche_data.get('related_keywords', [])
            if related_keywords:
                print(f"   Related Keywords: {', '.join(related_keywords[:3])}")
    
    except Exception as e:
        logger.error(f"Error in niche discovery demonstration: {e}")
        print(f"‚ùå Error: {e}")


async def demonstrate_competitor_analysis(agent: KDPStrategistAgent):
    """Demonstrate competitor analysis functionality."""
    print("\n" + "="*60)
    print("üìä COMPETITOR ANALYSIS DEMONSTRATION")
    print("="*60)
    
    try:
        # Example: Analyze a competitor ASIN (using a sample ASIN)
        result = await agent.analyze_competitors("analyze_competitor_asin", {
            "asin": "B08EXAMPLE123",  # Sample ASIN
            "include_market_analysis": True,
            "analyze_pricing_history": True
        })
        
        if "error" in result:
            print(f"‚ùå Error: {result['error']}")
            return
        
        competitor_metrics = result.get("competitor_metrics", {})
        market_analysis = result.get("market_analysis", {})
        
        print("‚úÖ Competitor Analysis Completed")
        print(f"üìà Sales Rank: {competitor_metrics.get('sales_rank', 'N/A')}")
        print(f"üí∞ Current Price: ${competitor_metrics.get('current_price', 'N/A')}")
        print(f"‚≠ê Rating: {competitor_metrics.get('rating', 'N/A')}/5")
        print(f"üìù Review Count: {competitor_metrics.get('review_count', 'N/A')}")
        
        if market_analysis:
            print(f"\nüè™ Market Analysis:")
            print(f"   Total Competitors: {market_analysis.get('total_competitors', 'N/A')}")
            print(f"   Average Price: ${market_analysis.get('average_price', 'N/A')}")
            print(f"   Market Saturation: {market_analysis.get('saturation_level', 'N/A')}")
    
    except Exception as e:
        logger.error(f"Error in competitor analysis demonstration: {e}")
        print(f"‚ùå Error: {e}")


async def demonstrate_listing_generation(agent: KDPStrategistAgent):
    """Demonstrate KDP listing generation functionality."""
    print("\n" + "="*60)
    print("üìù LISTING GENERATION DEMONSTRATION")
    print("="*60)
    
    try:
        # Example: Generate a KDP listing for a cookbook
        result = await agent.generate_listing("generate_kdp_listing", {
            "niche_keyword": "healthy meal prep",
            "target_audience": "busy professionals and health-conscious individuals",
            "book_type": "cookbook",
            "include_keywords": True,
            "include_categories": True
        })
        
        if "error" in result:
            print(f"‚ùå Error: {result['error']}")
            return
        
        generated_listing = result.get("generated_listing", {})
        optimization_suggestions = result.get("optimization_suggestions", [])
        
        print("‚úÖ KDP Listing Generated Successfully")
        print(f"\nüìñ Title: {generated_listing.get('title', 'N/A')}")
        print(f"\nüìÑ Description:")
        description = generated_listing.get('description', 'N/A')
        print(f"   {description[:200]}{'...' if len(description) > 200 else ''}")
        
        keywords = generated_listing.get('keywords', [])
        if keywords:
            print(f"\nüîç Keywords: {', '.join(keywords[:5])}")
        
        categories = generated_listing.get('categories', [])
        if categories:
            print(f"\nüìÇ Categories: {', '.join(categories[:3])}")
        
        pricing = generated_listing.get('pricing_recommendation', {})
        if pricing:
            print(f"\nüí∞ Pricing Recommendation: ${pricing.get('recommended_price', 'N/A')}")
        
        if optimization_suggestions:
            print(f"\nüí° Optimization Suggestions:")
            for i, suggestion in enumerate(optimization_suggestions[:3], 1):
                print(f"   {i}. {suggestion}")
    
    except Exception as e:
        logger.error(f"Error in listing generation demonstration: {e}")
        print(f"‚ùå Error: {e}")


async def demonstrate_trend_validation(agent: KDPStrategistAgent):
    """Demonstrate trend validation functionality."""
    print("\n" + "="*60)
    print("üìà TREND VALIDATION DEMONSTRATION")
    print("="*60)
    
    try:
        # Example: Validate trend for "intermittent fasting"
        result = await agent.validate_trendsl("validate_trend", {
            "keyword": "intermittent fasting",
            "timeframe": "today 12-m",
            "include_forecasts": True,
            "include_seasonality": True
        })
        
        if "error" in result:
            print(f"‚ùå Error: {result['error']}")
            return
        
        validation_result = result.get("validation_result", {})
        trend_analysis = result.get("trend_analysis", {})
        forecasts = result.get("forecasts", [])
        seasonal_analysis = result.get("seasonal_analysis", {})
        
        print("‚úÖ Trend Validation Completed")
        print(f"\nüéØ Validation Result:")
        print(f"   Valid Trend: {'‚úÖ Yes' if validation_result.get('is_valid') else '‚ùå No'}")
        print(f"   Overall Score: {validation_result.get('overall_score', 'N/A')}/100")
        
        print(f"\nüìä Current Trend Analysis:")
        print(f"   Trend Score: {trend_analysis.get('current_score', 'N/A')}/100")
        print(f"   Direction: {trend_analysis.get('direction', 'N/A')}")
        print(f"   Strength: {trend_analysis.get('strength', 'N/A')}")
        print(f"   Confidence: {trend_analysis.get('confidence', 'N/A')}")
        
        if forecasts:
            print(f"\nüîÆ Forecasts:")
            for forecast in forecasts[:3]:
                timeframe = forecast.get('timeframe', 'N/A')
                predicted_score = forecast.get('predicted_score', 'N/A')
                direction = forecast.get('direction', 'N/A')
                print(f"   {timeframe}: {predicted_score}/100 ({direction})")
        
        if seasonal_analysis and seasonal_analysis.get('has_seasonality'):
            print(f"\nüåü Seasonality Detected:")
            peak_months = seasonal_analysis.get('peak_months', [])
            if peak_months:
                month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                             'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
                peak_names = [month_names[m-1] for m in peak_months if 1 <= m <= 12]
                print(f"   Peak Months: {', '.join(peak_names)}")
    
    except Exception as e:
        logger.error(f"Error in trend validation demonstration: {e}")
        print(f"‚ùå Error: {e}")


async def demonstrate_stress_testing(agent: KDPStrategistAgent):
    """Demonstrate stress testing functionality."""
    print("\n" + "="*60)
    print("üß™ STRESS TESTING DEMONSTRATION")
    print("="*60)
    
    try:
        # Example: Stress test the "keto recipes" niche
        result = await agent.stress_test_niche("niche_stress_test", {
            "niche_keyword": "keto recipes",
            "include_all_scenarios": True
        })
        
        if "error" in result:
            print(f"‚ùå Error: {result['error']}")
            return
        
        stress_test_summary = result.get("stress_test_summary", {})
        scenario_results = result.get("scenario_results", [])
        risk_analysis = result.get("risk_analysis", {})
        recommendations = result.get("recommendations", {})
        
        print("‚úÖ Stress Test Completed")
        print(f"\nüõ°Ô∏è Overall Resilience: {stress_test_summary.get('overall_resilience', 'N/A')}/100")
        print(f"‚ö†Ô∏è Risk Profile: {stress_test_summary.get('risk_profile', 'N/A')}")
        print(f"üß™ Scenarios Tested: {stress_test_summary.get('scenarios_tested', 'N/A')}")
        
        if scenario_results:
            print(f"\nüìã Top Risk Scenarios:")
            # Sort by impact percentage and show top 3
            sorted_scenarios = sorted(
                scenario_results, 
                key=lambda x: x.get('impact_percentage', 0), 
                reverse=True
            )
            
            for i, scenario in enumerate(sorted_scenarios[:3], 1):
                scenario_name = scenario.get('scenario', 'N/A').replace('_', ' ').title()
                impact = scenario.get('impact_percentage', 'N/A')
                survival = scenario.get('survival_probability', 'N/A')
                print(f"   {i}. {scenario_name}: {impact}% impact, {survival} survival rate")
        
        critical_vulnerabilities = risk_analysis.get('critical_vulnerabilities', [])
        if critical_vulnerabilities:
            print(f"\n‚ö†Ô∏è Critical Vulnerabilities:")
            for i, vulnerability in enumerate(critical_vulnerabilities[:3], 1):
                print(f"   {i}. {vulnerability}")
        
        immediate_actions = recommendations.get('immediate_actions', [])
        if immediate_actions:
            print(f"\nüöÄ Immediate Actions:")
            for i, action in enumerate(immediate_actions[:3], 1):
                print(f"   {i}. {action}")
    
    except Exception as e:
        logger.error(f"Error in stress testing demonstration: {e}")
        print(f"‚ùå Error: {e}")


async def main():
    """Main demonstration function."""
    print("üöÄ KDP Strategist AI Agent - Basic Usage Demonstration")
    print("This example shows how to use the KDP Strategist programmatically.")
    print("\nNote: This demonstration uses simulated data for API calls.")
    print("In production, ensure you have valid API keys configured.")
    
    try:
        # Load configuration
        settings = load_settings()
        
        # Initialize the agent
        print("\nüîß Initializing KDP Strategist Agent...")
        agent = KDPStrategistAgent(settings)
        await agent.initialize()
        
        print("‚úÖ Agent initialized successfully!")
        print(f"üõ†Ô∏è Available tools: {', '.join(await agent.list_tools())}")
        
        # Run demonstrations
        await demonstrate_niche_discovery(agent)
        await demonstrate_competitor_analysis(agent)
        await demonstrate_listing_generation(agent)
        await demonstrate_trend_validation(agent)
        await demonstrate_stress_testing(agent)
        
        print("\n" + "="*60)
        print("üéâ DEMONSTRATION COMPLETED SUCCESSFULLY!")
        print("="*60)
        print("\nüí° Next Steps:")
        print("   1. Configure your API keys in the .env file")
        print("   2. Run the agent in interactive mode: kdp_strategist --interactive")
        print("   3. Integrate the agent into your publishing workflow")
        print("   4. Explore advanced features and customization options")
        
        # Cleanup
        await agent.cleanup()
        
    except Exception as e:
        logger.error(f"Demonstration failed: {e}")
        print(f"\n‚ùå Demonstration failed: {e}")
        print("\nüîß Troubleshooting:")
        print("   1. Ensure all dependencies are installed: pip install -r requirements.txt")
        print("   2. Check your configuration in the .env file")
        print("   3. Verify your Python environment and version (3.8+)")
        print("   4. Check the logs for detailed error information")


if __name__ == "__main__":
    # Handle Windows event loop policy
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Demonstration interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        print(f"\nüí• Fatal error: {e}")
</file>

<file path=".gitignore">
# Node.js
node_modules
.env

# Python
virtualenv
virtualenvs
venv
__pycache__
*.pyc
*.egg-info/
.pytest_cache/
.mypy_cache/
.ipynb_checkpoints/

# Logs
*.log

# OS generated files
.DS_Store
thumbs.db

# Vibe Tools
.trae/

# PRPs
PRPs/
</file>

<file path="api/__init__.py">
"""KDP Strategist API package.
This package provides a FastAPI-based web API that interfaces with the existing
MCP agent to provide web UI functionality for the KDP Strategist tool.
"""
__version__ = "1.0.0"
</file>

<file path="api/main.py">
"""FastAPI application for KDP Strategist UI backend.
This module creates a FastAPI application that serves as a bridge between
the React frontend and the existing MCP agent backend.
"""
import asyncio
import logging
from contextlib import asynccontextmanager
from typing import Dict, Any
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse
import uvicorn
# Import existing MCP agent
from src.kdp_strategist.agent.kdp_strategist_agent import KDPStrategistAgent
# Import API routers
from .routers import niches, competitors, listings, trends, stress
# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        logger.info(f"WebSocket connected. Total connections: {len(self.active_connections)}")
    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        logger.info(f"WebSocket disconnected. Total connections: {len(self.active_connections)}")
    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)
    async def broadcast(self, message: str):
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except Exception as e:
                logger.error(f"Error broadcasting message: {e}")
                self.disconnect(connection)
manager = ConnectionManager()
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle - startup and shutdown events."""
    # Startup
    logger.info("Starting KDP Strategist API...")
    try:
        # Initialize MCP agent
        app.state.agent = await KDPStrategistAgent.create()
        logger.info("MCP Agent initialized successfully")
        yield
    except Exception as e:
        logger.error(f"Failed to initialize MCP agent: {e}")
        raise
    finally:
        # Shutdown
        logger.info("Shutting down KDP Strategist API...")
        if hasattr(app.state, 'agent'):
            try:
                await app.state.agent.cleanup()
                logger.info("MCP Agent cleaned up successfully")
            except Exception as e:
                logger.error(f"Error during agent cleanup: {e}")
# Create FastAPI application
app = FastAPI(
    title="KDP Strategist API",
    description="Web API for KDP Strategist AI Agent",
    version="1.0.0",
    lifespan=lifespan
)
# Configure CORS for React frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # React dev server
        "http://localhost:5173",  # Vite dev server
        "http://127.0.0.1:3000",
        "http://127.0.0.1:5173",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# Include API routers
app.include_router(niches.router, prefix="/api/niches", tags=["niches"])
app.include_router(competitors.router, prefix="/api/competitors", tags=["competitors"])
app.include_router(listings.router, prefix="/api/listings", tags=["listings"])
app.include_router(trends.router, prefix="/api/trends", tags=["trends"])
app.include_router(stress.router, prefix="/api/stress", tags=["stress"])
# Health check endpoint
@app.get("/api/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "service": "kdp_strategist-api"}
# WebSocket endpoint for real-time updates
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time updates."""
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # Echo back for now - can be enhanced for real-time analysis updates
            await manager.send_personal_message(f"Message received: {data}", websocket)
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        manager.disconnect(websocket)
# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Global exception handler for unhandled errors."""
    logger.error(f"Unhandled exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "type": "internal_error"}
    )
# Serve static files (React build) - will be added after UI is built
# app.mount("/", StaticFiles(directory="ui/dist", html=True), name="ui")
if __name__ == "__main__":
    uvicorn.run(
        "api.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
</file>

<file path="api/models/__init__.py">
"""API models package for KDP Strategist.
This package contains Pydantic models for API requests and responses.
"""
</file>

<file path="api/models/requests.py">
"""Request models for KDP Strategist API.
These Pydantic models define the structure of incoming API requests
from the React frontend to the FastAPI backend.
"""
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field, validator
class NicheDiscoveryRequest(BaseModel):
    """Request model for niche discovery endpoint."""
    base_keywords: List[str] = Field(
        ..., 
        description="List of base keywords to analyze",
        min_items=1,
        max_items=10
    )
    max_niches: int = Field(
        default=5,
        description="Maximum number of niches to return",
        ge=1,
        le=20
    )
    filters: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Optional filters for niche discovery"
    )
    @validator('base_keywords')
    def validate_keywords(cls, v):
        if not v:
            raise ValueError('At least one keyword is required')
        # Clean and validate keywords
        cleaned = [kw.strip().lower() for kw in v if kw.strip()]
        if not cleaned:
            raise ValueError('Keywords cannot be empty after cleaning')
        return cleaned
class CompetitorAnalysisRequest(BaseModel):
    """Request model for competitor analysis endpoint."""
    asins: List[str] = Field(
        ...,
        description="List of Amazon ASINs to analyze",
        min_items=1,
        max_items=20
    )
    analysis_depth: str = Field(
        default="standard",
        description="Depth of analysis: basic, standard, or detailed"
    )
    include_trends: bool = Field(
        default=True,
        description="Whether to include trend analysis"
    )
    @validator('asins')
    def validate_asins(cls, v):
        # Basic ASIN validation (10 characters, alphanumeric)
        cleaned_asins = []
        for asin in v:
            asin = asin.strip().upper()
            if len(asin) != 10:
                raise ValueError(f'ASIN {asin} must be exactly 10 characters')
            if not asin.isalnum():
                raise ValueError(f'ASIN {asin} must be alphanumeric')
            cleaned_asins.append(asin)
        return cleaned_asins
    @validator('analysis_depth')
    def validate_analysis_depth(cls, v):
        if v not in ['basic', 'standard', 'detailed']:
            raise ValueError('Analysis depth must be basic, standard, or detailed')
        return v
class ListingGenerationRequest(BaseModel):
    """Request model for listing generation endpoint."""
    niche: str = Field(
        ...,
        description="Target niche for listing generation",
        min_length=3,
        max_length=100
    )
    target_audience: Optional[str] = Field(
        default=None,
        description="Specific target audience",
        max_length=200
    )
    book_type: str = Field(
        default="paperback",
        description="Type of book: paperback, ebook, or hardcover"
    )
    include_keywords: bool = Field(
        default=True,
        description="Whether to include keyword optimization"
    )
    tone: str = Field(
        default="professional",
        description="Tone for the listing: professional, casual, or creative"
    )
    @validator('book_type')
    def validate_book_type(cls, v):
        if v not in ['paperback', 'ebook', 'hardcover']:
            raise ValueError('Book type must be paperback, ebook, or hardcover')
        return v
    @validator('tone')
    def validate_tone(cls, v):
        if v not in ['professional', 'casual', 'creative']:
            raise ValueError('Tone must be professional, casual, or creative')
        return v
class TrendValidationRequest(BaseModel):
    """Request model for trend validation endpoint."""
    keywords: List[str] = Field(
        ...,
        description="Keywords to validate trends for",
        min_items=1,
        max_items=15
    )
    timeframe: str = Field(
        default="12m",
        description="Timeframe for trend analysis: 1m, 3m, 6m, 12m, or 24m"
    )
    geo: str = Field(
        default="US",
        description="Geographic region for trend analysis"
    )
    include_seasonality: bool = Field(
        default=True,
        description="Whether to include seasonality analysis"
    )
    @validator('timeframe')
    def validate_timeframe(cls, v):
        if v not in ['1m', '3m', '6m', '12m', '24m']:
            raise ValueError('Timeframe must be 1m, 3m, 6m, 12m, or 24m')
        return v
    @validator('keywords')
    def validate_keywords(cls, v):
        cleaned = [kw.strip().lower() for kw in v if kw.strip()]
        if not cleaned:
            raise ValueError('At least one valid keyword is required')
        return cleaned
class StressTestingRequest(BaseModel):
    """Request model for stress testing endpoint."""
    niche: str = Field(
        ...,
        description="Niche to stress test",
        min_length=3,
        max_length=100
    )
    test_scenarios: List[str] = Field(
        default=["market_saturation", "seasonal_decline", "trend_reversal"],
        description="List of stress test scenarios to run"
    )
    severity_level: str = Field(
        default="moderate",
        description="Severity level: mild, moderate, or severe"
    )
    include_recommendations: bool = Field(
        default=True,
        description="Whether to include mitigation recommendations"
    )
    @validator('severity_level')
    def validate_severity_level(cls, v):
        if v not in ['mild', 'moderate', 'severe']:
            raise ValueError('Severity level must be mild, moderate, or severe')
        return v
    @validator('test_scenarios')
    def validate_test_scenarios(cls, v):
        valid_scenarios = [
            "market_saturation", "seasonal_decline", "trend_reversal",
            "competition_increase", "demand_drop", "keyword_shift"
        ]
        for scenario in v:
            if scenario not in valid_scenarios:
                raise ValueError(f'Invalid scenario: {scenario}. Must be one of {valid_scenarios}')
        return v
class ExportRequest(BaseModel):
    """Request model for export functionality."""
    data_type: str = Field(
        ...,
        description="Type of data to export: niche, competitor, listing, trend, or stress"
    )
    format: str = Field(
        ...,
        description="Export format: json, csv, or docx"
    )
    data: Dict[str, Any] = Field(
        ...,
        description="Data to export"
    )
    filename: Optional[str] = Field(
        default=None,
        description="Custom filename for export"
    )
    @validator('data_type')
    def validate_data_type(cls, v):
        if v not in ['niche', 'competitor', 'listing', 'trend', 'stress']:
            raise ValueError('Data type must be niche, competitor, listing, trend, or stress')
        return v
    @validator('format')
    def validate_format(cls, v):
        if v not in ['json', 'csv', 'docx']:
            raise ValueError('Format must be json, csv, or docx')
        return v
</file>

<file path="api/models/responses.py">
"""Response models for KDP Strategist API.
These Pydantic models define the structure of API responses
sent from the FastAPI backend to the React frontend.
"""
from typing import List, Optional, Dict, Any, Union
from datetime import datetime
from pydantic import BaseModel, Field
class ChartData(BaseModel):
    """Model for chart data visualization."""
    type: str = Field(..., description="Chart type: line, bar, pie, scatter")
    title: str = Field(..., description="Chart title")
    data: List[Dict[str, Any]] = Field(..., description="Chart data points")
    labels: Optional[List[str]] = Field(None, description="Chart labels")
    colors: Optional[List[str]] = Field(None, description="Chart colors")
    options: Optional[Dict[str, Any]] = Field(None, description="Chart options")
class AnalysisMetadata(BaseModel):
    """Metadata for analysis operations."""
    execution_time: float = Field(..., description="Execution time in seconds")
    timestamp: datetime = Field(default_factory=datetime.now, description="Analysis timestamp")
    tool_version: str = Field(default="1.0.0", description="Tool version used")
    data_sources: List[str] = Field(default=[], description="Data sources used")
    cache_hit: bool = Field(default=False, description="Whether result was cached")
    warnings: List[str] = Field(default=[], description="Analysis warnings")
class NicheData(BaseModel):
    """Model for individual niche data."""
    name: str = Field(..., description="Niche name")
    score: float = Field(..., description="Profitability score (0-100)")
    competition_level: str = Field(..., description="Competition level: low, medium, high")
    search_volume: int = Field(..., description="Monthly search volume")
    trend_direction: str = Field(..., description="Trend direction: rising, stable, declining")
    keywords: List[str] = Field(default=[], description="Related keywords")
    estimated_revenue: Optional[float] = Field(None, description="Estimated monthly revenue")
    seasonality: Optional[Dict[str, float]] = Field(None, description="Seasonal patterns")
    barriers_to_entry: List[str] = Field(default=[], description="Market barriers")
class NicheDiscoveryResponse(BaseModel):
    """Response model for niche discovery."""
    status: str = Field(default="success", description="Response status")
    niches: List[NicheData] = Field(..., description="Discovered niches")
    total_analyzed: int = Field(..., description="Total niches analyzed")
    analysis_metadata: AnalysisMetadata = Field(..., description="Analysis metadata")
    charts: List[ChartData] = Field(default=[], description="Visualization data")
    export_options: List[str] = Field(default=["json", "csv"], description="Available export formats")
    recommendations: List[str] = Field(default=[], description="Strategic recommendations")
class CompetitorData(BaseModel):
    """Model for individual competitor data."""
    asin: str = Field(..., description="Amazon ASIN")
    title: str = Field(..., description="Product title")
    author: Optional[str] = Field(None, description="Author name")
    price: Optional[float] = Field(None, description="Current price")
    rank: Optional[int] = Field(None, description="Best seller rank")
    rating: Optional[float] = Field(None, description="Average rating")
    review_count: Optional[int] = Field(None, description="Number of reviews")
    publication_date: Optional[str] = Field(None, description="Publication date")
    page_count: Optional[int] = Field(None, description="Number of pages")
    categories: List[str] = Field(default=[], description="Product categories")
    keywords: List[str] = Field(default=[], description="Identified keywords")
    strengths: List[str] = Field(default=[], description="Competitive strengths")
    weaknesses: List[str] = Field(default=[], description="Competitive weaknesses")
    market_share: Optional[float] = Field(None, description="Estimated market share")
class CompetitorAnalysisResponse(BaseModel):
    """Response model for competitor analysis."""
    status: str = Field(default="success", description="Response status")
    competitors: List[CompetitorData] = Field(..., description="Competitor analysis data")
    market_overview: Dict[str, Any] = Field(..., description="Market overview statistics")
    analysis_metadata: AnalysisMetadata = Field(..., description="Analysis metadata")
    charts: List[ChartData] = Field(default=[], description="Visualization data")
    export_options: List[str] = Field(default=["json", "csv"], description="Available export formats")
    insights: List[str] = Field(default=[], description="Key insights")
    opportunities: List[str] = Field(default=[], description="Market opportunities")
class ListingData(BaseModel):
    """Model for generated listing data."""
    title: str = Field(..., description="Optimized book title")
    subtitle: Optional[str] = Field(None, description="Book subtitle")
    description: str = Field(..., description="Book description")
    keywords: List[str] = Field(..., description="Optimized keywords")
    categories: List[str] = Field(..., description="Recommended categories")
    target_price: Optional[float] = Field(None, description="Recommended price")
    bullet_points: List[str] = Field(default=[], description="Key selling points")
    author_bio: Optional[str] = Field(None, description="Suggested author bio")
    back_cover_text: Optional[str] = Field(None, description="Back cover description")
    marketing_hooks: List[str] = Field(default=[], description="Marketing angles")
class ListingGenerationResponse(BaseModel):
    """Response model for listing generation."""
    status: str = Field(default="success", description="Response status")
    listing: ListingData = Field(..., description="Generated listing data")
    optimization_score: float = Field(..., description="SEO optimization score (0-100)")
    analysis_metadata: AnalysisMetadata = Field(..., description="Analysis metadata")
    export_options: List[str] = Field(default=["json", "docx"], description="Available export formats")
    seo_recommendations: List[str] = Field(default=[], description="SEO improvement suggestions")
    compliance_check: Dict[str, bool] = Field(default={}, description="KDP compliance status")
class TrendData(BaseModel):
    """Model for trend analysis data."""
    keyword: str = Field(..., description="Analyzed keyword")
    trend_score: float = Field(..., description="Trend strength score (0-100)")
    direction: str = Field(..., description="Trend direction: rising, stable, declining")
    volatility: float = Field(..., description="Trend volatility (0-100)")
    seasonal_pattern: Optional[Dict[str, float]] = Field(None, description="Seasonal patterns")
    peak_months: List[str] = Field(default=[], description="Peak search months")
    related_queries: List[str] = Field(default=[], description="Related search queries")
    forecast: Optional[Dict[str, float]] = Field(None, description="3-month forecast")
    confidence_level: float = Field(..., description="Forecast confidence (0-100)")
class TrendValidationResponse(BaseModel):
    """Response model for trend validation."""
    status: str = Field(default="success", description="Response status")
    trends: List[TrendData] = Field(..., description="Trend analysis data")
    overall_trend_health: str = Field(..., description="Overall trend assessment")
    analysis_metadata: AnalysisMetadata = Field(..., description="Analysis metadata")
    charts: List[ChartData] = Field(default=[], description="Visualization data")
    export_options: List[str] = Field(default=["json", "csv"], description="Available export formats")
    recommendations: List[str] = Field(default=[], description="Strategic recommendations")
    risk_factors: List[str] = Field(default=[], description="Identified risk factors")
class StressTestScenario(BaseModel):
    """Model for stress test scenario results."""
    scenario: str = Field(..., description="Stress test scenario name")
    severity: str = Field(..., description="Test severity level")
    impact_score: float = Field(..., description="Impact score (0-100)")
    probability: float = Field(..., description="Scenario probability (0-100)")
    description: str = Field(..., description="Scenario description")
    potential_losses: Optional[float] = Field(None, description="Estimated potential losses")
    mitigation_strategies: List[str] = Field(default=[], description="Mitigation recommendations")
    recovery_time: Optional[str] = Field(None, description="Estimated recovery time")
class StressTestingResponse(BaseModel):
    """Response model for stress testing."""
    status: str = Field(default="success", description="Response status")
    niche: str = Field(..., description="Tested niche")
    overall_resilience: float = Field(..., description="Overall resilience score (0-100)")
    risk_level: str = Field(..., description="Risk level: low, medium, high")
    scenarios: List[StressTestScenario] = Field(..., description="Stress test scenarios")
    analysis_metadata: AnalysisMetadata = Field(..., description="Analysis metadata")
    charts: List[ChartData] = Field(default=[], description="Visualization data")
    export_options: List[str] = Field(default=["json", "csv"], description="Available export formats")
    recommendations: List[str] = Field(default=[], description="Risk mitigation recommendations")
    contingency_plans: List[str] = Field(default=[], description="Contingency planning suggestions")
class ExportResponse(BaseModel):
    """Response model for export operations."""
    status: str = Field(default="success", description="Export status")
    download_url: str = Field(..., description="Download URL for exported file")
    filename: str = Field(..., description="Generated filename")
    file_size: int = Field(..., description="File size in bytes")
    format: str = Field(..., description="Export format")
    expires_at: datetime = Field(..., description="Download link expiration")
class ErrorResponse(BaseModel):
    """Response model for API errors."""
    status: str = Field(default="error", description="Response status")
    error_type: str = Field(..., description="Error type")
    message: str = Field(..., description="Error message")
    details: Optional[Dict[str, Any]] = Field(None, description="Additional error details")
    timestamp: datetime = Field(default_factory=datetime.now, description="Error timestamp")
    request_id: Optional[str] = Field(None, description="Request ID for tracking")
class HealthResponse(BaseModel):
    """Response model for health check."""
    status: str = Field(default="healthy", description="Service status")
    service: str = Field(default="kdp_strategist-api", description="Service name")
    version: str = Field(default="1.0.0", description="API version")
    timestamp: datetime = Field(default_factory=datetime.now, description="Health check timestamp")
    dependencies: Dict[str, str] = Field(default={}, description="Dependency status")
    uptime: Optional[float] = Field(None, description="Service uptime in seconds")
</file>

<file path="api/routers/__init__.py">
"""API routers package for KDP Strategist.
This package contains all the FastAPI routers that handle different
aspects of the KDP Strategist functionality.
"""
</file>

<file path="api/routers/competitors.py">
"""Competitor analysis API router.
This module provides REST API endpoints for competitor analysis functionality,
integrating with the existing MCP agent's competitor analysis tool.
"""
import logging
from typing import Dict, Any, List
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from ..models.requests import CompetitorAnalysisRequest
from ..models.responses import (
    CompetitorAnalysisResponse,
    CompetitorData,
    AnalysisMetadata,
    ChartData,
    ErrorResponse
)
logger = logging.getLogger(__name__)
router = APIRouter()
def get_agent():
    """Dependency to get the MCP agent from app state."""
    from fastapi import Request
    def _get_agent(request: Request):
        if not hasattr(request.app.state, 'agent'):
            raise HTTPException(
                status_code=503,
                detail="MCP agent not available"
            )
        return request.app.state.agent
    return _get_agent
def convert_mcp_competitor_to_api(mcp_competitor: Dict[str, Any]) -> CompetitorData:
    """Convert MCP agent competitor data to API response format."""
    return CompetitorData(
        asin=mcp_competitor.get('asin', ''),
        title=mcp_competitor.get('title', ''),
        author=mcp_competitor.get('author'),
        price=mcp_competitor.get('price'),
        rank=mcp_competitor.get('rank'),
        rating=mcp_competitor.get('rating'),
        review_count=mcp_competitor.get('review_count'),
        publication_date=mcp_competitor.get('publication_date'),
        page_count=mcp_competitor.get('page_count'),
        categories=mcp_competitor.get('categories', []),
        keywords=mcp_competitor.get('keywords', []),
        strengths=mcp_competitor.get('strengths', []),
        weaknesses=mcp_competitor.get('weaknesses', []),
        market_share=mcp_competitor.get('market_share')
    )
def create_competitor_charts(competitors: List[CompetitorData], market_overview: Dict[str, Any]) -> List[ChartData]:
    """Create chart data for competitor visualization."""
    charts = []
    # Price distribution chart
    price_data = [
        {"asin": comp.asin, "title": comp.title[:30] + "...", "price": comp.price}
        for comp in competitors if comp.price is not None
    ]
    if price_data:
        charts.append(ChartData(
            type="bar",
            title="Competitor Price Comparison",
            data=price_data,
            labels=[item["title"] for item in price_data],
            colors=["#3B82F6"] * len(price_data)
        ))
    # Rating vs Review Count scatter plot
    rating_data = [
        {
            "x": comp.review_count or 0,
            "y": comp.rating or 0,
            "label": comp.title[:20] + "...",
            "asin": comp.asin
        }
        for comp in competitors 
        if comp.rating is not None and comp.review_count is not None
    ]
    if rating_data:
        charts.append(ChartData(
            type="scatter",
            title="Rating vs Review Count",
            data=rating_data
        ))
    # Market share pie chart
    market_share_data = [
        {"label": comp.title[:20] + "...", "value": comp.market_share or 0}
        for comp in competitors if comp.market_share is not None
    ]
    if market_share_data:
        charts.append(ChartData(
            type="pie",
            title="Market Share Distribution",
            data=market_share_data,
            colors=["#10B981", "#3B82F6", "#F59E0B", "#EF4444", "#8B5CF6"]
        ))
    # Best seller rank comparison
    rank_data = [
        {"asin": comp.asin, "title": comp.title[:30] + "...", "rank": comp.rank}
        for comp in competitors if comp.rank is not None
    ]
    if rank_data:
        # Sort by rank (lower is better)
        rank_data.sort(key=lambda x: x["rank"])
        charts.append(ChartData(
            type="bar",
            title="Best Seller Rank Comparison (Lower is Better)",
            data=rank_data,
            labels=[item["title"] for item in rank_data],
            colors=["#10B981"] * len(rank_data)
        ))
    return charts
def generate_market_insights(competitors: List[CompetitorData], market_overview: Dict[str, Any]) -> List[str]:
    """Generate insights from competitor analysis."""
    insights = []
    if competitors:
        # Price insights
        prices = [c.price for c in competitors if c.price is not None]
        if prices:
            avg_price = sum(prices) / len(prices)
            min_price = min(prices)
            max_price = max(prices)
            insights.append(
                f"Average competitor price: ${avg_price:.2f} (range: ${min_price:.2f} - ${max_price:.2f})"
            )
        # Rating insights
        ratings = [c.rating for c in competitors if c.rating is not None]
        if ratings:
            avg_rating = sum(ratings) / len(ratings)
            insights.append(f"Average competitor rating: {avg_rating:.1f}/5.0")
        # Review count insights
        review_counts = [c.review_count for c in competitors if c.review_count is not None]
        if review_counts:
            avg_reviews = sum(review_counts) / len(review_counts)
            insights.append(f"Average review count: {int(avg_reviews)} reviews")
        # Competition level insight
        high_rated = len([c for c in competitors if c.rating and c.rating >= 4.0])
        if high_rated > len(competitors) * 0.7:
            insights.append("High competition: Most competitors have strong ratings (4.0+)")
        elif high_rated < len(competitors) * 0.3:
            insights.append("Opportunity: Many competitors have lower ratings (<4.0)")
    return insights
def identify_opportunities(competitors: List[CompetitorData]) -> List[str]:
    """Identify market opportunities based on competitor analysis."""
    opportunities = []
    if competitors:
        # Low rating opportunities
        low_rated = [c for c in competitors if c.rating and c.rating < 3.5]
        if low_rated:
            opportunities.append(
                f"Quality opportunity: {len(low_rated)} competitors have ratings below 3.5"
            )
        # High price opportunities
        prices = [c.price for c in competitors if c.price is not None]
        if prices:
            avg_price = sum(prices) / len(prices)
            if avg_price > 15:
                opportunities.append(
                    f"Price opportunity: Average price is ${avg_price:.2f} - consider competitive pricing"
                )
        # Low review count opportunities
        review_counts = [c.review_count for c in competitors if c.review_count is not None]
        if review_counts:
            low_review_count = len([c for c in review_counts if c < 100])
            if low_review_count > len(review_counts) * 0.5:
                opportunities.append(
                    "Market entry opportunity: Many competitors have fewer than 100 reviews"
                )
        # Keyword gaps
        all_keywords = set()
        for comp in competitors:
            all_keywords.update(comp.keywords)
        if len(all_keywords) < 20:
            opportunities.append(
                "SEO opportunity: Limited keyword diversity among competitors"
            )
    return opportunities
@router.post("/analyze", response_model=CompetitorAnalysisResponse)
async def analyze_competitors(
    request: CompetitorAnalysisRequest,
    background_tasks: BackgroundTasks,
    agent=Depends(get_agent())
):
    """Analyze competitors based on ASINs.
    This endpoint analyzes the provided Amazon ASINs to gather
    competitive intelligence using the MCP agent's competitor analysis tool.
    """
    try:
        logger.info(f"Starting competitor analysis for ASINs: {request.asins}")
        # Prepare parameters for MCP agent
        mcp_params = {
            "asins": request.asins,
            "analysis_depth": request.analysis_depth,
            "include_trends": request.include_trends
        }
        # Call MCP agent's competitor analysis method
        mcp_result = await agent.analyze_competitors(
            niche=request.asins[0] if request.asins else "general",
            analysis_depth=request.analysis_depth,
            include_trends=request.include_trends
        )
        # Convert MCP result to API format
        competitors = [
            convert_mcp_competitor_to_api(comp)
            for comp in mcp_result.get('competitors', [])
        ]
        # Extract market overview
        market_overview = mcp_result.get('market_overview', {})
        # Create visualization charts
        charts = create_competitor_charts(competitors, market_overview)
        # Generate insights and opportunities
        insights = generate_market_insights(competitors, market_overview)
        opportunities = identify_opportunities(competitors)
        # Create analysis metadata
        metadata = AnalysisMetadata(
            execution_time=mcp_result.get('execution_time', 0.0),
            data_sources=mcp_result.get('data_sources', ['keepa']),
            cache_hit=mcp_result.get('cache_hit', False),
            warnings=mcp_result.get('warnings', [])
        )
        response = CompetitorAnalysisResponse(
            competitors=competitors,
            market_overview=market_overview,
            analysis_metadata=metadata,
            charts=charts,
            insights=insights,
            opportunities=opportunities
        )
        logger.info(f"Competitor analysis completed. Analyzed {len(competitors)} competitors")
        return response
    except Exception as e:
        logger.error(f"Error in competitor analysis: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to analyze competitors: {str(e)}"
        )
@router.get("/search")
async def search_competitors(
    keyword: str,
    limit: int = 10,
    category: str = None,
    agent=Depends(get_agent())
):
    """Search for competitors by keyword.
    This endpoint searches for competitor products based on keywords
    and returns their ASINs for further analysis.
    """
    try:
        logger.info(f"Searching competitors for keyword: {keyword}")
        # This would typically use Amazon API or web scraping
        # For now, return mock data structure
        mock_results = [
            {
                "asin": f"B{i:09d}",
                "title": f"Sample Book {i} about {keyword}",
                "author": f"Author {i}",
                "price": 9.99 + i,
                "rating": 4.0 + (i % 5) * 0.2,
                "review_count": 100 + i * 50
            }
            for i in range(1, min(limit + 1, 11))
        ]
        return {
            "keyword": keyword,
            "results": mock_results,
            "total_found": len(mock_results),
            "category": category,
            "note": "This is mock data. Real implementation would use Amazon API or web scraping."
        }
    except Exception as e:
        logger.error(f"Error searching competitors: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to search competitors: {str(e)}"
        )
@router.get("/market-overview")
async def get_market_overview(
    category: str = None,
    timeframe: str = "30d"
):
    """Get market overview statistics.
    This endpoint provides general market statistics and trends
    for the specified category and timeframe.
    """
    try:
        # Mock market overview data
        overview = {
            "category": category or "All Categories",
            "timeframe": timeframe,
            "total_products": 15420,
            "average_price": 12.99,
            "average_rating": 4.1,
            "top_keywords": [
                "productivity", "mindfulness", "self-help", 
                "business", "health", "fitness"
            ],
            "market_trends": {
                "growth_rate": 8.5,
                "new_entries": 234,
                "price_trend": "stable",
                "demand_level": "high"
            },
            "competition_metrics": {
                "saturation_level": "medium",
                "barrier_to_entry": "low",
                "average_reviews_needed": 150
            }
        }
        return overview
    except Exception as e:
        logger.error(f"Error getting market overview: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get market overview: {str(e)}"
        )
</file>

<file path="api/routers/dashboard.py">
"""Dashboard API router.
This module provides REST API endpoints for dashboard functionality,
including statistics and recent activity data.
"""
import logging
from typing import Dict, Any, List
from datetime import datetime, timedelta
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import JSONResponse
from pydantic import BaseModel
logger = logging.getLogger(__name__)
router = APIRouter()
# Response models
class DashboardStats(BaseModel):
    """Dashboard statistics model."""
    totalAnalyses: int
    successfulListings: int
    averageScore: float
    trendsValidated: int
    totalRevenue: float = 0.0
    activeProjects: int = 0
class ActivityItem(BaseModel):
    """Recent activity item model."""
    id: int
    type: str
    title: str
    timestamp: str
    status: str
    score: float = None
class DashboardActivity(BaseModel):
    """Dashboard activity response model."""
    recent_activities: List[ActivityItem]
    total_count: int
@router.get("/stats", response_model=DashboardStats)
async def get_dashboard_stats():
    """Get dashboard statistics.
    Returns aggregated statistics for the dashboard including:
    - Total analyses performed
    - Successful listings generated
    - Average performance score
    - Trends validated
    """
    try:
        # For now, return mock data that matches the frontend expectations
        # In a real implementation, this would query your database
        stats = DashboardStats(
            totalAnalyses=47,
            successfulListings=23,
            averageScore=78.5,
            trendsValidated=12,
            totalRevenue=15420.50,
            activeProjects=8
        )
        logger.info("Dashboard stats retrieved successfully")
        return stats
    except Exception as e:
        logger.error(f"Error retrieving dashboard stats: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve dashboard statistics: {str(e)}"
        )
@router.get("/activity", response_model=DashboardActivity)
async def get_dashboard_activity(limit: int = 10):
    """Get recent dashboard activity.
    Args:
        limit: Maximum number of activity items to return (default: 10)
    Returns recent activity items including:
    - Analysis results
    - Listing generations
    - Trend validations
    - Other user actions
    """
    try:
        # For now, return mock data that matches the frontend expectations
        # In a real implementation, this would query your database
        recent_activities = [
            ActivityItem(
                id=1,
                type="niche_discovery",
                title="Self-Help Niche Analysis",
                timestamp=(datetime.now() - timedelta(hours=2)).isoformat(),
                status="completed",
                score=85.2
            ),
            ActivityItem(
                id=2,
                type="competitor_analysis",
                title="Romance Novel Market Research",
                timestamp=(datetime.now() - timedelta(hours=4)).isoformat(),
                status="completed",
                score=92.1
            ),
            ActivityItem(
                id=3,
                type="listing_generation",
                title="Cookbook Listing Created",
                timestamp=(datetime.now() - timedelta(hours=6)).isoformat(),
                status="completed",
                score=78.9
            ),
            ActivityItem(
                id=4,
                type="trend_validation",
                title="Mindfulness Trend Analysis",
                timestamp=(datetime.now() - timedelta(hours=8)).isoformat(),
                status="completed",
                score=88.7
            ),
            ActivityItem(
                id=5,
                type="niche_discovery",
                title="Tech Tutorial Niche",
                timestamp=(datetime.now() - timedelta(days=1)).isoformat(),
                status="completed",
                score=76.3
            ),
            ActivityItem(
                id=6,
                type="competitor_analysis",
                title="Fantasy Genre Analysis",
                timestamp=(datetime.now() - timedelta(days=1, hours=2)).isoformat(),
                status="completed",
                score=81.5
            ),
            ActivityItem(
                id=7,
                type="listing_generation",
                title="Travel Guide Listing",
                timestamp=(datetime.now() - timedelta(days=1, hours=4)).isoformat(),
                status="completed",
                score=89.2
            ),
            ActivityItem(
                id=8,
                type="trend_validation",
                title="Fitness Trend Research",
                timestamp=(datetime.now() - timedelta(days=2)).isoformat(),
                status="completed",
                score=83.6
            )
        ]
        # Limit the results
        limited_activities = recent_activities[:limit]
        activity_response = DashboardActivity(
            recent_activities=limited_activities,
            total_count=len(recent_activities)
        )
        logger.info(f"Dashboard activity retrieved successfully ({len(limited_activities)} items)")
        return activity_response
    except Exception as e:
        logger.error(f"Error retrieving dashboard activity: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve dashboard activity: {str(e)}"
        )
@router.get("/summary")
async def get_dashboard_summary():
    """Get a combined dashboard summary with both stats and recent activity.
    This endpoint provides a single call to get both statistics and activity data.
    """
    try:
        stats = await get_dashboard_stats()
        activity = await get_dashboard_activity(limit=5)
        summary = {
            "stats": stats.dict(),
            "recent_activity": activity.dict(),
            "last_updated": datetime.now().isoformat()
        }
        logger.info("Dashboard summary retrieved successfully")
        return summary
    except Exception as e:
        logger.error(f"Error retrieving dashboard summary: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve dashboard summary: {str(e)}"
        )
</file>

<file path="api/routers/listings.py">
"""Listing generation API router.
This module provides REST API endpoints for KDP listing generation functionality,
integrating with the existing MCP agent's listing generation tool.
"""
import logging
from typing import Dict, Any
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from ..models.requests import ListingGenerationRequest
from ..models.responses import (
    ListingGenerationResponse,
    ListingData,
    AnalysisMetadata,
    ErrorResponse
)
logger = logging.getLogger(__name__)
router = APIRouter()
def get_agent():
    """Dependency to get the MCP agent from app state."""
    from fastapi import Request
    def _get_agent(request: Request):
        if not hasattr(request.app.state, 'agent'):
            raise HTTPException(
                status_code=503,
                detail="MCP agent not available"
            )
        return request.app.state.agent
    return _get_agent
def convert_mcp_listing_to_api(mcp_listing: Dict[str, Any]) -> ListingData:
    """Convert MCP agent listing data to API response format."""
    return ListingData(
        title=mcp_listing.get('title', ''),
        subtitle=mcp_listing.get('subtitle'),
        description=mcp_listing.get('description', ''),
        keywords=mcp_listing.get('keywords', []),
        categories=mcp_listing.get('categories', []),
        target_price=mcp_listing.get('target_price'),
        bullet_points=mcp_listing.get('bullet_points', []),
        author_bio=mcp_listing.get('author_bio'),
        back_cover_text=mcp_listing.get('back_cover_text'),
        marketing_hooks=mcp_listing.get('marketing_hooks', [])
    )
def calculate_optimization_score(listing: ListingData) -> float:
    """Calculate SEO optimization score for the listing."""
    score = 0.0
    max_score = 100.0
    # Title optimization (25 points)
    if listing.title:
        title_length = len(listing.title)
        if 30 <= title_length <= 60:  # Optimal title length
            score += 25
        elif 20 <= title_length <= 80:
            score += 15
        else:
            score += 5
    # Keywords optimization (20 points)
    if listing.keywords:
        keyword_count = len(listing.keywords)
        if 5 <= keyword_count <= 7:  # Optimal keyword count
            score += 20
        elif 3 <= keyword_count <= 10:
            score += 15
        else:
            score += 5
    # Description optimization (20 points)
    if listing.description:
        desc_length = len(listing.description)
        if 200 <= desc_length <= 4000:  # Good description length
            score += 20
        elif 100 <= desc_length <= 5000:
            score += 15
        else:
            score += 5
    # Categories optimization (15 points)
    if listing.categories:
        if len(listing.categories) >= 2:
            score += 15
        elif len(listing.categories) == 1:
            score += 10
    # Bullet points optimization (10 points)
    if listing.bullet_points:
        if 3 <= len(listing.bullet_points) <= 5:
            score += 10
        elif len(listing.bullet_points) > 0:
            score += 5
    # Marketing hooks optimization (10 points)
    if listing.marketing_hooks:
        if len(listing.marketing_hooks) >= 3:
            score += 10
        elif len(listing.marketing_hooks) > 0:
            score += 5
    return min(score, max_score)
def generate_seo_recommendations(listing: ListingData, score: float) -> list[str]:
    """Generate SEO improvement recommendations."""
    recommendations = []
    # Title recommendations
    if listing.title:
        title_length = len(listing.title)
        if title_length < 30:
            recommendations.append("Consider expanding your title to 30-60 characters for better SEO")
        elif title_length > 80:
            recommendations.append("Consider shortening your title to under 80 characters")
    else:
        recommendations.append("Title is required for listing")
    # Keywords recommendations
    if not listing.keywords or len(listing.keywords) < 3:
        recommendations.append("Add more relevant keywords (aim for 5-7 keywords)")
    elif len(listing.keywords) > 10:
        recommendations.append("Consider reducing keywords to focus on most relevant ones")
    # Description recommendations
    if not listing.description or len(listing.description) < 200:
        recommendations.append("Expand your description to at least 200 characters for better engagement")
    # Categories recommendations
    if not listing.categories:
        recommendations.append("Select at least 2 relevant categories for better discoverability")
    elif len(listing.categories) < 2:
        recommendations.append("Consider adding a second category to increase visibility")
    # Bullet points recommendations
    if not listing.bullet_points or len(listing.bullet_points) < 3:
        recommendations.append("Add 3-5 bullet points highlighting key benefits")
    # Marketing hooks recommendations
    if not listing.marketing_hooks or len(listing.marketing_hooks) < 3:
        recommendations.append("Develop 3-5 marketing hooks to attract different customer segments")
    return recommendations
def check_kdp_compliance(listing: ListingData) -> Dict[str, bool]:
    """Check KDP compliance requirements."""
    compliance = {
        "title_length": len(listing.title) <= 200 if listing.title else False,
        "subtitle_length": len(listing.subtitle) <= 200 if listing.subtitle else True,
        "description_length": len(listing.description) <= 4000 if listing.description else False,
        "keywords_count": len(listing.keywords) <= 7 if listing.keywords else False,
        "has_required_fields": bool(listing.title and listing.description),
        "appropriate_content": True  # Would need content analysis in real implementation
    }
    return compliance
@router.post("/generate", response_model=ListingGenerationResponse)
async def generate_listing(
    request: ListingGenerationRequest,
    background_tasks: BackgroundTasks,
    agent=Depends(get_agent())
):
    """Generate optimized KDP listing.
    This endpoint generates an optimized Amazon KDP listing based on
    the provided niche and parameters using the MCP agent's listing generation tool.
    """
    try:
        logger.info(f"Starting listing generation for niche: {request.niche}")
        # Prepare parameters for MCP agent
        mcp_params = {
            "niche": request.niche,
            "target_audience": request.target_audience,
            "book_type": request.book_type,
            "include_keywords": request.include_keywords,
            "tone": request.tone
        }
        # Call MCP agent's listing generation method
        mcp_result = await agent.generate_listing(
            niche=request.niche,
            target_audience=request.target_audience,
            book_type=request.book_type,
            include_keywords=request.include_keywords,
            tone=request.tone
        )
        # Convert MCP result to API format
        listing = convert_mcp_listing_to_api(mcp_result.get('listing', {}))
        # Calculate optimization score
        optimization_score = calculate_optimization_score(listing)
        # Generate SEO recommendations
        seo_recommendations = generate_seo_recommendations(listing, optimization_score)
        # Check KDP compliance
        compliance_check = check_kdp_compliance(listing)
        # Create analysis metadata
        metadata = AnalysisMetadata(
            execution_time=mcp_result.get('execution_time', 0.0),
            data_sources=mcp_result.get('data_sources', ['openai', 'keepa']),
            cache_hit=mcp_result.get('cache_hit', False),
            warnings=mcp_result.get('warnings', [])
        )
        response = ListingGenerationResponse(
            listing=listing,
            optimization_score=optimization_score,
            analysis_metadata=metadata,
            seo_recommendations=seo_recommendations,
            compliance_check=compliance_check
        )
        logger.info(f"Listing generation completed with optimization score: {optimization_score:.1f}")
        return response
    except Exception as e:
        logger.error(f"Error in listing generation: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to generate listing: {str(e)}"
        )
@router.post("/optimize")
async def optimize_existing_listing(
    title: str,
    description: str,
    keywords: list[str] = None,
    agent=Depends(get_agent())
):
    """Optimize an existing listing.
    This endpoint takes an existing listing and provides optimization
    suggestions to improve its performance.
    """
    try:
        logger.info(f"Optimizing existing listing: {title[:50]}...")
        # Create a ListingData object from input
        current_listing = ListingData(
            title=title,
            description=description,
            keywords=keywords or [],
            categories=[],
            bullet_points=[],
            marketing_hooks=[]
        )
        # Calculate current optimization score
        current_score = calculate_optimization_score(current_listing)
        # Generate recommendations
        recommendations = generate_seo_recommendations(current_listing, current_score)
        # Check compliance
        compliance = check_kdp_compliance(current_listing)
        # Generate optimized version using MCP agent
        mcp_params = {
            "existing_title": title,
            "existing_description": description,
            "existing_keywords": keywords or [],
            "optimization_focus": "seo_and_conversion"
        }
        try:
            mcp_result = await agent.generate_listing(
                niche="optimization",
                target_audience="general",
                book_type="general"
            )
            optimized_listing = convert_mcp_listing_to_api(mcp_result.get('listing', {}))
            optimized_score = calculate_optimization_score(optimized_listing)
        except Exception as e:
            logger.warning(f"MCP optimization failed, using rule-based optimization: {e}")
            # Fallback to rule-based optimization
            optimized_listing = current_listing
            optimized_score = current_score
        return {
            "current_listing": current_listing,
            "current_score": current_score,
            "optimized_listing": optimized_listing,
            "optimized_score": optimized_score,
            "improvement": optimized_score - current_score,
            "recommendations": recommendations,
            "compliance_check": compliance
        }
    except Exception as e:
        logger.error(f"Error optimizing listing: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to optimize listing: {str(e)}"
        )
@router.get("/templates")
async def get_listing_templates():
    """Get listing templates for different niches and book types."""
    templates = {
        "self_help": {
            "title_template": "{Main Benefit}: {Specific Method} to {Desired Outcome} in {Timeframe}",
            "description_template": "Discover the proven strategies that have helped thousands...",
            "common_keywords": ["self-help", "personal development", "motivation", "success", "mindset"]
        },
        "business": {
            "title_template": "{Business Topic}: The Complete Guide to {Specific Goal}",
            "description_template": "Master the essential skills and strategies needed to...",
            "common_keywords": ["business", "entrepreneurship", "leadership", "strategy", "growth"]
        },
        "health_fitness": {
            "title_template": "{Health Goal}: {Method/System} for {Target Audience}",
            "description_template": "Transform your health and fitness with this comprehensive guide...",
            "common_keywords": ["health", "fitness", "nutrition", "wellness", "exercise"]
        },
        "cooking": {
            "title_template": "{Cuisine/Diet} Cookbook: {Number} Delicious Recipes for {Occasion}",
            "description_template": "Bring the flavors of {cuisine} to your kitchen with...",
            "common_keywords": ["cookbook", "recipes", "cooking", "food", "kitchen"]
        }
    }
    return {
        "templates": templates,
        "usage_tips": [
            "Customize templates based on your specific niche",
            "Include emotional triggers in titles",
            "Use numbers and specific benefits",
            "Test different variations"
        ]
    }
@router.get("/compliance-check")
async def get_compliance_guidelines():
    """Get KDP compliance guidelines and requirements."""
    guidelines = {
        "title_requirements": {
            "max_length": 200,
            "restrictions": [
                "No promotional language (e.g., 'Best Seller')",
                "No misleading claims",
                "No excessive punctuation or symbols"
            ]
        },
        "description_requirements": {
            "max_length": 4000,
            "recommendations": [
                "Use compelling opening hook",
                "Include benefits and outcomes",
                "Add social proof if available",
                "End with clear call-to-action"
            ]
        },
        "keyword_requirements": {
            "max_count": 7,
            "best_practices": [
                "Use relevant, specific keywords",
                "Avoid keyword stuffing",
                "Include long-tail keywords",
                "Research competitor keywords"
            ]
        },
        "content_restrictions": [
            "No copyrighted material",
            "No offensive or inappropriate content",
            "No misleading information",
            "No spam or low-quality content"
        ]
    }
    return guidelines
</file>

<file path="api/routers/niches.py">
"""Niche discovery API router.
This module provides REST API endpoints for niche discovery functionality,
integrating with the existing MCP agent's niche discovery tool.
"""
import logging
from typing import Dict, Any
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from ..models.requests import NicheDiscoveryRequest
from ..models.responses import (
    NicheDiscoveryResponse, 
    NicheData, 
    AnalysisMetadata, 
    ChartData,
    ErrorResponse
)
logger = logging.getLogger(__name__)
router = APIRouter()
def get_agent():
    """Dependency to get the MCP agent from app state."""
    from fastapi import Request
    def _get_agent(request: Request):
        if not hasattr(request.app.state, 'agent'):
            raise HTTPException(
                status_code=503,
                detail="MCP agent not available"
            )
        return request.app.state.agent
    return _get_agent
def convert_mcp_niche_to_api(mcp_niche: Dict[str, Any]) -> NicheData:
    """Convert MCP agent niche data to API response format."""
    return NicheData(
        name=mcp_niche.get('name', ''),
        score=float(mcp_niche.get('profitability_score', 0)),
        competition_level=mcp_niche.get('competition_level', 'unknown'),
        search_volume=int(mcp_niche.get('search_volume', 0)),
        trend_direction=mcp_niche.get('trend_direction', 'stable'),
        keywords=mcp_niche.get('keywords', []),
        estimated_revenue=mcp_niche.get('estimated_revenue'),
        seasonality=mcp_niche.get('seasonality'),
        barriers_to_entry=mcp_niche.get('barriers_to_entry', [])
    )
def create_niche_charts(niches: list[NicheData]) -> list[ChartData]:
    """Create chart data for niche visualization."""
    charts = []
    # Profitability score chart
    charts.append(ChartData(
        type="bar",
        title="Niche Profitability Scores",
        data=[
            {"name": niche.name, "score": niche.score}
            for niche in niches
        ],
        labels=[niche.name for niche in niches],
        colors=["#3B82F6", "#10B981", "#F59E0B", "#EF4444", "#8B5CF6"]
    ))
    # Competition level distribution
    competition_counts = {}
    for niche in niches:
        level = niche.competition_level
        competition_counts[level] = competition_counts.get(level, 0) + 1
    charts.append(ChartData(
        type="pie",
        title="Competition Level Distribution",
        data=[
            {"label": level, "value": count}
            for level, count in competition_counts.items()
        ],
        colors=["#10B981", "#F59E0B", "#EF4444"]
    ))
    # Search volume vs profitability scatter
    charts.append(ChartData(
        type="scatter",
        title="Search Volume vs Profitability",
        data=[
            {
                "x": niche.search_volume,
                "y": niche.score,
                "label": niche.name
            }
            for niche in niches
        ]
    ))
    return charts
@router.post("/discover", response_model=NicheDiscoveryResponse)
async def discover_niches(
    request: NicheDiscoveryRequest,
    background_tasks: BackgroundTasks,
    agent=Depends(get_agent())
):
    """Discover profitable niches based on keywords.
    This endpoint analyzes the provided keywords to identify profitable
    niches using the MCP agent's niche discovery tool.
    """
    try:
        logger.info(f"Starting niche discovery for keywords: {request.base_keywords}")
        # Prepare parameters for MCP agent
        mcp_params = {
            "base_keywords": request.base_keywords,
            "max_niches": request.max_niches
        }
        # Add filters if provided
        if request.filters:
            mcp_params.update(request.filters)
        # Call MCP agent's niche discovery method
        mcp_result = await agent.discover_niches(
            keywords=request.base_keywords,
            max_niches=request.max_niches,
            **(request.filters or {})
        )
        # Convert MCP result to API format
        niches = [
            convert_mcp_niche_to_api(niche)
            for niche in mcp_result.get('niches', [])
        ]
        # Create visualization charts
        charts = create_niche_charts(niches)
        # Generate recommendations
        recommendations = []
        if niches:
            top_niche = max(niches, key=lambda n: n.score)
            recommendations.append(
                f"Consider focusing on '{top_niche.name}' with the highest profitability score of {top_niche.score:.1f}"
            )
            low_competition = [n for n in niches if n.competition_level == 'low']
            if low_competition:
                recommendations.append(
                    f"Found {len(low_competition)} low-competition niches for easier market entry"
                )
        # Create analysis metadata
        metadata = AnalysisMetadata(
            execution_time=mcp_result.get('execution_time', 0.0),
            data_sources=mcp_result.get('data_sources', ['keepa', 'google_trends']),
            cache_hit=mcp_result.get('cache_hit', False),
            warnings=mcp_result.get('warnings', [])
        )
        response = NicheDiscoveryResponse(
            niches=niches,
            total_analyzed=len(niches),
            analysis_metadata=metadata,
            charts=charts,
            recommendations=recommendations
        )
        logger.info(f"Niche discovery completed. Found {len(niches)} niches")
        return response
    except Exception as e:
        logger.error(f"Error in niche discovery: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to discover niches: {str(e)}"
        )
@router.get("/trending", response_model=NicheDiscoveryResponse)
async def get_trending_niches(
    limit: int = 10,
    timeframe: str = "7d",
    agent=Depends(get_agent())
):
    """Get currently trending niches.
    This endpoint returns niches that are currently trending
    based on search volume and market activity.
    """
    try:
        logger.info(f"Getting trending niches with limit: {limit}, timeframe: {timeframe}")
        # For now, use a default set of trending keywords
        # In a real implementation, this would come from trend analysis
        trending_keywords = [
            "productivity", "mindfulness", "remote work", 
            "sustainable living", "digital detox"
        ]
        # Call the niche discovery with trending keywords
        mcp_params = {
            "base_keywords": trending_keywords[:limit//2],  # Use subset
            "max_niches": limit,
            "trending_only": True
        }
        mcp_result = await agent.discover_niches(
            keywords=trending_keywords[:limit//2],
            max_niches=limit
        )
        # Convert and format response
        niches = [
            convert_mcp_niche_to_api(niche)
            for niche in mcp_result.get('niches', [])
        ]
        # Filter for trending niches (rising trend direction)
        trending_niches = [
            niche for niche in niches 
            if niche.trend_direction == 'rising'
        ]
        charts = create_niche_charts(trending_niches)
        metadata = AnalysisMetadata(
            execution_time=mcp_result.get('execution_time', 0.0),
            data_sources=['google_trends', 'keepa'],
            cache_hit=mcp_result.get('cache_hit', False)
        )
        response = NicheDiscoveryResponse(
            niches=trending_niches,
            total_analyzed=len(trending_niches),
            analysis_metadata=metadata,
            charts=charts,
            recommendations=[
                "These niches show strong upward trends in the past week",
                "Consider quick market entry for trending opportunities"
            ]
        )
        logger.info(f"Found {len(trending_niches)} trending niches")
        return response
    except Exception as e:
        logger.error(f"Error getting trending niches: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get trending niches: {str(e)}"
        )
@router.get("/categories")
async def get_niche_categories():
    """Get available niche categories for filtering."""
    categories = [
        "Health & Fitness",
        "Business & Money",
        "Self-Help",
        "Technology",
        "Arts & Crafts",
        "Cooking & Food",
        "Travel",
        "Education",
        "Parenting",
        "Relationships",
        "Spirituality",
        "Sports",
        "Hobbies",
        "Home & Garden",
        "Fashion & Beauty"
    ]
    return {
        "categories": categories,
        "total": len(categories)
    }
</file>

<file path="api/routers/stress.py">
"""Stress testing API router.
This module provides REST API endpoints for stress testing functionality,
integrating with the existing MCP agent's stress testing tool.
"""
import logging
from typing import Dict, Any, List
from datetime import datetime
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from ..models.requests import StressTestingRequest
from ..models.responses import (
    StressTestingResponse,
    StressTestScenario,
    AnalysisMetadata,
    ChartData,
    ErrorResponse
)
logger = logging.getLogger(__name__)
router = APIRouter()
def get_agent():
    """Dependency to get the MCP agent from app state."""
    from fastapi import Request
    def _get_agent(request: Request):
        if not hasattr(request.app.state, 'agent'):
            raise HTTPException(
                status_code=503,
                detail="MCP agent not available"
            )
        return request.app.state.agent
    return _get_agent
def convert_mcp_scenario_to_api(mcp_scenario: Dict[str, Any]) -> StressTestScenario:
    """Convert MCP agent stress test scenario to API response format."""
    return StressTestScenario(
        scenario=mcp_scenario.get('scenario', ''),
        severity=mcp_scenario.get('severity', 'moderate'),
        impact_score=float(mcp_scenario.get('impact_score', 0)),
        probability=float(mcp_scenario.get('probability', 0)),
        description=mcp_scenario.get('description', ''),
        potential_losses=mcp_scenario.get('potential_losses'),
        mitigation_strategies=mcp_scenario.get('mitigation_strategies', []),
        recovery_time=mcp_scenario.get('recovery_time')
    )
def calculate_overall_resilience(scenarios: List[StressTestScenario]) -> float:
    """Calculate overall resilience score based on stress test scenarios."""
    if not scenarios:
        return 0.0
    # Weight scenarios by probability and impact
    total_weighted_impact = 0.0
    total_weight = 0.0
    for scenario in scenarios:
        weight = scenario.probability / 100.0  # Convert percentage to decimal
        weighted_impact = scenario.impact_score * weight
        total_weighted_impact += weighted_impact
        total_weight += weight
    if total_weight == 0:
        return 50.0  # Default neutral score
    # Calculate resilience as inverse of weighted impact
    avg_weighted_impact = total_weighted_impact / total_weight
    resilience_score = max(0, 100 - avg_weighted_impact)
    return resilience_score
def determine_risk_level(resilience_score: float) -> str:
    """Determine risk level based on resilience score."""
    if resilience_score >= 75:
        return "low"
    elif resilience_score >= 50:
        return "medium"
    else:
        return "high"
def create_stress_test_charts(scenarios: List[StressTestScenario], resilience_score: float) -> List[ChartData]:
    """Create chart data for stress test visualization."""
    charts = []
    # Impact vs Probability scatter plot
    scenario_data = [
        {
            "x": scenario.probability,
            "y": scenario.impact_score,
            "label": scenario.scenario.replace('_', ' ').title(),
            "severity": scenario.severity
        }
        for scenario in scenarios
    ]
    charts.append(ChartData(
        type="scatter",
        title="Risk Matrix: Impact vs Probability",
        data=scenario_data,
        options={
            "xAxis": {"title": "Probability (%)"},
            "yAxis": {"title": "Impact Score"}
        }
    ))
    # Scenario impact comparison
    impact_data = [
        {"scenario": scenario.scenario.replace('_', ' ').title(), "impact": scenario.impact_score}
        for scenario in scenarios
    ]
    charts.append(ChartData(
        type="bar",
        title="Scenario Impact Comparison",
        data=impact_data,
        labels=[item["scenario"] for item in impact_data],
        colors=["#EF4444", "#F59E0B", "#10B981", "#3B82F6", "#8B5CF6"]
    ))
    # Severity distribution
    severity_counts = {}
    for scenario in scenarios:
        severity = scenario.severity
        severity_counts[severity] = severity_counts.get(severity, 0) + 1
    charts.append(ChartData(
        type="pie",
        title="Scenario Severity Distribution",
        data=[
            {"label": severity.title(), "value": count}
            for severity, count in severity_counts.items()
        ],
        colors=["#10B981", "#F59E0B", "#EF4444"]
    ))
    # Resilience gauge
    charts.append(ChartData(
        type="gauge",
        title="Overall Resilience Score",
        data=[{"value": resilience_score, "max": 100}],
        colors=["#EF4444" if resilience_score < 50 else "#F59E0B" if resilience_score < 75 else "#10B981"]
    ))
    return charts
def generate_stress_test_recommendations(scenarios: List[StressTestScenario], resilience_score: float, risk_level: str) -> List[str]:
    """Generate recommendations based on stress test results."""
    recommendations = []
    # Overall resilience recommendations
    if risk_level == "high":
        recommendations.append("High risk detected - implement immediate risk mitigation strategies")
        recommendations.append("Consider diversifying into multiple niches to reduce concentration risk")
    elif risk_level == "medium":
        recommendations.append("Moderate risk level - develop contingency plans for key scenarios")
        recommendations.append("Monitor market conditions closely and prepare adaptive strategies")
    else:
        recommendations.append("Low risk profile - maintain current strategy with regular monitoring")
    # Scenario-specific recommendations
    high_impact_scenarios = [s for s in scenarios if s.impact_score > 70]
    if high_impact_scenarios:
        recommendations.append(
            f"Focus on mitigating {len(high_impact_scenarios)} high-impact scenarios first"
        )
    high_probability_scenarios = [s for s in scenarios if s.probability > 60]
    if high_probability_scenarios:
        recommendations.append(
            f"Prepare for {len(high_probability_scenarios)} likely scenarios with immediate action plans"
        )
    # Mitigation strategy recommendations
    all_strategies = set()
    for scenario in scenarios:
        all_strategies.update(scenario.mitigation_strategies)
    if "diversification" in str(all_strategies).lower():
        recommendations.append("Diversification appears in multiple mitigation strategies - prioritize this approach")
    if "monitoring" in str(all_strategies).lower():
        recommendations.append("Enhanced monitoring recommended across multiple scenarios")
    return recommendations
def generate_contingency_plans(scenarios: List[StressTestScenario]) -> List[str]:
    """Generate contingency planning suggestions."""
    contingency_plans = []
    # Emergency response plans
    contingency_plans.append("Establish early warning indicators for each identified risk scenario")
    contingency_plans.append("Create decision trees for rapid response to market changes")
    # Financial contingencies
    high_loss_scenarios = [s for s in scenarios if s.potential_losses and s.potential_losses > 1000]
    if high_loss_scenarios:
        contingency_plans.append("Maintain emergency fund to cover potential losses from high-impact scenarios")
    # Operational contingencies
    contingency_plans.append("Develop alternative marketing channels to reduce dependency on single platforms")
    contingency_plans.append("Build relationships with multiple suppliers/partners for operational resilience")
    # Strategic contingencies
    contingency_plans.append("Prepare pivot strategies for entering adjacent niches if primary market declines")
    contingency_plans.append("Establish regular stress testing schedule (quarterly) to update risk assessments")
    return contingency_plans
@router.post("/run", response_model=StressTestingResponse)
async def run_stress_test(
    request: StressTestingRequest,
    background_tasks: BackgroundTasks,
    agent=Depends(get_agent())
):
    """Run stress test on a niche.
    This endpoint performs stress testing on the specified niche
    using the MCP agent's stress testing tool.
    """
    try:
        logger.info(f"Starting stress test for niche: {request.niche}")
        # Prepare parameters for MCP agent
        mcp_params = {
            "niche": request.niche,
            "test_scenarios": request.test_scenarios,
            "severity_level": request.severity_level,
            "include_recommendations": request.include_recommendations
        }
        # Call MCP agent's stress testing method
        mcp_result = await agent.stress_test_niche(
            niche=request.niche,
            test_scenarios=request.test_scenarios,
            severity_level=request.severity_level,
            include_recommendations=request.include_recommendations
        )
        # Convert MCP result to API format
        scenarios = [
            convert_mcp_scenario_to_api(scenario)
            for scenario in mcp_result.get('scenarios', [])
        ]
        # Calculate overall resilience and risk level
        overall_resilience = calculate_overall_resilience(scenarios)
        risk_level = determine_risk_level(overall_resilience)
        # Create visualization charts
        charts = create_stress_test_charts(scenarios, overall_resilience)
        # Generate recommendations and contingency plans
        recommendations = generate_stress_test_recommendations(scenarios, overall_resilience, risk_level)
        contingency_plans = generate_contingency_plans(scenarios)
        # Create analysis metadata
        metadata = AnalysisMetadata(
            execution_time=mcp_result.get('execution_time', 0.0),
            data_sources=mcp_result.get('data_sources', ['market_analysis', 'historical_data']),
            cache_hit=mcp_result.get('cache_hit', False),
            warnings=mcp_result.get('warnings', [])
        )
        response = StressTestingResponse(
            niche=request.niche,
            overall_resilience=overall_resilience,
            risk_level=risk_level,
            scenarios=scenarios,
            analysis_metadata=metadata,
            charts=charts,
            recommendations=recommendations,
            contingency_plans=contingency_plans
        )
        logger.info(f"Stress test completed. Resilience score: {overall_resilience:.1f}, Risk level: {risk_level}")
        return response
    except Exception as e:
        logger.error(f"Error in stress testing: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to run stress test: {str(e)}"
        )
@router.get("/scenarios")
async def get_available_scenarios():
    """Get available stress test scenarios.
    This endpoint returns a list of available stress test scenarios
    that can be used for testing niche resilience.
    """
    scenarios = {
        "market_saturation": {
            "name": "Market Saturation",
            "description": "Sudden increase in competitors entering the market",
            "typical_impact": "High",
            "typical_probability": "Medium",
            "indicators": ["Increased competition", "Price pressure", "Reduced market share"]
        },
        "seasonal_decline": {
            "name": "Seasonal Decline",
            "description": "Significant drop in demand during off-season periods",
            "typical_impact": "Medium",
            "typical_probability": "High",
            "indicators": ["Seasonal search patterns", "Historical sales data", "Consumer behavior"]
        },
        "trend_reversal": {
            "name": "Trend Reversal",
            "description": "Major shift in consumer preferences away from the niche",
            "typical_impact": "High",
            "typical_probability": "Low",
            "indicators": ["Declining search trends", "Social media sentiment", "Industry reports"]
        },
        "competition_increase": {
            "name": "Competition Increase",
            "description": "Entry of major players or significant marketing spend by competitors",
            "typical_impact": "Medium",
            "typical_probability": "Medium",
            "indicators": ["New product launches", "Advertising spend", "Market consolidation"]
        },
        "demand_drop": {
            "name": "Demand Drop",
            "description": "Sudden decrease in overall market demand",
            "typical_impact": "High",
            "typical_probability": "Low",
            "indicators": ["Economic indicators", "Consumer spending", "Market research"]
        },
        "keyword_shift": {
            "name": "Keyword Shift",
            "description": "Changes in how consumers search for products in the niche",
            "typical_impact": "Medium",
            "typical_probability": "Medium",
            "indicators": ["Search term evolution", "Platform algorithm changes", "User behavior"]
        }
    }
    return {
        "scenarios": scenarios,
        "total_available": len(scenarios),
        "usage_tips": [
            "Select scenarios most relevant to your niche",
            "Consider both high-impact and high-probability scenarios",
            "Run tests at different severity levels",
            "Update stress tests regularly as market conditions change"
        ]
    }
@router.get("/risk-matrix")
async def get_risk_matrix():
    """Get risk assessment matrix guidelines.
    This endpoint provides guidelines for interpreting
    risk matrix results from stress testing.
    """
    risk_matrix = {
        "probability_levels": {
            "low": {"range": "0-30%", "description": "Unlikely to occur"},
            "medium": {"range": "31-70%", "description": "Possible occurrence"},
            "high": {"range": "71-100%", "description": "Likely to occur"}
        },
        "impact_levels": {
            "low": {"range": "0-30", "description": "Minor impact on business"},
            "medium": {"range": "31-70", "description": "Moderate impact on business"},
            "high": {"range": "71-100", "description": "Severe impact on business"}
        },
        "risk_categories": {
            "low_risk": {
                "criteria": "Low probability AND low impact",
                "action": "Monitor and accept",
                "color": "green"
            },
            "medium_risk": {
                "criteria": "Medium probability OR medium impact",
                "action": "Develop mitigation plans",
                "color": "yellow"
            },
            "high_risk": {
                "criteria": "High probability AND/OR high impact",
                "action": "Immediate action required",
                "color": "red"
            }
        },
        "resilience_scoring": {
            "excellent": {"range": "80-100", "description": "Highly resilient to stress scenarios"},
            "good": {"range": "60-79", "description": "Generally resilient with some vulnerabilities"},
            "fair": {"range": "40-59", "description": "Moderate resilience, requires attention"},
            "poor": {"range": "0-39", "description": "Low resilience, high risk exposure"}
        }
    }
    return risk_matrix
@router.post("/compare")
async def compare_niches(
    niches: List[str],
    scenarios: List[str] = None,
    agent=Depends(get_agent())
):
    """Compare stress test results across multiple niches.
    This endpoint runs stress tests on multiple niches
    and provides a comparative analysis.
    """
    try:
        if len(niches) > 5:
            raise HTTPException(
                status_code=400,
                detail="Maximum 5 niches can be compared at once"
            )
        logger.info(f"Comparing stress tests for niches: {niches}")
        comparison_results = []
        for niche in niches:
            # Run stress test for each niche
            mcp_params = {
                "niche": niche,
                "test_scenarios": scenarios or ["market_saturation", "seasonal_decline", "trend_reversal"],
                "severity_level": "moderate",
                "include_recommendations": False  # Skip for comparison
            }
            try:
                mcp_result = await agent.stress_test_niche(
                    niche=niche,
                    test_scenarios=scenarios or ["market_saturation", "seasonal_decline", "trend_reversal"],
                    severity_level="moderate",
                    include_recommendations=False
                )
                scenarios_data = [
                    convert_mcp_scenario_to_api(scenario)
                    for scenario in mcp_result.get('scenarios', [])
                ]
                resilience_score = calculate_overall_resilience(scenarios_data)
                risk_level = determine_risk_level(resilience_score)
                comparison_results.append({
                    "niche": niche,
                    "resilience_score": resilience_score,
                    "risk_level": risk_level,
                    "scenario_count": len(scenarios_data),
                    "avg_impact": sum(s.impact_score for s in scenarios_data) / len(scenarios_data) if scenarios_data else 0,
                    "avg_probability": sum(s.probability for s in scenarios_data) / len(scenarios_data) if scenarios_data else 0
                })
            except Exception as e:
                logger.warning(f"Failed to test niche {niche}: {e}")
                comparison_results.append({
                    "niche": niche,
                    "resilience_score": 0,
                    "risk_level": "unknown",
                    "error": str(e)
                })
        # Sort by resilience score (highest first)
        comparison_results.sort(key=lambda x: x.get('resilience_score', 0), reverse=True)
        # Generate comparison insights
        insights = []
        if comparison_results:
            best_niche = comparison_results[0]
            worst_niche = comparison_results[-1]
            insights.append(f"'{best_niche['niche']}' shows highest resilience ({best_niche['resilience_score']:.1f})")
            insights.append(f"'{worst_niche['niche']}' shows lowest resilience ({worst_niche['resilience_score']:.1f})")
            low_risk_count = len([r for r in comparison_results if r.get('risk_level') == 'low'])
            if low_risk_count > 0:
                insights.append(f"{low_risk_count} out of {len(niches)} niches show low risk profile")
        return {
            "comparison_results": comparison_results,
            "insights": insights,
            "recommendation": f"Consider focusing on '{comparison_results[0]['niche']}' for lowest risk exposure" if comparison_results else "No valid results for comparison"
        }
    except Exception as e:
        logger.error(f"Error in niche comparison: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to compare niches: {str(e)}"
        )
</file>

<file path="api/routers/trends.py">
"""Trend validation API router.
This module provides REST API endpoints for trend validation functionality,
integrating with the existing MCP agent's trend validation tool.
"""
import logging
from typing import Dict, Any, List
from datetime import datetime, timedelta
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from ..models.requests import TrendValidationRequest
from ..models.responses import (
    TrendValidationResponse,
    TrendData,
    AnalysisMetadata,
    ChartData,
    ErrorResponse
)
logger = logging.getLogger(__name__)
router = APIRouter()
def get_agent():
    """Dependency to get the MCP agent from app state."""
    from fastapi import Request
    def _get_agent(request: Request):
        if not hasattr(request.app.state, 'agent'):
            raise HTTPException(
                status_code=503,
                detail="MCP agent not available"
            )
        return request.app.state.agent
    return _get_agent
def convert_mcp_trend_to_api(mcp_trend: Dict[str, Any]) -> TrendData:
    """Convert MCP agent trend data to API response format."""
    return TrendData(
        keyword=mcp_trend.get('keyword', ''),
        trend_score=float(mcp_trend.get('trend_score', 0)),
        direction=mcp_trend.get('direction', 'stable'),
        volatility=float(mcp_trend.get('volatility', 0)),
        seasonal_pattern=mcp_trend.get('seasonal_pattern'),
        peak_months=mcp_trend.get('peak_months', []),
        related_queries=mcp_trend.get('related_queries', []),
        forecast=mcp_trend.get('forecast'),
        confidence_level=float(mcp_trend.get('confidence_level', 0))
    )
def create_trend_charts(trends: List[TrendData]) -> List[ChartData]:
    """Create chart data for trend visualization."""
    charts = []
    # Trend scores comparison
    trend_scores = [
        {"keyword": trend.keyword, "score": trend.trend_score}
        for trend in trends
    ]
    charts.append(ChartData(
        type="bar",
        title="Trend Strength Comparison",
        data=trend_scores,
        labels=[trend.keyword for trend in trends],
        colors=["#10B981", "#3B82F6", "#F59E0B", "#EF4444", "#8B5CF6"]
    ))
    # Trend direction distribution
    direction_counts = {}
    for trend in trends:
        direction = trend.direction
        direction_counts[direction] = direction_counts.get(direction, 0) + 1
    charts.append(ChartData(
        type="pie",
        title="Trend Direction Distribution",
        data=[
            {"label": direction.title(), "value": count}
            for direction, count in direction_counts.items()
        ],
        colors=["#10B981", "#F59E0B", "#EF4444"]
    ))
    # Volatility vs Trend Score scatter
    volatility_data = [
        {
            "x": trend.volatility,
            "y": trend.trend_score,
            "label": trend.keyword
        }
        for trend in trends
    ]
    charts.append(ChartData(
        type="scatter",
        title="Volatility vs Trend Strength",
        data=volatility_data
    ))
    # Seasonal patterns (if available)
    seasonal_trends = [trend for trend in trends if trend.seasonal_pattern]
    if seasonal_trends:
        # Create a combined seasonal chart
        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        seasonal_data = []
        for trend in seasonal_trends[:3]:  # Limit to top 3 for readability
            if trend.seasonal_pattern:
                seasonal_data.append({
                    "keyword": trend.keyword,
                    "data": [trend.seasonal_pattern.get(month, 0) for month in months]
                })
        if seasonal_data:
            charts.append(ChartData(
                type="line",
                title="Seasonal Patterns",
                data=seasonal_data,
                labels=months
            ))
    return charts
def assess_overall_trend_health(trends: List[TrendData]) -> str:
    """Assess overall trend health based on individual trends."""
    if not trends:
        return "unknown"
    rising_count = len([t for t in trends if t.direction == 'rising'])
    stable_count = len([t for t in trends if t.direction == 'stable'])
    declining_count = len([t for t in trends if t.direction == 'declining'])
    total = len(trends)
    rising_ratio = rising_count / total
    declining_ratio = declining_count / total
    avg_score = sum(t.trend_score for t in trends) / total
    avg_volatility = sum(t.volatility for t in trends) / total
    if rising_ratio >= 0.6 and avg_score >= 70:
        return "excellent"
    elif rising_ratio >= 0.4 and avg_score >= 50 and avg_volatility < 60:
        return "good"
    elif declining_ratio <= 0.3 and avg_score >= 30:
        return "fair"
    else:
        return "poor"
def generate_trend_recommendations(trends: List[TrendData], overall_health: str) -> List[str]:
    """Generate strategic recommendations based on trend analysis."""
    recommendations = []
    if not trends:
        return ["No trend data available for analysis"]
    # Overall health recommendations
    if overall_health == "excellent":
        recommendations.append("Strong trend momentum - excellent time for market entry")
    elif overall_health == "good":
        recommendations.append("Positive trend indicators - favorable market conditions")
    elif overall_health == "fair":
        recommendations.append("Mixed trend signals - proceed with caution and monitor closely")
    else:
        recommendations.append("Weak trend indicators - consider alternative niches or timing")
    # Specific trend recommendations
    rising_trends = [t for t in trends if t.direction == 'rising']
    if rising_trends:
        top_rising = max(rising_trends, key=lambda t: t.trend_score)
        recommendations.append(
            f"Focus on '{top_rising.keyword}' - strongest rising trend with {top_rising.trend_score:.1f} score"
        )
    # Volatility recommendations
    high_volatility = [t for t in trends if t.volatility > 70]
    if high_volatility:
        recommendations.append(
            f"High volatility detected in {len(high_volatility)} keywords - consider risk management"
        )
    # Seasonal recommendations
    seasonal_trends = [t for t in trends if t.peak_months]
    if seasonal_trends:
        current_month = datetime.now().strftime('%b')
        relevant_seasonal = [t for t in seasonal_trends if current_month in t.peak_months]
        if relevant_seasonal:
            recommendations.append(
                f"Current month ({current_month}) is peak season for {len(relevant_seasonal)} keywords"
            )
    # Confidence recommendations
    low_confidence = [t for t in trends if t.confidence_level < 60]
    if low_confidence:
        recommendations.append(
            f"Low confidence in {len(low_confidence)} trend predictions - gather more data"
        )
    return recommendations
def identify_risk_factors(trends: List[TrendData]) -> List[str]:
    """Identify potential risk factors from trend analysis."""
    risk_factors = []
    if not trends:
        return ["Insufficient trend data for risk assessment"]
    # Declining trends
    declining_trends = [t for t in trends if t.direction == 'declining']
    if declining_trends:
        risk_factors.append(
            f"{len(declining_trends)} keywords showing declining trends"
        )
    # High volatility
    volatile_trends = [t for t in trends if t.volatility > 80]
    if volatile_trends:
        risk_factors.append(
            f"High volatility in {len(volatile_trends)} keywords may indicate unstable market"
        )
    # Low trend scores
    weak_trends = [t for t in trends if t.trend_score < 30]
    if weak_trends:
        risk_factors.append(
            f"{len(weak_trends)} keywords have weak trend strength"
        )
    # Seasonal dependency
    seasonal_dependent = [t for t in trends if t.seasonal_pattern and max(t.seasonal_pattern.values()) > 80]
    if seasonal_dependent:
        risk_factors.append(
            f"{len(seasonal_dependent)} keywords are highly seasonal - revenue may fluctuate significantly"
        )
    # Low confidence predictions
    uncertain_forecasts = [t for t in trends if t.confidence_level < 50]
    if uncertain_forecasts:
        risk_factors.append(
            f"Uncertain forecasts for {len(uncertain_forecasts)} keywords"
        )
    return risk_factors
@router.post("/validate", response_model=TrendValidationResponse)
async def validate_trends(
    request: TrendValidationRequest,
    background_tasks: BackgroundTasks,
    agent=Depends(get_agent())
):
    """Validate trends for given keywords.
    This endpoint analyzes trend data for the provided keywords
    using the MCP agent's trend validation tool.
    """
    try:
        logger.info(f"Starting trend validation for keywords: {request.keywords}")
        # Prepare parameters for MCP agent
        mcp_params = {
            "keywords": request.keywords,
            "timeframe": request.timeframe,
            "geo": request.geo,
            "include_seasonality": request.include_seasonality
        }
        # Call MCP agent's trend validation method
        mcp_result = await agent.validate_trends(
            keywords=request.keywords,
            timeframe=request.timeframe,
            geo=request.geo,
            include_seasonality=request.include_seasonality
        )
        # Convert MCP result to API format
        trends = [
            convert_mcp_trend_to_api(trend)
            for trend in mcp_result.get('trends', [])
        ]
        # Assess overall trend health
        overall_health = assess_overall_trend_health(trends)
        # Create visualization charts
        charts = create_trend_charts(trends)
        # Generate recommendations and risk factors
        recommendations = generate_trend_recommendations(trends, overall_health)
        risk_factors = identify_risk_factors(trends)
        # Create analysis metadata
        metadata = AnalysisMetadata(
            execution_time=mcp_result.get('execution_time', 0.0),
            data_sources=mcp_result.get('data_sources', ['google_trends']),
            cache_hit=mcp_result.get('cache_hit', False),
            warnings=mcp_result.get('warnings', [])
        )
        response = TrendValidationResponse(
            trends=trends,
            overall_trend_health=overall_health,
            analysis_metadata=metadata,
            charts=charts,
            recommendations=recommendations,
            risk_factors=risk_factors
        )
        logger.info(f"Trend validation completed. Analyzed {len(trends)} trends")
        return response
    except Exception as e:
        logger.error(f"Error in trend validation: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to validate trends: {str(e)}"
        )
@router.get("/trending-keywords")
async def get_trending_keywords(
    category: str = None,
    geo: str = "US",
    limit: int = 20
):
    """Get currently trending keywords.
    This endpoint returns keywords that are currently trending
    in the specified category and geographic region.
    """
    try:
        logger.info(f"Getting trending keywords for category: {category}, geo: {geo}")
        # Mock trending keywords data
        # In real implementation, this would come from Google Trends API
        trending_keywords = [
            {
                "keyword": "productivity hacks",
                "trend_score": 85,
                "growth_rate": 15.2,
                "search_volume": 12000,
                "category": "Business"
            },
            {
                "keyword": "mindful eating",
                "trend_score": 78,
                "growth_rate": 22.1,
                "search_volume": 8500,
                "category": "Health"
            },
            {
                "keyword": "remote work tips",
                "trend_score": 72,
                "growth_rate": 8.7,
                "search_volume": 15000,
                "category": "Business"
            },
            {
                "keyword": "sustainable living",
                "trend_score": 69,
                "growth_rate": 18.3,
                "search_volume": 9200,
                "category": "Lifestyle"
            },
            {
                "keyword": "digital minimalism",
                "trend_score": 65,
                "growth_rate": 12.9,
                "search_volume": 6800,
                "category": "Self-Help"
            }
        ]
        # Filter by category if specified
        if category:
            trending_keywords = [
                kw for kw in trending_keywords 
                if kw["category"].lower() == category.lower()
            ]
        # Limit results
        trending_keywords = trending_keywords[:limit]
        return {
            "trending_keywords": trending_keywords,
            "category": category,
            "geo": geo,
            "total_found": len(trending_keywords),
            "last_updated": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting trending keywords: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get trending keywords: {str(e)}"
        )
@router.get("/forecast/{keyword}")
async def get_keyword_forecast(
    keyword: str,
    months: int = 3,
    geo: str = "US",
    agent=Depends(get_agent())
):
    """Get trend forecast for a specific keyword.
    This endpoint provides a trend forecast for the specified keyword
    over the requested time period.
    """
    try:
        logger.info(f"Getting forecast for keyword: {keyword}")
        # Mock forecast data
        # In real implementation, this would use trend analysis algorithms
        base_score = 65
        forecast_data = []
        for i in range(months):
            # Simulate trend variation
            variation = (-1) ** i * (i * 2)
            score = max(0, min(100, base_score + variation))
            forecast_data.append({
                "month": (datetime.now() + timedelta(days=30*i)).strftime("%Y-%m"),
                "predicted_score": score,
                "confidence": max(50, 90 - i * 10),  # Decreasing confidence over time
                "trend_direction": "rising" if score > base_score else "declining"
            })
        return {
            "keyword": keyword,
            "forecast_period": f"{months} months",
            "geo": geo,
            "forecast": forecast_data,
            "methodology": "Statistical trend analysis with seasonal adjustments",
            "last_updated": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting keyword forecast: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get keyword forecast: {str(e)}"
        )
@router.get("/seasonal-patterns")
async def get_seasonal_patterns(
    keywords: List[str] = None,
    years: int = 2
):
    """Get seasonal patterns for keywords.
    This endpoint analyzes seasonal patterns in search trends
    for the specified keywords over the given time period.
    """
    try:
        if not keywords:
            keywords = ["fitness", "diet", "travel", "gifts", "gardening"]
        logger.info(f"Getting seasonal patterns for keywords: {keywords}")
        # Mock seasonal pattern data
        patterns = {}
        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        for keyword in keywords:
            # Generate mock seasonal data based on keyword type
            if "fitness" in keyword.lower() or "diet" in keyword.lower():
                # Peak in January (New Year resolutions)
                pattern = [100, 80, 60, 50, 45, 40, 35, 40, 50, 60, 70, 85]
            elif "travel" in keyword.lower():
                # Peak in summer months
                pattern = [40, 45, 60, 70, 85, 100, 95, 90, 75, 60, 45, 50]
            elif "gift" in keyword.lower():
                # Peak in December
                pattern = [30, 25, 35, 40, 50, 45, 40, 45, 55, 70, 85, 100]
            else:
                # Relatively stable with slight variations
                pattern = [60, 55, 65, 70, 75, 80, 75, 70, 65, 60, 65, 70]
            patterns[keyword] = {
                "monthly_scores": dict(zip(months, pattern)),
                "peak_month": months[pattern.index(max(pattern))],
                "low_month": months[pattern.index(min(pattern))],
                "volatility": (max(pattern) - min(pattern)) / max(pattern) * 100,
                "trend_type": "seasonal" if max(pattern) - min(pattern) > 30 else "stable"
            }
        return {
            "seasonal_patterns": patterns,
            "analysis_period": f"{years} years",
            "insights": [
                "Fitness keywords peak in January due to New Year resolutions",
                "Travel keywords show strong summer seasonality",
                "Gift-related keywords spike during holiday season"
            ]
        }
    except Exception as e:
        logger.error(f"Error getting seasonal patterns: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get seasonal patterns: {str(e)}"
        )
</file>

<file path="config/logging.conf">
[loggers]
keys=root,kdp_strategist,keepa_client,trends_client,cache_manager

[handlers]
keys=consoleHandler,fileHandler,structuredHandler

[formatters]
keys=simpleFormatter,detailedFormatter,structuredFormatter

[logger_root]
level=INFO
handlers=consoleHandler

[logger_kdp_strategist]
level=INFO
handlers=consoleHandler,fileHandler
qualname=kdp_strategist
propagate=0

[logger_keepa_client]
level=INFO
handlers=consoleHandler,fileHandler
qualname=kdp_strategist.data.keepa_client
propagate=0

[logger_trends_client]
level=INFO
handlers=consoleHandler,fileHandler
qualname=kdp_strategist.data.trends_client
propagate=0

[logger_cache_manager]
level=DEBUG
handlers=consoleHandler,fileHandler
qualname=kdp_strategist.data.cache_manager
propagate=0

[handler_consoleHandler]
class=StreamHandler
level=INFO
formatter=simpleFormatter
args=(sys.stdout,)

[handler_fileHandler]
class=handlers.RotatingFileHandler
level=DEBUG
formatter=detailedFormatter
args=('logs/kdp_strategist.log', 'a', 10485760, 5)

[handler_structuredHandler]
class=StreamHandler
level=INFO
formatter=structuredFormatter
args=(sys.stdout,)

[formatter_simpleFormatter]
format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
datefmt=%Y-%m-%d %H:%M:%S

[formatter_detailedFormatter]
format=%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s
datefmt=%Y-%m-%d %H:%M:%S

[formatter_structuredFormatter]
format=%(message)s
datefmt=%Y-%m-%d %H:%M:%S
</file>

<file path="config/settings.py">
"""Configuration management for KDP Strategist AI Agent.
This module handles all configuration settings including:
- API keys and credentials
- Cache settings
- Rate limiting parameters
- Model configurations
- Logging settings
"""
import os
from typing import Optional, Dict, Any
from dataclasses import dataclass, field
from pathlib import Path
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass
@dataclass
class APIConfig:
    """Configuration for external API integrations."""
    keepa_api_key: Optional[str] = field(default_factory=lambda: os.getenv("KEEPA_API_KEY"))
    keepa_rate_limit: int = field(default=60)  # requests per minute
    trends_rate_limit: int = field(default=30)  # requests per minute
    request_timeout: int = field(default=30)  # seconds
    max_retries: int = field(default=3)
    retry_delay: float = field(default=1.0)  # seconds
@dataclass
class CacheConfig:
    """Configuration for data caching system."""
    cache_type: str = field(default="file")  # "file", "redis", "memory"
    cache_dir: Path = field(default_factory=lambda: Path("cache"))
    redis_url: Optional[str] = field(default_factory=lambda: os.getenv("REDIS_URL"))
    default_ttl: int = field(default=3600)  # seconds (1 hour)
    trends_ttl: int = field(default=86400)  # seconds (24 hours)
    keepa_ttl: int = field(default=3600)  # seconds (1 hour)
    max_cache_size: int = field(default=1000)  # max items in memory cache
@dataclass
class ModelConfig:
    """Configuration for ML models and embeddings."""
    embedding_model: str = field(default="sentence-transformers/all-MiniLM-L6-v2")
    embedding_cache_size: int = field(default=10000)
    similarity_threshold: float = field(default=0.7)
    max_keywords_per_query: int = field(default=50)
    trend_analysis_window: int = field(default=365)  # days
@dataclass
class BusinessConfig:
    """Configuration for business logic and scoring."""
    min_profitability_score: float = field(default=50.0)
    max_competition_score: float = field(default=70.0)
    trend_weight: float = field(default=0.4)
    competition_weight: float = field(default=0.3)
    profitability_weight: float = field(default=0.3)
    max_niches_per_query: int = field(default=10)
    min_monthly_searches: int = field(default=1000)
@dataclass
class LoggingConfig:
    """Configuration for logging system."""
    level: str = field(default="INFO")
    format: str = field(default="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    file_path: Optional[Path] = field(default=None)
    max_file_size: int = field(default=10 * 1024 * 1024)  # 10MB
    backup_count: int = field(default=5)
    enable_structured_logging: bool = field(default=True)
@dataclass
class MCPConfig:
    """Configuration for Model Context Protocol integration."""
    server_name: str = field(default="kdp_strategist")
    server_version: str = field(default="0.1.0")
    transport_type: str = field(default="stdio")  # "stdio", "sse", "websocket"
    host: str = field(default="localhost")
    port: int = field(default=8000)
    enable_cors: bool = field(default=True)
    max_request_size: int = field(default=1024 * 1024)  # 1MB
@dataclass
class Settings:
    """Main configuration class containing all settings."""
    api: APIConfig = field(default_factory=APIConfig)
    cache: CacheConfig = field(default_factory=CacheConfig)
    models: ModelConfig = field(default_factory=ModelConfig)
    business: BusinessConfig = field(default_factory=BusinessConfig)
    logging: LoggingConfig = field(default_factory=LoggingConfig)
    mcp: MCPConfig = field(default_factory=MCPConfig)
    # Environment settings
    environment: str = field(default_factory=lambda: os.getenv("ENVIRONMENT", "development"))
    debug: bool = field(default_factory=lambda: os.getenv("DEBUG", "false").lower() == "true")
    def __post_init__(self):
        """Validate configuration after initialization."""
        self._validate_config()
        self._setup_directories()
    def _validate_config(self) -> None:
        """Validate configuration settings."""
        if self.environment == "production" and not self.api.keepa_api_key:
            raise ValueError("KEEPA_API_KEY is required in production environment")
        if self.cache.cache_type == "redis" and not self.cache.redis_url:
            raise ValueError("REDIS_URL is required when using Redis cache")
        if self.business.trend_weight + self.business.competition_weight + self.business.profitability_weight != 1.0:
            raise ValueError("Business weights must sum to 1.0")
    def _setup_directories(self) -> None:
        """Create necessary directories."""
        if self.cache.cache_type == "file":
            self.cache.cache_dir.mkdir(parents=True, exist_ok=True)
        if self.logging.file_path:
            self.logging.file_path.parent.mkdir(parents=True, exist_ok=True)
    def to_dict(self) -> Dict[str, Any]:
        """Convert settings to dictionary."""
        return {
            "api": self.api.__dict__,
            "cache": {**self.cache.__dict__, "cache_dir": str(self.cache.cache_dir)},
            "models": self.models.__dict__,
            "business": self.business.__dict__,
            "logging": {**self.logging.__dict__, "file_path": str(self.logging.file_path) if self.logging.file_path else None},
            "mcp": self.mcp.__dict__,
            "environment": self.environment,
            "debug": self.debug,
        }
    @classmethod
    def from_env(cls) -> "Settings":
        """Create settings from environment variables."""
        return cls()
    @classmethod
    def from_file(cls, config_path: Path) -> "Settings":
        """Load settings from configuration file."""
        import yaml
        with open(config_path, "r") as f:
            config_data = yaml.safe_load(f)
        # Create settings with loaded data
        # This is a simplified implementation - in practice, you'd want
        # more sophisticated config file parsing
        return cls()
# Global settings instance
settings = Settings.from_env()
</file>

<file path="frontend/package.json">
{
  "name": "kdp_strategist-frontend",
  "version": "1.0.0",
  "description": "React frontend for KDP Strategist AI Agent",
  "private": true,
  "dependencies": {
    "@headlessui/react": "^1.7.17",
    "@heroicons/react": "^2.0.18",
    "@tailwindcss/aspect-ratio": "^0.4.2",
    "@tailwindcss/forms": "^0.5.10",
    "@tailwindcss/typography": "^0.5.16",
    "@testing-library/jest-dom": "^5.17.0",
    "@testing-library/react": "^13.4.0",
    "@testing-library/user-event": "^13.5.0",
    "chart.js": "^4.4.0",
    "chartjs-adapter-date-fns": "^3.0.0",
    "clsx": "^2.1.1",
    "date-fns": "^2.30.0",
    "lucide-react": "^0.525.0",
    "react": "^18.2.0",
    "react-chartjs-2": "^5.2.0",
    "react-dom": "^18.2.0",
    "react-router-dom": "^6.16.0",
    "react-scripts": "5.0.1",
    "react-toastify": "^9.1.3",
    "recharts": "^3.1.0",
    "tailwind-merge": "^3.3.1",
    "web-vitals": "^2.1.4"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test",
    "eject": "react-scripts eject",
    "lint": "eslint src --ext .js,.jsx,.ts,.tsx",
    "lint:fix": "eslint src --ext .js,.jsx,.ts,.tsx --fix",
    "format": "prettier --write src/**/*.{js,jsx,ts,tsx,css,md}"
  },
  "eslintConfig": {
    "extends": [
      "react-app",
      "react-app/jest"
    ]
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  },
  "devDependencies": {
    "@types/jest": "^27.5.2",
    "@types/node": "^16.18.58",
    "@types/react": "^18.2.25",
    "@types/react-dom": "^18.2.11",
    "autoprefixer": "^10.4.16",
    "eslint": "^8.51.0",
    "eslint-config-prettier": "^9.0.0",
    "eslint-plugin-prettier": "^5.0.1",
    "postcss": "^8.4.31",
    "prettier": "^3.0.3",
    "tailwindcss": "^3.3.5",
    "typescript": "^4.9.5"
  },
  "proxy": "http://localhost:8001"
}
</file>

<file path="frontend/public/index.html">
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="KDP Strategist - AI-powered tool for Amazon KDP market analysis, niche discovery, and listing optimization"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!-- Preconnect to external domains for performance -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- Meta tags for SEO -->
    <meta name="keywords" content="KDP, Amazon KDP, book publishing, niche research, market analysis, AI tools" />
    <meta name="author" content="KDP Strategist" />
    <!-- Open Graph meta tags -->
    <meta property="og:title" content="KDP Strategist - AI-Powered Publishing Intelligence" />
    <meta property="og:description" content="Discover profitable niches, analyze competitors, and optimize your KDP listings with AI-powered insights." />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="%PUBLIC_URL%" />
    <meta property="og:image" content="%PUBLIC_URL%/og-image.png" />
    <!-- Twitter Card meta tags -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="KDP Strategist - AI-Powered Publishing Intelligence" />
    <meta name="twitter:description" content="Discover profitable niches, analyze competitors, and optimize your KDP listings with AI-powered insights." />
    <meta name="twitter:image" content="%PUBLIC_URL%/twitter-image.png" />
    <title>KDP Strategist - AI-Powered Publishing Intelligence</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!-- Loading fallback -->
    <div id="loading-fallback" style="display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: #f9fafb; z-index: 9999; display: flex; align-items: center; justify-content: center; flex-direction: column;">
      <div style="width: 40px; height: 40px; border: 4px solid #e5e7eb; border-top: 4px solid #3b82f6; border-radius: 50%; animation: spin 1s linear infinite;"></div>
      <p style="margin-top: 16px; color: #6b7280; font-family: system-ui, -apple-system, sans-serif;">Loading KDP Strategist...</p>
    </div>
    <style>
      @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
      }
      /* Hide loading fallback when React loads */
      #root:not(:empty) ~ #loading-fallback {
        display: none !important;
      }
    </style>
    <!-- Error boundary fallback -->
    <script>
      window.addEventListener('error', function(e) {
        console.error('Global error:', e.error);
        // You could send this to an error reporting service
      });
      window.addEventListener('unhandledrejection', function(e) {
        console.error('Unhandled promise rejection:', e.reason);
        // You could send this to an error reporting service
      });
    </script>
  </body>
</html>
</file>

<file path="frontend/src/App.css">
@tailwind base;
@tailwind components;
@tailwind utilities;
/* Import Inter font */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
@import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&display=swap');
/* Base styles */
@layer base {
  html {
    font-family: 'Inter', system-ui, sans-serif;
  }
  body {
    @apply bg-gray-50 text-gray-900 antialiased;
  }
  /* Custom scrollbar */
  ::-webkit-scrollbar {
    @apply w-2;
  }
  ::-webkit-scrollbar-track {
    @apply bg-gray-100;
  }
  ::-webkit-scrollbar-thumb {
    @apply bg-gray-300 rounded-full;
  }
  ::-webkit-scrollbar-thumb:hover {
    @apply bg-gray-400;
  }
}
/* Component styles */
@layer components {
  /* Button variants */
  .btn {
    @apply inline-flex items-center justify-center px-4 py-2 border border-transparent text-sm font-medium rounded-md shadow-sm transition-colors duration-200 focus:outline-none focus:ring-2 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed;
  }
  .btn-primary {
    @apply btn bg-primary-600 text-white hover:bg-primary-700 focus:ring-primary-500;
  }
  .btn-secondary {
    @apply btn bg-white text-gray-700 border-gray-300 hover:bg-gray-50 focus:ring-primary-500;
  }
  .btn-success {
    @apply btn bg-success-600 text-white hover:bg-success-700 focus:ring-success-500;
  }
  .btn-warning {
    @apply btn bg-warning-600 text-white hover:bg-warning-700 focus:ring-warning-500;
  }
  .btn-danger {
    @apply btn bg-danger-600 text-white hover:bg-danger-700 focus:ring-danger-500;
  }
  .btn-sm {
    @apply px-3 py-1.5 text-xs;
  }
  .btn-lg {
    @apply px-6 py-3 text-base;
  }
  /* Card styles */
  .card {
    @apply bg-white rounded-lg shadow-soft border border-gray-200;
  }
  .card-header {
    @apply px-6 py-4 border-b border-gray-200;
  }
  .card-body {
    @apply px-6 py-4;
  }
  .card-footer {
    @apply px-6 py-4 border-t border-gray-200 bg-gray-50 rounded-b-lg;
  }
  /* Form styles */
  .form-input {
    @apply block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary-500 focus:border-primary-500 sm:text-sm;
  }
  .form-select {
    @apply block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-primary-500 focus:border-primary-500 sm:text-sm;
  }
  .form-textarea {
    @apply block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary-500 focus:border-primary-500 sm:text-sm;
  }
  .form-label {
    @apply block text-sm font-medium text-gray-700 mb-1;
  }
  .form-error {
    @apply mt-1 text-sm text-danger-600;
  }
  .form-help {
    @apply mt-1 text-sm text-gray-500;
  }
  /* Badge styles */
  .badge {
    @apply inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium;
  }
  .badge-primary {
    @apply badge bg-primary-100 text-primary-800;
  }
  .badge-secondary {
    @apply badge bg-gray-100 text-gray-800;
  }
  .badge-success {
    @apply badge bg-success-100 text-success-800;
  }
  .badge-warning {
    @apply badge bg-warning-100 text-warning-800;
  }
  .badge-danger {
    @apply badge bg-danger-100 text-danger-800;
  }
  /* Alert styles */
  .alert {
    @apply p-4 rounded-md;
  }
  .alert-info {
    @apply alert bg-primary-50 border border-primary-200 text-primary-800;
  }
  .alert-success {
    @apply alert bg-success-50 border border-success-200 text-success-800;
  }
  .alert-warning {
    @apply alert bg-warning-50 border border-warning-200 text-warning-800;
  }
  .alert-danger {
    @apply alert bg-danger-50 border border-danger-200 text-danger-800;
  }
  /* Loading spinner */
  .spinner {
    @apply animate-spin rounded-full border-2 border-gray-300 border-t-primary-600;
  }
  /* Chart container */
  .chart-container {
    @apply relative w-full h-64 md:h-80 lg:h-96;
  }
  /* Data table */
  .data-table {
    @apply min-w-full divide-y divide-gray-200;
  }
  .data-table thead {
    @apply bg-gray-50;
  }
  .data-table th {
    @apply px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider;
  }
  .data-table td {
    @apply px-6 py-4 whitespace-nowrap text-sm text-gray-900;
  }
  .data-table tbody tr:nth-child(even) {
    @apply bg-gray-50;
  }
  .data-table tbody tr:hover {
    @apply bg-gray-100;
  }
  /* Sidebar styles */
  .sidebar {
    @apply fixed inset-y-0 left-0 z-50 w-64 bg-white shadow-lg transform transition-transform duration-300 ease-in-out;
  }
  .sidebar-open {
    @apply translate-x-0;
  }
  .sidebar-closed {
    @apply -translate-x-full;
  }
  /* Navigation styles */
  .nav-item {
    @apply flex items-center px-4 py-2 text-sm font-medium rounded-md transition-colors duration-200;
  }
  .nav-item-active {
    @apply nav-item bg-primary-100 text-primary-700;
  }
  .nav-item-inactive {
    @apply nav-item text-gray-600 hover:bg-gray-100 hover:text-gray-900;
  }
  /* Dropdown styles */
  .dropdown {
    @apply absolute right-0 z-10 mt-2 w-48 origin-top-right rounded-md bg-white py-1 shadow-lg ring-1 ring-black ring-opacity-5 focus:outline-none;
  }
  .dropdown-item {
    @apply block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100 transition-colors duration-200;
  }
  /* Modal styles */
  .modal-overlay {
    @apply fixed inset-0 z-50 overflow-y-auto;
  }
  .modal-backdrop {
    @apply fixed inset-0 bg-gray-500 bg-opacity-75 transition-opacity;
  }
  .modal-container {
    @apply flex min-h-full items-end justify-center p-4 text-center sm:items-center sm:p-0;
  }
  .modal-content {
    @apply relative transform overflow-hidden rounded-lg bg-white text-left shadow-xl transition-all sm:my-8 sm:w-full sm:max-w-lg;
  }
  /* Tooltip styles */
  .tooltip {
    @apply absolute z-10 px-2 py-1 text-xs font-medium text-white bg-gray-900 rounded shadow-lg;
  }
  /* Progress bar */
  .progress-bar {
    @apply w-full bg-gray-200 rounded-full h-2;
  }
  .progress-fill {
    @apply h-2 bg-primary-600 rounded-full transition-all duration-300;
  }
}
/* Utility styles */
@layer utilities {
  /* Text utilities */
  .text-gradient {
    @apply bg-gradient-to-r from-primary-600 to-primary-800 bg-clip-text text-transparent;
  }
  /* Animation utilities - defined in tailwind.config.js */
  /* Layout utilities */
  .container-fluid {
    @apply w-full max-w-none px-4 sm:px-6 lg:px-8;
  }
  .container-narrow {
    @apply max-w-4xl mx-auto px-4 sm:px-6 lg:px-8;
  }
  /* Responsive grid */
  .grid-responsive {
    @apply grid grid-cols-1 gap-6 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4;
  }
  /* Focus styles */
  .focus-ring {
    @apply focus:outline-none focus:ring-2 focus:ring-primary-500 focus:ring-offset-2;
  }
  /* Hover effects */
  .hover-lift {
    @apply transition-transform duration-200 hover:-translate-y-1;
  }
  .hover-scale {
    @apply transition-transform duration-200 hover:scale-105;
  }
}
/* Custom animations - moved to tailwind.config.js to avoid duplication */
/* Print styles */
@media print {
  .no-print {
    display: none !important;
  }
  .print-break {
    page-break-before: always;
  }
  .print-avoid-break {
    page-break-inside: avoid;
  }
}
/* Dark mode styles (if needed in future) */
@media (prefers-color-scheme: dark) {
  .dark-mode {
    @apply bg-gray-900 text-gray-100;
  }
}
</file>

<file path="frontend/src/App.js">
import React, { useState, useEffect, createContext, useContext } from 'react';
import { BrowserRouter as Router, Routes, Route, Navigate } from 'react-router-dom';
import { ToastContainer } from 'react-toastify';
import 'react-toastify/dist/ReactToastify.css';
// Components
import Navbar from './components/layout/Navbar';
import Sidebar from './components/layout/Sidebar';
import LoadingSpinner from './components/common/LoadingSpinner';
import ErrorBoundary from './components/common/ErrorBoundary';
import { cn } from './lib/utils';
// Pages
import Dashboard from './pages/Dashboard';
import NicheDiscovery from './pages/NicheDiscovery';
import CompetitorAnalysis from './pages/CompetitorAnalysis';
import ListingGeneration from './pages/ListingGeneration';
import TrendValidation from './pages/TrendValidation';
import StressTesting from './pages/StressTesting';
import Settings from './pages/Settings';
// Services
import { apiService } from './services/api';
// Styles
import './App.css';
// Create API Context
const ApiContext = createContext({
  apiService: null,
  updateApiSettings: () => {},
  isConnected: false,
  refreshConnection: () => {}
});
// Custom hook to use API context
export const useApi = () => {
  const context = useContext(ApiContext);
  if (!context) {
    throw new Error('useApi must be used within an ApiProvider');
  }
  return context;
};
function App() {
  const [isLoading, setIsLoading] = useState(true);
  const [isConnected, setIsConnected] = useState(false);
  const [sidebarOpen, setSidebarOpen] = useState(true);
  const [currentUser, setCurrentUser] = useState(null);
  const [apiSettings, setApiSettings] = useState({ api_key: '', api_endpoint: '' });
  useEffect(() => {
    // Initialize the application
    initializeApp();
  }, []);
  const initializeApp = async () => {
    try {
      setIsLoading(true);
      // Load and apply saved API settings
      const savedSettings = localStorage.getItem('kdp_strategist_settings');
      if (savedSettings) {
        try {
          const parsed = JSON.parse(savedSettings);
          const newApiSettings = {
            api_key: parsed.api_key || '',
            api_endpoint: parsed.api_endpoint || ''
          };
          setApiSettings(newApiSettings);
          if (parsed.api_key) {
            apiService.setApiKey(parsed.api_key);
          }
          if (parsed.api_endpoint) {
            apiService.setBaseUrl(parsed.api_endpoint);
          }
        } catch (error) {
          console.error('Error loading saved settings:', error);
        }
      }
      // Check API health
      const healthCheck = await apiService.checkHealth();
      setIsConnected(healthCheck.status === 'healthy');
      // Initialize user session (if needed)
      // For now, we'll use a default user
      setCurrentUser({
        id: 'default',
        name: 'KDP Strategist User',
        preferences: {
          theme: 'light',
          defaultExportFormat: 'csv'
        }
      });
    } catch (error) {
      console.error('Failed to initialize app:', error);
      setIsConnected(false);
    } finally {
      setIsLoading(false);
    }
  };
  // Function to update API settings from Settings component
  const updateApiSettings = async (newSettings) => {
    try {
      const { api_key, api_endpoint } = newSettings;
      // Update local state
      setApiSettings({ api_key, api_endpoint });
      // Apply to apiService
      if (api_key) {
        apiService.setApiKey(api_key);
      } else {
        apiService.setApiKey(null);
      }
      if (api_endpoint) {
        apiService.setBaseUrl(api_endpoint);
      }
      // Test connection
      const healthCheck = await apiService.checkHealth();
      setIsConnected(healthCheck.status === 'healthy');
      return { success: true, connected: healthCheck.status === 'healthy' };
    } catch (error) {
      console.error('Error updating API settings:', error);
      setIsConnected(false);
      return { success: false, error: error.message };
    }
  };
  // Function to refresh connection
  const refreshConnection = async () => {
    try {
      const healthCheck = await apiService.checkHealth();
      setIsConnected(healthCheck.status === 'healthy');
      return healthCheck.status === 'healthy';
    } catch (error) {
      console.error('Connection refresh failed:', error);
      setIsConnected(false);
      return false;
    }
  };
  const toggleSidebar = () => {
    setSidebarOpen(!sidebarOpen);
  };
  if (isLoading) {
    return (
      <div className="min-h-screen bg-gray-50 flex items-center justify-center">
        <LoadingSpinner size="large" message="Initializing KDP Strategist..." />
      </div>
    );
  }
  if (!isConnected) {
    return (
      <div className="min-h-screen bg-gray-50 flex items-center justify-center">
        <div className="text-center">
          <div className="mb-4">
            <svg className="mx-auto h-12 w-12 text-red-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-2.5L13.732 4c-.77-.833-1.964-.833-2.732 0L3.732 16.5c-.77.833.192 2.5 1.732 2.5z" />
            </svg>
          </div>
          <h3 className="mt-2 text-sm font-medium text-gray-900">Connection Failed</h3>
          <p className="mt-1 text-sm text-gray-500">
            Unable to connect to the KDP Strategist API. Please ensure the backend server is running.
          </p>
          <div className="mt-6">
            <button
              onClick={initializeApp}
              className="inline-flex items-center px-4 py-2 border border-transparent shadow-sm text-sm font-medium rounded-md text-white bg-blue-600 hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500"
            >
              Retry Connection
            </button>
          </div>
        </div>
      </div>
    );
  }
  // API Context value
  const apiContextValue = {
    apiService,
    updateApiSettings,
    isConnected,
    refreshConnection,
    apiSettings
  };
  return (
    <ErrorBoundary>
      <ApiContext.Provider value={apiContextValue}>
        <Router>
          <div className="flex min-h-screen bg-gray-50">
            <Sidebar isOpen={sidebarOpen} toggleSidebar={toggleSidebar} />
            <div className={cn("flex flex-col flex-1 transition-all duration-300 ease-in-out", sidebarOpen ? 'ml-64' : 'ml-0')}>
              <Navbar toggleSidebar={toggleSidebar} isSidebarOpen={sidebarOpen} />
              <main className="flex-1 p-6 overflow-auto">
                <Routes>
                  <Route path="/" element={<Dashboard />} />
                  <Route path="/niche-discovery" element={<NicheDiscovery />} />
                  <Route path="/competitor-analysis" element={<CompetitorAnalysis />} />
                  <Route path="/listing-generation" element={<ListingGeneration />} />
                  <Route path="/trend-validation" element={<TrendValidation />} />
                  <Route path="/stress-testing" element={<StressTesting />} />
                  <Route path="/settings" element={<Settings />} />
                  <Route path="*" element={<Navigate to="/" replace />} />
                </Routes>
              </main>
            </div>
          </div>
          <ToastContainer position="bottom-right" autoClose={5000} hideProgressBar={false} newestOnTop={false} closeOnClick rtl={false} pauseOnFocusLoss draggable pauseOnHover />
        </Router>
      </ApiContext.Provider>
    </ErrorBoundary>
  );
}
            {/* Navigation */}
            <div className="flex">
            {/* Sidebar */}
            <Sidebar 
              isOpen={sidebarOpen}
              onClose={() => setSidebarOpen(false)}
            />
            {/* Main Content Area */}
            <div className={`flex-1 transition-all duration-300 ${sidebarOpen ? 'ml-64' : 'ml-0'}`}>
              <Navbar
                onToggleSidebar={toggleSidebar}
                currentUser={currentUser}
                isConnected={isConnected}
                sidebarOpen={sidebarOpen}
              />
              <main className="pt-16">
                <div className="py-6">
                  <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
                    <Routes>
                      <Route path="/" element={<Navigate to="/dashboard" replace />} />
                      <Route path="/dashboard" element={<Dashboard />} />
                      <Route path="/niche-discovery" element={<NicheDiscovery />} />
                      <Route path="/competitor-analysis" element={<CompetitorAnalysis />} />
                      <Route path="/listing-generation" element={<ListingGeneration />} />
                      <Route path="/trend-validation" element={<TrendValidation />} />
                      <Route path="/stress-testing" element={<StressTesting />} />
                      <Route path="/settings" element={<Settings />} />
                      <Route path="*" element={<Navigate to="/dashboard" replace />} />
                    </Routes>
                  </div>
                </div>
              </main>
            </div>
            </div>
            {/* Toast Notifications */}
            <ToastContainer
              position="top-right"
              autoClose={5000}
              hideProgressBar={false}
              newestOnTop={false}
              closeOnClick
              rtl={false}
              pauseOnFocusLoss
              draggable
              pauseOnHover
              theme="light"
            />
          </div>
        </Router>
      </ApiContext.Provider>
    </ErrorBoundary>
  );
}
export default App;
</file>

<file path="frontend/src/index.js">
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App';
import reportWebVitals from './reportWebVitals';
// Create root element
const root = ReactDOM.createRoot(document.getElementById('root'));
// Render the app
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);
// If you want to start measuring performance in your app, pass a function
// to log results (for example: reportWebVitals(console.log))
// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
reportWebVitals();
</file>

<file path="frontend/src/pages/Dashboard.jsx">
import React, { useState, useEffect } from 'react';
import { Link } from 'react-router-dom';
import {
  MagnifyingGlassIcon,
  ChartBarIcon,
  DocumentTextIcon,
  TrendingUpIcon,
  ShieldCheckIcon,
  ArrowRightIcon,
  EyeIcon,
  CurrencyDollarIcon,
  StarIcon
} from '@heroicons/react/24/outline';
import LoadingSpinner from '../components/common/LoadingSpinner';
import ChartContainer from '../components/common/ChartContainer';
import { useApi } from '../App';
import { toast } from 'react-toastify';
const Dashboard = () => {
  const [stats, setStats] = useState(null);
  const [recentActivity, setRecentActivity] = useState([]);
  const [isLoading, setIsLoading] = useState(true);
  const [error, setError] = useState(null);
  const { apiService } = useApi();
  useEffect(() => {
    const loadDashboardData = async () => {
      setIsLoading(true);
      setError(null);
      try {
        // Fetch real dashboard stats and activity from the backend
        const dashboardStats = await apiService.get('/api/dashboard/stats');
        const activityData = await apiService.get('/api/dashboard/activity');
        setStats(dashboardStats);
        setRecentActivity(activityData.recent_activities);
        toast.success('Dashboard data loaded successfully!');
      } catch (err) {
        console.error('Failed to load dashboard data:', err);
        setError(apiService.formatError(err));
        toast.error(`Failed to load dashboard data: ${apiService.formatError(err)}`);
        // Fallback to empty data on error
        setStats({
          totalAnalyses: 0,
          successfulListings: 0,
          averageScore: 0,
          trendsValidated: 0
        });
        setRecentActivity([]);
      } finally {
        setIsLoading(false);
      }
    };
    loadDashboardData();
  }, [apiService]);
  const quickActions = [
    {
      name: 'Discover Niches',
      description: 'Find profitable book niches',
      href: '/niche-discovery',
      icon: MagnifyingGlassIcon,
      color: 'bg-blue-500 hover:bg-blue-600'
    },
    {
      name: 'Analyze Competitors',
      description: 'Research competitor performance',
      href: '/competitor-analysis',
      icon: ChartBarIcon,
      color: 'bg-green-500 hover:bg-green-600'
    },
    {
      name: 'Generate Listing',
      description: 'Create optimized book listings',
      href: '/listing-generation',
      icon: DocumentTextIcon,
      color: 'bg-purple-500 hover:bg-purple-600'
    },
    {
      name: 'Validate Trends',
      description: 'Check market trend validity',
      href: '/trend-validation',
      icon: TrendingUpIcon,
      color: 'bg-orange-500 hover:bg-orange-600'
    }
  ];
  const getActivityIcon = (type) => {
    switch (type) {
      case 'niche_discovery':
        return MagnifyingGlassIcon;
      case 'competitor_analysis':
        return ChartBarIcon;
      case 'listing_generation':
        return DocumentTextIcon;
      case 'trend_validation':
        return TrendingUpIcon;
      default:
        return EyeIcon;
    }
  };
  const formatTimestamp = (timestamp) => {
    try {
      const date = new Date(timestamp);
      const now = new Date();
      const diffInHours = Math.floor((now - date) / (1000 * 60 * 60));
      if (diffInHours < 1) {
        return 'Just now';
      } else if (diffInHours < 24) {
        return `${diffInHours} hour${diffInHours > 1 ? 's' : ''} ago`;
      } else {
        const diffInDays = Math.floor(diffInHours / 24);
        return `${diffInDays} day${diffInDays > 1 ? 's' : ''} ago`;
      }
    } catch (error) {
      return timestamp; // Fallback to original timestamp if parsing fails
    }
  };
  const getScoreColor = (score) => {
    if (score >= 80) return 'text-green-600 bg-green-100';
    if (score >= 60) return 'text-yellow-600 bg-yellow-100';
    return 'text-red-600 bg-red-100';
  };
  // Mock chart data
  const performanceChart = {
    type: 'line',
    title: 'Analysis Performance Trend',
    data: {
      labels: ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],
      datasets: [
        {
          label: 'Average Score',
          data: [65, 72, 68, 78, 82, 85],
          borderColor: 'rgba(59, 130, 246, 1)',
          backgroundColor: 'rgba(59, 130, 246, 0.1)',
          fill: true
        }
      ]
    }
  };
  if (isLoading) {
    return (
      <div className="min-h-screen bg-gray-50 flex items-center justify-center">
        <LoadingSpinner size="large" />
      </div>
    );
  }
  return (
    <div className="animate-fade-in">
      {/* Header */}
      <div className="mb-8">
        <h1 className="text-3xl font-bold text-gray-900">Dashboard</h1>
        <p className="text-gray-600 mt-2">
          Welcome back! Here's an overview of your KDP strategy performance.
        </p>
      </div>
      {/* Error Display */}
      {error && (
        <div className="mb-8 bg-red-50 border border-red-200 rounded-md p-4">
          <div className="flex">
            <div className="flex-shrink-0">
              <svg className="h-5 w-5 text-red-400" viewBox="0 0 20 20" fill="currentColor">
                <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8.707 7.293a1 1 0 00-1.414 1.414L8.586 10l-1.293 1.293a1 1 0 101.414 1.414L10 11.414l1.293 1.293a1 1 0 001.414-1.414L11.414 10l1.293-1.293a1 1 0 00-1.414-1.414L10 8.586 8.707 7.293z" clipRule="evenodd" />
              </svg>
            </div>
            <div className="ml-3">
              <h3 className="text-sm font-medium text-red-800">Error Loading Dashboard Data</h3>
              <div className="mt-2 text-sm text-red-700">
                <p>{error}</p>
              </div>
            </div>
          </div>
        </div>
      )}
      {/* Stats Grid */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6 mb-8">
        <div className="card">
          <div className="card-body">
            <div className="flex items-center">
              <div className="flex-shrink-0">
                <EyeIcon className="h-8 w-8 text-blue-600" />
              </div>
              <div className="ml-4">
                <p className="text-sm font-medium text-gray-500">Total Analyses</p>
                <p className="text-2xl font-bold text-gray-900">{stats?.totalAnalyses}</p>
              </div>
            </div>
          </div>
        </div>
        <div className="card">
          <div className="card-body">
            <div className="flex items-center">
              <div className="flex-shrink-0">
                <DocumentTextIcon className="h-8 w-8 text-green-600" />
              </div>
              <div className="ml-4">
                <p className="text-sm font-medium text-gray-500">Successful Listings</p>
                <p className="text-2xl font-bold text-gray-900">{stats?.successfulListings}</p>
              </div>
            </div>
          </div>
        </div>
        <div className="card">
          <div className="card-body">
            <div className="flex items-center">
              <div className="flex-shrink-0">
                <StarIcon className="h-8 w-8 text-yellow-600" />
              </div>
              <div className="ml-4">
                <p className="text-sm font-medium text-gray-500">Average Score</p>
                <p className="text-2xl font-bold text-gray-900">{stats?.averageScore}%</p>
              </div>
            </div>
          </div>
        </div>
        <div className="card">
          <div className="card-body">
            <div className="flex items-center">
              <div className="flex-shrink-0">
                <TrendingUpIcon className="h-8 w-8 text-purple-600" />
              </div>
              <div className="ml-4">
                <p className="text-sm font-medium text-gray-500">Trends Validated</p>
                <p className="text-2xl font-bold text-gray-900">{stats?.trendsValidated}</p>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div className="grid grid-cols-1 lg:grid-cols-3 gap-8">
        {/* Quick Actions */}
        <div className="lg:col-span-2">
          <div className="card">
            <div className="card-header">
              <h2 className="text-lg font-medium text-gray-900">Quick Actions</h2>
              <p className="text-sm text-gray-600">Start your analysis with these tools</p>
            </div>
            <div className="card-body">
              <div className="grid grid-cols-1 sm:grid-cols-2 gap-4">
                {quickActions.map((action) => {
                  const Icon = action.icon;
                  return (
                    <Link
                      key={action.name}
                      to={action.href}
                      className="group relative bg-white p-6 rounded-lg border border-gray-200 hover:border-gray-300 hover:shadow-md transition-all duration-200"
                    >
                      <div className="flex items-center">
                        <div className={`flex-shrink-0 p-3 rounded-lg ${action.color} transition-colors duration-200`}>
                          <Icon className="h-6 w-6 text-white" />
                        </div>
                        <div className="ml-4 flex-1">
                          <h3 className="text-sm font-medium text-gray-900 group-hover:text-primary-600">
                            {action.name}
                          </h3>
                          <p className="text-xs text-gray-500 mt-1">
                            {action.description}
                          </p>
                        </div>
                        <ArrowRightIcon className="h-4 w-4 text-gray-400 group-hover:text-primary-600 transition-colors duration-200" />
                      </div>
                    </Link>
                  );
                })}
              </div>
            </div>
          </div>
        </div>
        {/* Recent Activity */}
        <div>
          <div className="card">
            <div className="card-header">
              <h2 className="text-lg font-medium text-gray-900">Recent Activity</h2>
            </div>
            <div className="card-body">
              <div className="space-y-4">
                {recentActivity.map((activity) => {
                  const Icon = getActivityIcon(activity.type);
                  return (
                    <div key={activity.id} className="flex items-start space-x-3">
                      <div className="flex-shrink-0">
                        <Icon className="h-5 w-5 text-gray-400" />
                      </div>
                      <div className="flex-1 min-w-0">
                        <p className="text-sm font-medium text-gray-900 truncate">
                          {activity.title}
                        </p>
                        <p className="text-xs text-gray-500">{formatTimestamp(activity.timestamp)}</p>
                      </div>
                      <div className={`flex-shrink-0 px-2 py-1 text-xs font-medium rounded-full ${getScoreColor(activity.score)}`}>
                        {activity.score}%
                      </div>
                    </div>
                  );
                })}
              </div>
            </div>
            <div className="card-footer">
              <Link
                to="/history"
                className="text-sm text-primary-600 hover:text-primary-700 font-medium"
              >
                View all activity ‚Üí
              </Link>
            </div>
          </div>
        </div>
      </div>
      {/* Performance Chart */}
      <div className="mt-8">
        <ChartContainer chartData={performanceChart} />
      </div>
    </div>
  );
};
export default Dashboard;
</file>

<file path="frontend/src/reportWebVitals.js">
const reportWebVitals = onPerfEntry => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};
export default reportWebVitals;
</file>

<file path="frontend/src/services/api.js">
/**
 * API Service for KDP Strategist Frontend
 * 
 * This module provides a centralized interface for all API calls
 * to the FastAPI backend, including error handling and request/response formatting.
 */
const API_BASE_URL = process.env.REACT_APP_API_URL || 'http://localhost:8000';
class ApiService {
  constructor() {
    this.baseURL = API_BASE_URL;
    this.defaultHeaders = {
      'Content-Type': 'application/json',
    };
  }
  /**
   * Make a generic API request
   */
  async request(endpoint, options = {}) {
    const url = `${this.baseURL}${endpoint}`;
    const config = {
      headers: { ...this.defaultHeaders, ...options.headers },
      ...options,
    };
    try {
      const response = await fetch(url, config);
      if (!response.ok) {
        const errorData = await response.json().catch(() => ({
          detail: `HTTP ${response.status}: ${response.statusText}`
        }));
        throw new Error(errorData.detail || 'API request failed');
      }
      return await response.json();
    } catch (error) {
      console.error(`API request failed: ${endpoint}`, error);
      throw error;
    }
  }
  /**
   * GET request
   */
  async get(endpoint, params = {}) {
    const queryString = new URLSearchParams(params).toString();
    const url = queryString ? `${endpoint}?${queryString}` : endpoint;
    return this.request(url, {
      method: 'GET',
    });
  }
  /**
   * POST request
   */
  async post(endpoint, data = {}) {
    return this.request(endpoint, {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }
  /**
   * PUT request
   */
  async put(endpoint, data = {}) {
    return this.request(endpoint, {
      method: 'PUT',
      body: JSON.stringify(data),
    });
  }
  /**
   * DELETE request
   */
  async delete(endpoint) {
    return this.request(endpoint, {
      method: 'DELETE',
    });
  }
  // Health Check
  async checkHealth() {
    return this.get('/health');
  }
  // Niche Discovery APIs
  async discoverNiches(keywords, options = {}) {
    const requestData = {
      keywords: Array.isArray(keywords) ? keywords : [keywords],
      max_results: options.maxResults || 20,
      min_search_volume: options.minSearchVolume || 1000,
      max_competition: options.maxCompetition || 0.7,
      include_seasonal: options.includeSeasonal || true,
      include_charts: options.includeCharts !== false,
      ...options
    };
    return this.post('/api/niches/discover', requestData);
  }
  async getTrendingNiches(options = {}) {
    return this.get('/api/niches/trending', {
      timeframe: options.timeframe || '30d',
      category: options.category || 'all',
      min_growth_rate: options.minGrowthRate || 0.1
    });
  }
  // Competitor Analysis APIs
  async analyzeCompetitors(asins, options = {}) {
    const requestData = {
      asins: Array.isArray(asins) ? asins : [asins],
      include_pricing: options.includePricing !== false,
      include_reviews: options.includeReviews !== false,
      include_keywords: options.includeKeywords !== false,
      include_charts: options.includeCharts !== false,
      analysis_depth: options.analysisDepth || 'standard',
      ...options
    };
    return this.post('/api/competitors/analyze', requestData);
  }
  async searchCompetitors(keyword, options = {}) {
    return this.get('/api/competitors/search', {
      keyword,
      max_results: options.maxResults || 50,
      min_reviews: options.minReviews || 10,
      max_price: options.maxPrice,
      sort_by: options.sortBy || 'relevance'
    });
  }
  async getMarketOverview(niche, options = {}) {
    return this.get('/api/competitors/market-overview', {
      niche,
      timeframe: options.timeframe || '90d',
      include_forecasts: options.includeForecasts !== false
    });
  }
  // Listing Generation APIs
  async generateListing(niche, options = {}) {
    const requestData = {
      niche,
      target_keywords: options.targetKeywords || [],
      style_preferences: options.stylePreferences || {},
      compliance_level: options.complianceLevel || 'strict',
      include_seo_analysis: options.includeSeoAnalysis !== false,
      include_variations: options.includeVariations !== false,
      ...options
    };
    return this.post('/api/listings/generate', requestData);
  }
  async optimizeListing(listingData, options = {}) {
    const requestData = {
      current_listing: listingData,
      optimization_goals: options.optimizationGoals || ['seo', 'conversion'],
      target_keywords: options.targetKeywords || [],
      competitor_analysis: options.competitorAnalysis || false,
      ...options
    };
    return this.post('/api/listings/optimize', requestData);
  }
  async getListingTemplates(category = 'all') {
    return this.get('/api/listings/templates', { category });
  }
  async checkCompliance(listingData) {
    return this.post('/api/listings/compliance-check', {
      listing_data: listingData
    });
  }
  // Trend Validation APIs
  async validateTrends(keywords, options = {}) {
    const requestData = {
      keywords: Array.isArray(keywords) ? keywords : [keywords],
      timeframe: options.timeframe || '12m',
      include_forecasts: options.includeForecasts !== false,
      include_seasonal: options.includeSeasonal !== false,
      include_charts: options.includeCharts !== false,
      analysis_depth: options.analysisDepth || 'comprehensive',
      ...options
    };
    return this.post('/api/trends/validate', requestData);
  }
  async getTrendingKeywords(options = {}) {
    return this.get('/api/trends/trending-keywords', {
      category: options.category || 'all',
      timeframe: options.timeframe || '7d',
      min_growth_rate: options.minGrowthRate || 0.2,
      region: options.region || 'US'
    });
  }
  async forecastTrend(keyword, options = {}) {
    return this.get(`/api/trends/forecast/${encodeURIComponent(keyword)}`, {
      forecast_period: options.forecastPeriod || '6m',
      confidence_level: options.confidenceLevel || 0.8,
      include_scenarios: options.includeScenarios !== false
    });
  }
  async getSeasonalPatterns(keywords, options = {}) {
    const requestData = {
      keywords: Array.isArray(keywords) ? keywords : [keywords],
      years_back: options.yearsBack || 3,
      include_predictions: options.includePredictions !== false
    };
    return this.post('/api/trends/seasonal-patterns', requestData);
  }
  // Stress Testing APIs
  async runStressTest(niche, options = {}) {
    const requestData = {
      niche,
      test_scenarios: options.testScenarios || [
        'market_saturation',
        'seasonal_decline',
        'trend_reversal',
        'competition_increase'
      ],
      severity_level: options.severityLevel || 'moderate',
      include_recommendations: options.includeRecommendations !== false,
      ...options
    };
    return this.post('/api/stress/run', requestData);
  }
  async getAvailableScenarios() {
    return this.get('/api/stress/scenarios');
  }
  async getRiskMatrix() {
    return this.get('/api/stress/risk-matrix');
  }
  async compareNiches(niches, scenarios = null) {
    const requestData = {
      niches: Array.isArray(niches) ? niches : [niches],
      scenarios: scenarios
    };
    return this.post('/api/stress/compare', requestData);
  }
  // Export functionality
  async exportData(data, format = 'csv', options = {}) {
    const requestData = {
      data,
      format,
      filename: options.filename,
      include_metadata: options.includeMetadata !== false,
      ...options
    };
    const response = await this.request('/api/export', {
      method: 'POST',
      body: JSON.stringify(requestData),
      headers: {
        ...this.defaultHeaders,
        'Accept': format === 'pdf' ? 'application/pdf' : 'application/octet-stream'
      }
    });
    return response;
  }
  // WebSocket connection for real-time updates
  createWebSocket(onMessage, onError = null, onClose = null) {
    const wsUrl = this.baseURL.replace('http', 'ws') + '/ws';
    const ws = new WebSocket(wsUrl);
    ws.onmessage = (event) => {
      try {
        const data = JSON.parse(event.data);
        onMessage(data);
      } catch (error) {
        console.error('Failed to parse WebSocket message:', error);
      }
    };
    ws.onerror = (error) => {
      console.error('WebSocket error:', error);
      if (onError) onError(error);
    };
    ws.onclose = (event) => {
      console.log('WebSocket connection closed:', event.code, event.reason);
      if (onClose) onClose(event);
    };
    return ws;
  }
  // Utility methods
  formatError(error) {
    if (error.response && error.response.data) {
      return error.response.data.detail || error.response.data.message || 'An error occurred';
    }
    return error.message || 'An unexpected error occurred';
  }
  isNetworkError(error) {
    return error.message.includes('fetch') || error.message.includes('Network');
  }
  // Cache management (simple in-memory cache)
  cache = new Map();
  getCached(key) {
    const cached = this.cache.get(key);
    if (cached && Date.now() - cached.timestamp < 300000) { // 5 minutes
      return cached.data;
    }
    return null;
  }
  setCache(key, data) {
    this.cache.set(key, {
      data,
      timestamp: Date.now()
    });
  }
  clearCache() {
    this.cache.clear();
  }
}
// Create and export a singleton instance
export const apiService = new ApiService();
export default apiService;
</file>

<file path="frontend/src/utils/chartUtils.js">
/**
 * Utility functions for transforming API chart data to Chart.js format
 */
/**
 * Transform API chart data to Chart.js compatible format
 * @param {Object} apiChartData - Chart data from API
 * @returns {Object} Chart.js compatible data structure
 */
export function transformChartData(apiChartData) {
  if (!apiChartData || !apiChartData.data) {
    return {
      labels: [],
      datasets: []
    };
  }
  const { type, data, labels, colors = [] } = apiChartData;
  // Default colors if none provided
  const defaultColors = [
    'rgba(59, 130, 246, 0.8)',   // blue
    'rgba(16, 185, 129, 0.8)',   // green
    'rgba(245, 158, 11, 0.8)',   // yellow
    'rgba(239, 68, 68, 0.8)',    // red
    'rgba(139, 92, 246, 0.8)',   // purple
    'rgba(236, 72, 153, 0.8)',   // pink
    'rgba(6, 182, 212, 0.8)',    // cyan
    'rgba(34, 197, 94, 0.8)'     // emerald
  ];
  const borderColors = [
    'rgba(59, 130, 246, 1)',
    'rgba(16, 185, 129, 1)',
    'rgba(245, 158, 11, 1)',
    'rgba(239, 68, 68, 1)',
    'rgba(139, 92, 246, 1)',
    'rgba(236, 72, 153, 1)',
    'rgba(6, 182, 212, 1)',
    'rgba(34, 197, 94, 1)'
  ];
  switch (type.toLowerCase()) {
    case 'bar':
      return transformBarChartData(data, labels, colors, defaultColors, borderColors);
    case 'line':
      return transformLineChartData(data, labels, colors, defaultColors, borderColors);
    case 'pie':
    case 'doughnut':
      return transformPieChartData(data, colors, defaultColors);
    case 'scatter':
      return transformScatterChartData(data, colors, defaultColors, borderColors);
    default:
      console.warn(`Unsupported chart type: ${type}`);
      return {
        labels: [],
        datasets: []
      };
  }
}
/**
 * Transform data for bar charts
 */
function transformBarChartData(data, labels, colors, defaultColors, borderColors) {
  const chartLabels = labels || data.map(item => item.name || item.label || item.x);
  const values = data.map(item => item.score || item.value || item.y);
  return {
    labels: chartLabels,
    datasets: [{
      label: 'Values',
      data: values,
      backgroundColor: colors.length > 0 ? colors : defaultColors,
      borderColor: borderColors,
      borderWidth: 1
    }]
  };
}
/**
 * Transform data for line charts
 */
function transformLineChartData(data, labels, colors, defaultColors, borderColors) {
  const chartLabels = labels || data.map(item => item.name || item.label || item.x);
  const values = data.map(item => item.score || item.value || item.y);
  return {
    labels: chartLabels,
    datasets: [{
      label: 'Values',
      data: values,
      backgroundColor: (colors && colors.length > 0) ? colors[0] : defaultColors[0],
      borderColor: (colors && colors.length > 0) ? colors[0] : borderColors[0],
      borderWidth: 2,
      fill: false,
      tension: 0.4
    }]
  };
}
/**
 * Transform data for pie/doughnut charts
 */
function transformPieChartData(data, colors, defaultColors) {
  const chartLabels = data.map(item => item.label || item.name);
  const values = data.map(item => item.value || item.score);
  return {
    labels: chartLabels,
    datasets: [{
      data: values,
      backgroundColor: colors.length > 0 ? colors : defaultColors.slice(0, data.length),
      borderWidth: 1,
      borderColor: '#ffffff'
    }]
  };
}
/**
 * Transform data for scatter charts
 */
function transformScatterChartData(data, colors, defaultColors, borderColors) {
  const scatterData = data.map(item => ({
    x: item.x,
    y: item.y,
    label: item.label
  }));
  return {
    datasets: [{
      label: 'Data Points',
      data: scatterData,
      backgroundColor: (colors && colors.length > 0) ? colors[0] : defaultColors[0],
      borderColor: (colors && colors.length > 0) ? colors[0] : borderColors[0],
      borderWidth: 1
    }]
  };
}
/**
 * Validate chart data structure
 * @param {Object} chartData - Chart data to validate
 * @returns {boolean} True if valid
 */
export function validateChartData(chartData) {
  if (!chartData) return false;
  if (!chartData.type) return false;
  if (!chartData.data || !Array.isArray(chartData.data)) return false;
  if (chartData.data.length === 0) return false;
  return true;
}
</file>

<file path="frontend/tailwind.config.js">
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    "./src/**/*.{js,jsx,ts,tsx}",
    "./public/index.html"
  ],
  theme: {
    extend: {
      colors: {
        primary: {
          50: '#eff6ff',
          100: '#dbeafe',
          200: '#bfdbfe',
          300: '#93c5fd',
          400: '#60a5fa',
          500: '#3b82f6',
          600: '#2563eb',
          700: '#1d4ed8',
          800: '#1e40af',
          900: '#1e3a8a',
        },
        secondary: {
          50: '#f8fafc',
          100: '#f1f5f9',
          200: '#e2e8f0',
          300: '#cbd5e1',
          400: '#94a3b8',
          500: '#64748b',
          600: '#475569',
          700: '#334155',
          800: '#1e293b',
          900: '#0f172a',
        },
        success: {
          50: '#f0fdf4',
          100: '#dcfce7',
          200: '#bbf7d0',
          300: '#86efac',
          400: '#4ade80',
          500: '#22c55e',
          600: '#16a34a',
          700: '#15803d',
          800: '#166534',
          900: '#14532d',
        },
        warning: {
          50: '#fffbeb',
          100: '#fef3c7',
          200: '#fde68a',
          300: '#fcd34d',
          400: '#fbbf24',
          500: '#f59e0b',
          600: '#d97706',
          700: '#b45309',
          800: '#92400e',
          900: '#78350f',
        },
        danger: {
          50: '#fef2f2',
          100: '#fee2e2',
          200: '#fecaca',
          300: '#fca5a5',
          400: '#f87171',
          500: '#ef4444',
          600: '#dc2626',
          700: '#b91c1c',
          800: '#991b1b',
          900: '#7f1d1d',
        },
      },
      fontFamily: {
        sans: ['Inter', 'ui-sans-serif', 'system-ui', '-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'Helvetica Neue', 'Arial', 'Noto Sans', 'sans-serif'],
        mono: ['JetBrains Mono', 'ui-monospace', 'SFMono-Regular', 'Menlo', 'Monaco', 'Consolas', 'Liberation Mono', 'Courier New', 'monospace'],
      },
      spacing: {
        '18': '4.5rem',
        '88': '22rem',
        '128': '32rem',
      },
      animation: {
        'fade-in': 'fadeIn 0.5s ease-in-out',
        'slide-in': 'slideIn 0.3s ease-out',
        'bounce-subtle': 'bounceSubtle 2s infinite',
        'pulse-slow': 'pulse 3s cubic-bezier(0.4, 0, 0.6, 1) infinite',
      },
      keyframes: {
        fadeIn: {
          '0%': { opacity: '0' },
          '100%': { opacity: '1' },
        },
        slideIn: {
          '0%': { transform: 'translateX(-100%)' },
          '100%': { transform: 'translateX(0)' },
        },
        bounceSubtle: {
          '0%, 100%': {
            transform: 'translateY(-5%)',
            animationTimingFunction: 'cubic-bezier(0.8, 0, 1, 1)',
          },
          '50%': {
            transform: 'translateY(0)',
            animationTimingFunction: 'cubic-bezier(0, 0, 0.2, 1)',
          },
        },
      },
      boxShadow: {
        'soft': '0 2px 15px -3px rgba(0, 0, 0, 0.07), 0 10px 20px -2px rgba(0, 0, 0, 0.04)',
        'medium': '0 4px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04)',
        'strong': '0 10px 40px -10px rgba(0, 0, 0, 0.15), 0 4px 25px -5px rgba(0, 0, 0, 0.1)',
      },
      borderRadius: {
        'xl': '1rem',
        '2xl': '1.5rem',
        '3xl': '2rem',
      },
      backdropBlur: {
        xs: '2px',
      },
      screens: {
        'xs': '475px',
        '3xl': '1600px',
      },
      zIndex: {
        '60': '60',
        '70': '70',
        '80': '80',
        '90': '90',
        '100': '100',
      },
    },
  },
  plugins: [
    require('@tailwindcss/forms'),
    require('@tailwindcss/typography'),
    require('@tailwindcss/aspect-ratio'),
  ],
  darkMode: 'class',
}
</file>

<file path="kdp-strategist.mcp.json">
{
  "version": "1.0",
  "id": "kdp_strategist",
  "name": "KDP Strategist",
  "description": "An AI agent that discovers, validates, and generates profitable Amazon KDP book opportunities using real-time data, semantic clustering, and natural language generation.",
  "persona": {
    "role": "AI Publishing Strategist",
    "tone": "strategic, data-driven, clear",
    "goals": [
      "Find low-competition, high-demand niches for self-publishing",
      "Generate complete, optimized book listings",
      "Advise users on portfolio and series growth strategy"
    ]
  },
  "context": {
    "short_term_memory": true,
    "long_term_memory": {
      "type": "vector_db",
      "namespace": "kdp-niches"
    },
    "tools": [
      {
        "name": "find_profitable_niches",
        "description": "Scrape and rank KDP categories using BSR, review count, and semantic clustering",
        "input_schema": {
          "category": "string",
          "filters": {
            "max_bsr": "number",
            "max_competitors": "number"
          }
        }
      },
      {
        "name": "generate_kdp_listing",
        "description": "Creates an optimized KDP title, subtitle, bullet points, keywords, and HTML description",
        "input_schema": {
          "niche": "string",
          "tone": "string",
          "audience": "string"
        }
      },
      {
        "name": "analyze_competitor_asin",
        "description": "Provides review breakdown, keyword analysis, and pricing from an Amazon ASIN",
        "input_schema": {
          "asin": "string"
        }
      },
      {
        "name": "validate_trend",
        "description": "Validates niche momentum using Google Trends",
        "input_schema": {
          "keyword": "string"
        }
      },
      {
        "name": "niche_stress_test",
        "description": "Checks if a niche idea is over-saturated or profitable",
        "input_schema": {
          "niche": "string"
        }
      }
    ]
  },
  "output_format": {
    "listing": {
      "title": "string",
      "subtitle": "string",
      "bullets": [
        "string"
      ],
      "description_html": "string",
      "keywords": [
        "string"
      ],
      "price": "number"
    },
    "niche_report": {
      "niche_name": "string",
      "bsr_range": "string",
      "trend_score": "number",
      "competition_count": "number",
      "go_no_go": "boolean",
      "rationale": "string"
    }
  },
  "requirements": {
    "embedding_model": "openai/text-embedding-3-small",
    "llm_model": "gpt-4o",
    "retrieval_plugin": "supabase_pgvector"
  }
}
</file>

<file path="README.md">
# KDP Strategist AI Agent

A sophisticated AI-powered agent designed to help Kindle Direct Publishing (KDP) authors and publishers discover profitable niches, analyze market competition, generate optimized book listings, validate trends, and perform comprehensive stress testing on market opportunities.

## üöÄ Features

### Core Capabilities

- **üîç Niche Discovery**: Intelligent discovery of profitable publishing niches using advanced keyword analysis and market research
- **üìä Competitor Analysis**: Deep analysis of competitor products, pricing strategies, and market positioning
- **üìù Listing Generation**: AI-powered generation of optimized KDP book listings with SEO-friendly titles, descriptions, and keywords
- **üìà Trend Validation**: Comprehensive trend analysis with forecasting and seasonality detection
- **üß™ Stress Testing**: Rigorous testing of niche resilience under various market scenarios

### Technical Features

- **MCP Integration**: Built on the Model Context Protocol for seamless AI assistant integration
- **Multi-API Support**: Integrates with Google Trends and Keepa APIs for comprehensive market data
- **Advanced Caching**: Intelligent caching system supporting file-based, Redis, and in-memory storage
- **Rate Limiting**: Built-in rate limiting and retry logic for API stability
- **Comprehensive Logging**: Detailed logging and monitoring capabilities
- **Data Validation**: Robust data validation and error handling

## üìã Prerequisites

- Python 3.8 or higher
- pip (Python package installer)
- Optional: Redis server (for Redis caching)
- API Keys:
  - Keepa API key (for Amazon product data)
  - Google Trends access (via pytrends)

## üõ†Ô∏è Installation

### 1. Clone the Repository

```bash
git clone <repository-url>
cd PublishingStrategist
```

### 2. Create Virtual Environment

```bash
python -m venv venv

# On Windows
venv\Scripts\activate

# On macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Install the Package

```bash
pip install -e .
```

### 5. Configuration

Create a `.env` file in the project root:

```env
# API Configuration
KEEPA_API_KEY=your_keepa_api_key_here

# Cache Configuration
CACHE_TYPE=file  # Options: file, redis, memory
CACHE_TTL=3600
CACHE_MAX_SIZE=1000

# Redis Configuration (if using Redis cache)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# Rate Limiting
KEEPA_RATE_LIMIT=100
TRENDS_RATE_LIMIT=50

# Development
DEVELOPMENT_MODE=true
LOG_LEVEL=INFO
```

## üöÄ Usage

### Command Line Interface

#### Start the MCP Server

```bash
kdp_strategist
```

#### Interactive Mode (for testing)

```bash
kdp_strategist --interactive
```

#### Custom Configuration

```bash
kdp_strategist --config custom.env --log-level DEBUG
```

### Python Module

```bash
python -m kdp_strategist.main
```

### Interactive Mode Commands

When running in interactive mode, you can use these commands:

- `help` - Show available commands
- `tools` - List all available tools
- `test <tool_name>` - Test a specific tool with sample data
- `quit` - Exit interactive mode

## üîß Available Tools

### 1. Find Profitable Niches

Discover profitable publishing niches based on keyword analysis.

```python
result = await agent.call_tool("find_profitable_niches", {
    "base_keywords": ["cooking", "fitness", "productivity"],
    "max_niches": 5,
    "min_profitability": 70,
    "include_seasonal": True
})
```

### 2. Analyze Competitor ASIN

Analyze specific competitor products on Amazon.

```python
result = await agent.call_tool("analyze_competitor_asin", {
    "asin": "B08EXAMPLE123",
    "include_market_analysis": True,
    "analyze_pricing_history": True
})
```

### 3. Generate KDP Listing

Generate optimized book listings for KDP.

```python
result = await agent.call_tool("generate_kdp_listing", {
    "niche_keyword": "meal prep for beginners",
    "target_audience": "busy professionals",
    "book_type": "cookbook",
    "include_keywords": True
})
```

### 4. Validate Trend

Validate and analyze market trends with forecasting.

```python
result = await agent.call_tool("validate_trend", {
    "keyword": "intermittent fasting",
    "timeframe": "today 12-m",
    "include_forecasts": True,
    "include_seasonality": True
})
```

### 5. Niche Stress Test

Perform comprehensive stress testing on market niches.

```python
result = await agent.call_tool("niche_stress_test", {
    "niche_keyword": "keto diet recipes",
    "include_all_scenarios": True,
    "custom_scenarios": []
})
```

## üìä Data Models

### Niche Model

Represents a publishing niche with comprehensive metrics:

- **Category**: Primary category/topic
- **Keywords**: Primary and related keywords
- **Scores**: Competition, profitability, market size, confidence
- **Trend Analysis**: Historical and forecasted trend data
- **Competitor Data**: Information about market competitors
- **Content Gaps**: Identified opportunities for content creation
- **Seasonal Factors**: Seasonal patterns and volatility

### KDP Listing Model

Represents an optimized book listing:

- **Title**: SEO-optimized book title
- **Description**: Compelling book description
- **Keywords**: Targeted keywords for discoverability
- **Categories**: Relevant KDP categories
- **Pricing**: Recommended pricing strategy
- **Target Audience**: Defined target readership

### Trend Analysis Model

Represents comprehensive trend analysis:

- **Trend Score**: Numerical trend strength (0-100)
- **Direction**: Rising, stable, or declining
- **Strength**: Weak, moderate, strong, very strong
- **Regional Interest**: Geographic trend distribution
- **Seasonal Patterns**: Cyclical behavior analysis
- **Forecasts**: 1, 3, 6, and 12-month predictions

## üèóÔ∏è Architecture



### Key Components

1. **Agent Layer**: MCP-compatible agent with tool registration and management
2. **Data Layer**: API clients with caching, rate limiting, and error handling
3. **Model Layer**: Structured data models with validation and serialization
4. **Configuration Layer**: Environment-based configuration management

## üîß Configuration Options

### Cache Configuration

- **File Cache**: Stores data in local files (default)
- **Redis Cache**: Uses Redis for distributed caching
- **Memory Cache**: In-memory caching for development

### API Configuration

- **Keepa API**: Amazon product data and pricing history
- **Google Trends**: Search trend data and analysis
- **Rate Limiting**: Configurable rate limits for API stability

### Logging Configuration

- **Multiple Loggers**: Separate loggers for different components
- **Multiple Handlers**: Console, file, and structured logging
- **Configurable Levels**: DEBUG, INFO, WARNING, ERROR, CRITICAL

## üß™ Testing

### Interactive Testing

Use the interactive mode to test individual tools:

```bash
kdp_strategist --interactive
```

### Tool Testing

Test specific tools with sample data:

```
KDP Strategist> test find_profitable_niches
KDP Strategist> test validate_trend
```

## üìà Performance Optimization

### Caching Strategy

- **Multi-level Caching**: Memory, file, and Redis caching
- **TTL Management**: Configurable time-to-live for cached data
- **Cache Invalidation**: Smart cache invalidation strategies

### Rate Limiting

- **API Protection**: Prevents API quota exhaustion
- **Retry Logic**: Exponential backoff for failed requests
- **Circuit Breaker**: Automatic failure detection and recovery

### Batch Processing

- **Keyword Expansion**: Efficient batch processing of keyword variations
- **Trend Analysis**: Batch trend data retrieval and analysis
- **Competitor Analysis**: Parallel processing of competitor data

## üö® Error Handling

### Robust Error Management

- **API Failures**: Graceful handling of API errors and timeouts
- **Data Validation**: Comprehensive input and output validation
- **Fallback Strategies**: Alternative data sources when primary APIs fail
- **Logging**: Detailed error logging for debugging and monitoring

### Recovery Mechanisms

- **Retry Logic**: Automatic retry with exponential backoff
- **Circuit Breaker**: Prevents cascade failures
- **Graceful Degradation**: Partial functionality when services are unavailable

## üîí Security Considerations

### API Key Management

- **Environment Variables**: Secure storage of API keys
- **No Hardcoding**: API keys never stored in code
- **Validation**: API key validation on startup

### Data Privacy

- **Local Processing**: Sensitive data processed locally
- **Cache Security**: Secure caching of sensitive information
- **Logging Safety**: No sensitive data in logs

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üÜò Support

### Common Issues

1. **API Key Errors**: Ensure your Keepa API key is valid and has sufficient quota
2. **Rate Limiting**: If you encounter rate limits, increase the delay between requests
3. **Cache Issues**: Clear the cache directory if you encounter stale data
4. **Memory Usage**: Use Redis cache for large-scale operations

### Getting Help

- Check the logs for detailed error information
- Use interactive mode to test individual components
- Review the configuration settings
- Consult the API documentation for external services

## üîÆ Future Enhancements
                                                                                    
- **Machine Learning Models**: Advanced predictive models for niche profitability
- **Real-time Monitoring**: Live market monitoring and alerts
- **Advanced Analytics**: Deeper market analysis and insights
- **Multi-platform Support**: Support for additional publishing platforms
- **Web Interface**: Browser-based interface for easier interaction
- **API Expansion**: Additional data sources and market intelligence

---

**KDP Strategist AI Agent** - Empowering publishers with AI-driven market intelligence.
</file>

<file path="run_server.py">
#!/usr/bin/env python3
"""
KDP Strategist FastAPI Server Launcher
This script starts the FastAPI backend server for the KDP Strategist web UI.
It handles server configuration, logging setup, and graceful shutdown.
"""
import os
import sys
import asyncio
import signal
from pathlib import Path
# Add the project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
try:
    import uvicorn
    from api.main import app
except ImportError as e:
    print(f"Error importing dependencies: {e}")
    print("Please install the required dependencies with: pip install -r requirements.txt")
    sys.exit(1)
def setup_logging():
    """Configure logging for the server."""
    import logging
    # Configure root logger
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('kdp_strategist.log')
        ]
    )
    # Set specific log levels
    logging.getLogger('uvicorn').setLevel(logging.INFO)
    logging.getLogger('uvicorn.access').setLevel(logging.INFO)
    logging.getLogger('fastapi').setLevel(logging.INFO)
    return logging.getLogger(__name__)
def get_server_config():
    """Get server configuration from environment variables."""
    return {
        'host': os.getenv('HOST', '0.0.0.0'),
        'port': int(os.getenv('PORT', 8000)),
        'reload': os.getenv('RELOAD', 'false').lower() == 'true',
        'workers': int(os.getenv('WORKERS', 1)),
        'log_level': os.getenv('LOG_LEVEL', 'info').lower(),
        'access_log': os.getenv('ACCESS_LOG', 'true').lower() == 'true',
    }
def print_startup_info(config):
    """Print server startup information."""
    print("\n" + "="*60)
    print("üöÄ KDP Strategist FastAPI Server")
    print("="*60)
    print(f"üìç Server URL: http://{config['host']}:{config['port']}")
    print(f"üìñ API Documentation: http://{config['host']}:{config['port']}/docs")
    print(f"üîß Interactive API: http://{config['host']}:{config['port']}/redoc")
    print(f"üíæ Log Level: {config['log_level'].upper()}")
    print(f"üîÑ Auto-reload: {'Enabled' if config['reload'] else 'Disabled'}")
    print(f"üë• Workers: {config['workers']}")
    print("="*60)
    print("Press Ctrl+C to stop the server")
    print("="*60 + "\n")
def handle_shutdown(signum, frame):
    """Handle graceful shutdown."""
    print("\nüõë Shutting down KDP Strategist server...")
    sys.exit(0)
def main():
    """Main entry point for the server."""
    # Setup logging
    logger = setup_logging()
    # Setup signal handlers for graceful shutdown
    signal.signal(signal.SIGINT, handle_shutdown)
    signal.signal(signal.SIGTERM, handle_shutdown)
    # Get configuration
    config = get_server_config()
    # Print startup information
    print_startup_info(config)
    try:
        # Check if MCP agent dependencies are available
        logger.info("Checking MCP agent dependencies...")
        # Start the server
        logger.info(f"Starting KDP Strategist server on {config['host']}:{config['port']}")
        uvicorn.run(
            "api.main:app",
            host=config['host'],
            port=config['port'],
            reload=config['reload'],
            workers=config['workers'] if not config['reload'] else 1,
            log_level=config['log_level'],
            access_log=config['access_log'],
            server_header=False,
            date_header=False,
        )
    except KeyboardInterrupt:
        logger.info("Server stopped by user")
    except Exception as e:
        logger.error(f"Server error: {e}")
        sys.exit(1)
if __name__ == "__main__":
    main()
</file>

<file path="setup.py">
"""Setup configuration for KDP Strategist AI Agent package."""
from setuptools import setup, find_packages
with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()
with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = [line.strip() for line in fh if line.strip() and not line.startswith("#")]
setup(
    name="kdp_strategist",
    version="0.1.0",
    author="KDP Strategist Team",
    author_email="team@kdpstrategist.ai",
    description="AI Agent for Amazon KDP Publishing Strategy",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/kdp_strategist/kdp_strategist",
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Topic :: Software Development :: Libraries :: Python Modules",
        "Topic :: Internet :: WWW/HTTP :: Dynamic Content",
        "Topic :: Office/Business :: Financial :: Investment",
    ],
    python_requires=">=3.8",
    install_requires=requirements,
    extras_require={
        "dev": [
            "pytest>=7.4.0",
            "pytest-asyncio>=0.21.0",
            "pytest-cov>=4.1.0",
            "black>=23.7.0",
            "flake8>=6.0.0",
            "mypy>=1.5.0",
        ],
        "docs": [
            "sphinx>=7.1.0",
            "sphinx-rtd-theme>=1.3.0",
        ],
    },
    entry_points={
        "console_scripts": [
            "kdp_strategist=kdp_strategist.cli:main",
        ],
    },
    include_package_data=True,
    zip_safe=False,
)
</file>

<file path="src/kdp_strategist/__init__.py">
"""KDP Strategist AI Agent
An AI agent for Amazon Kindle Direct Publishing (KDP) that provides:
- Niche discovery and analysis
- Trend validation using Google Trends
- Competitor analysis using Amazon data
- Optimized listing generation
- Market stress testing
This package implements the Model Context Protocol (MCP) for seamless
integration with AI development environments.
"""
__version__ = "0.1.0"
__author__ = "KDP Strategist Team"
__description__ = "AI Agent for Amazon KDP Publishing Strategy"
from .agent import KDPStrategistAgent
__all__ = ["KDPStrategistAgent"]
</file>

<file path="src/kdp_strategist/agent/kdp_strategist_agent.py">
"""KDP Strategist MCP Agent.
Main agent class that implements the Model Context Protocol (MCP) interface
for the KDP Strategist AI system. Coordinates all tools and services to provide
comprehensive publishing strategy analysis.
"""
import asyncio
import logging
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from pathlib import Path
try:
    from mcp import ClientSession, StdioServerParameters
    from mcp.client.stdio import stdio_client
    from mcp.types import (
        Tool, 
        TextContent, 
        CallToolRequest, 
        CallToolResult,
        ListToolsRequest,
        ListToolsResult
    )
except ImportError:
    raise ImportError("MCP library required. Install with: pip install mcp")
from ..data.cache_manager import CacheManager, CacheConfig
from ..data.keepa_client import KeepaClient, KeepaConfig
from ..data.trends_client import TrendsClient, TrendsConfig
from ..models.niche_model import Niche
from ..models.trend_model import TrendAnalysis
from config.settings import Settings
logger = logging.getLogger(__name__)
class KDPStrategistAgent:
    """Main KDP Strategist MCP Agent.
    Provides AI-powered publishing strategy analysis through MCP tools:
    - find_profitable_niches: Discover profitable publishing niches
    - analyze_competitor_asin: Analyze competitor products
    - generate_kdp_listing: Create optimized KDP listings
    - validate_trend: Validate trend strength and sustainability
    - niche_stress_test: Test niche viability under various conditions
    """
    def __init__(self, settings: Optional[Settings] = None):
        """Initialize the KDP Strategist Agent.
        Args:
            settings: Configuration settings. If None, loads from environment.
        """
        self.settings = settings or Settings()
        self.session_id = f"kdp_strategist_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        # Initialize cache manager
        cache_config = CacheConfig(
            cache_type=self.settings.cache.cache_type,
            cache_dir=Path(self.settings.cache.cache_dir),
            redis_url=self.settings.cache.redis_url,
            default_ttl=self.settings.cache.default_ttl,
            max_cache_size=self.settings.cache.max_cache_size
        )
        self.cache_manager = CacheManager(cache_config)
        # Initialize API clients
        self._init_api_clients()
        # Tool registry
        self.tools = self._register_tools()
        # Statistics
        self.stats = {
            "session_start": datetime.now(),
            "tools_called": 0,
            "cache_hits": 0,
            "api_calls": 0,
            "errors": 0
        }
        logger.info(f"KDP Strategist Agent initialized (session: {self.session_id})")
    def _init_api_clients(self) -> None:
        """Initialize external API clients."""
        # Initialize Keepa client
        if self.settings.keepa.api_key:
            keepa_config = KeepaConfig(
                api_key=self.settings.keepa.api_key,
                rate_limit_per_minute=self.settings.keepa.rate_limit_per_minute,
                cache_ttl=self.settings.keepa.cache_ttl,
                enable_caching=self.settings.keepa.enable_caching
            )
            self.keepa_client = KeepaClient(keepa_config, self.cache_manager)
            logger.info("Keepa client initialized")
        else:
            self.keepa_client = None
            logger.warning("Keepa API key not provided - competitor analysis will be limited")
        # Initialize Trends client
        trends_config = TrendsConfig(
            geo=self.settings.trends.geo,
            language=self.settings.trends.language,
            rate_limit_delay=self.settings.trends.rate_limit_delay,
            cache_ttl=self.settings.trends.cache_ttl,
            enable_caching=self.settings.trends.enable_caching
        )
        self.trends_client = TrendsClient(trends_config, self.cache_manager)
        logger.info("Google Trends client initialized")
    def _register_tools(self) -> Dict[str, Tool]:
        """Register MCP tools."""
        tools = {
            "find_profitable_niches": Tool(
                name="find_profitable_niches",
                description="Discover profitable publishing niches based on market analysis, trends, and competition data.",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "base_keywords": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Base keywords to explore for niche discovery"
                        },
                        "categories": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Amazon categories to focus on (optional)"
                        },
                        "min_profitability_score": {
                            "type": "number",
                            "minimum": 0,
                            "maximum": 100,
                            "default": 60,
                            "description": "Minimum profitability score (0-100)"
                        },
                        "max_competition_level": {
                            "type": "string",
                            "enum": ["low", "medium", "high"],
                            "default": "medium",
                            "description": "Maximum acceptable competition level"
                        },
                        "limit": {
                            "type": "integer",
                            "minimum": 1,
                            "maximum": 50,
                            "default": 10,
                            "description": "Maximum number of niches to return"
                        }
                    },
                    "required": ["base_keywords"]
                }
            ),
            "analyze_competitor_asin": Tool(
                name="analyze_competitor_asin",
                description="Analyze competitor products on Amazon using ASIN to understand market positioning, pricing, and performance.",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "asin": {
                            "type": "string",
                            "pattern": "^[A-Z0-9]{10}$",
                            "description": "Amazon ASIN (10-character alphanumeric code)"
                        },
                        "include_variations": {
                            "type": "boolean",
                            "default": False,
                            "description": "Include analysis of product variations"
                        },
                        "analyze_reviews": {
                            "type": "boolean",
                            "default": True,
                            "description": "Include review sentiment analysis"
                        },
                        "historical_data": {
                            "type": "boolean",
                            "default": True,
                            "description": "Include historical pricing and ranking data"
                        }
                    },
                    "required": ["asin"]
                }
            ),
            "generate_kdp_listing": Tool(
                name="generate_kdp_listing",
                description="Generate optimized KDP listing with title, description, keywords, and metadata based on niche analysis.",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "niche": {
                            "type": "object",
                            "description": "Niche data from find_profitable_niches or manual input"
                        },
                        "target_audience": {
                            "type": "string",
                            "description": "Primary target audience for the book"
                        },
                        "book_type": {
                            "type": "string",
                            "enum": ["journal", "notebook", "planner", "coloring_book", "workbook", "guide", "other"],
                            "default": "journal",
                            "description": "Type of book/product"
                        },
                        "unique_angle": {
                            "type": "string",
                            "description": "Unique selling proposition or angle"
                        },
                        "price_range": {
                            "type": "object",
                            "properties": {
                                "min": {"type": "number", "minimum": 0.99},
                                "max": {"type": "number", "minimum": 0.99}
                            },
                            "description": "Target price range"
                        }
                    },
                    "required": ["niche", "target_audience"]
                }
            ),
            "validate_trend": Tool(
                name="validate_trend",
                description="Validate trend strength, sustainability, and seasonal patterns for keywords or niches.",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "keywords": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Keywords to validate trends for"
                        },
                        "timeframe": {
                            "type": "string",
                            "enum": ["today 3-m", "today 12-m", "today 5-y", "all"],
                            "default": "today 12-m",
                            "description": "Time period for trend analysis"
                        },
                        "geo": {
                            "type": "string",
                            "default": "US",
                            "description": "Geographic region for analysis"
                        },
                        "include_forecast": {
                            "type": "boolean",
                            "default": True,
                            "description": "Include 6-month trend forecast"
                        },
                        "seasonal_analysis": {
                            "type": "boolean",
                            "default": True,
                            "description": "Include seasonal pattern analysis"
                        }
                    },
                    "required": ["keywords"]
                }
            ),
            "niche_stress_test": Tool(
                name="niche_stress_test",
                description="Perform comprehensive stress testing of a niche to assess viability under various market conditions.",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "niche": {
                            "type": "object",
                            "description": "Niche data to stress test"
                        },
                        "test_scenarios": {
                            "type": "array",
                            "items": {
                                "type": "string",
                                "enum": ["market_saturation", "trend_decline", "increased_competition", "seasonal_vulnerability", "price_pressure"]
                            },
                            "default": ["market_saturation", "trend_decline", "increased_competition"],
                            "description": "Stress test scenarios to run"
                        },
                        "severity_level": {
                            "type": "string",
                            "enum": ["mild", "moderate", "severe"],
                            "default": "moderate",
                            "description": "Severity level for stress testing"
                        },
                        "time_horizon": {
                            "type": "string",
                            "enum": ["3_months", "6_months", "12_months"],
                            "default": "6_months",
                            "description": "Time horizon for stress testing"
                        }
                    },
                    "required": ["niche"]
                }
            )
        }
        logger.info(f"Registered {len(tools)} MCP tools")
        return tools
    async def list_tools(self) -> ListToolsResult:
        """List available tools (MCP interface)."""
        return ListToolsResult(tools=list(self.tools.values()))
    async def call_tool(self, request: CallToolRequest) -> CallToolResult:
        """Call a tool (MCP interface)."""
        tool_name = request.params.name
        arguments = request.params.arguments or {}
        logger.info(f"Calling tool: {tool_name} with args: {list(arguments.keys())}")
        self.stats["tools_called"] += 1
        try:
            if tool_name == "find_profitable_niches":
                result = await self._find_profitable_niches(**arguments)
            elif tool_name == "analyze_competitor_asin":
                result = await self._analyze_competitor_asin(**arguments)
            elif tool_name == "generate_kdp_listing":
                result = await self._generate_kdp_listing(**arguments)
            elif tool_name == "validate_trend":
                result = await self._validate_trend(**arguments)
            elif tool_name == "niche_stress_test":
                result = await self._niche_stress_test(**arguments)
            else:
                raise ValueError(f"Unknown tool: {tool_name}")
            return CallToolResult(
                content=[
                    TextContent(
                        type="text",
                        text=str(result)
                    )
                ]
            )
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"Tool {tool_name} failed: {e}")
            return CallToolResult(
                content=[
                    TextContent(
                        type="text",
                        text=f"Error executing {tool_name}: {str(e)}"
                    )
                ],
                isError=True
            )
    async def _find_profitable_niches(self, base_keywords: List[str], 
                                     categories: Optional[List[str]] = None,
                                     min_profitability_score: float = 60,
                                     max_competition_level: str = "medium",
                                     limit: int = 10) -> Dict[str, Any]:
        """Find profitable niches implementation."""
        # Import here to avoid circular imports
        from .tools.niche_discovery import find_profitable_niches
        return await find_profitable_niches(
            trends_client=self.trends_client,
            keepa_client=self.keepa_client,
            cache_manager=self.cache_manager,
            base_keywords=base_keywords,
            categories=categories,
            min_profitability_score=min_profitability_score,
            max_competition_level=max_competition_level,
            limit=limit
        )
    async def _analyze_competitor_asin(self, asin: str,
                                      include_variations: bool = False,
                                      analyze_reviews: bool = True,
                                      historical_data: bool = True) -> Dict[str, Any]:
        """Analyze competitor ASIN implementation."""
        from .tools.competitor_analysis import analyze_competitor_asin
        return await analyze_competitor_asin(
            keepa_client=self.keepa_client,
            trends_client=self.trends_client,
            cache_manager=self.cache_manager,
            asin=asin,
            include_variations=include_variations,
            analyze_reviews=analyze_reviews,
            historical_data=historical_data
        )
    async def _generate_kdp_listing(self, niche: Dict[str, Any],
                                   target_audience: str,
                                   book_type: str = "journal",
                                   unique_angle: Optional[str] = None,
                                   price_range: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        """Generate KDP listing implementation."""
        from .tools.listing_generation import generate_kdp_listing
        return await generate_kdp_listing(
            trends_client=self.trends_client,
            keepa_client=self.keepa_client,
            cache_manager=self.cache_manager,
            niche=niche,
            target_audience=target_audience,
            book_type=book_type,
            unique_angle=unique_angle,
            price_range=price_range
        )
    async def _validate_trend(self, keywords: List[str],
                             timeframe: str = "today 12-m",
                             geo: str = "US",
                             include_forecast: bool = True,
                             seasonal_analysis: bool = True) -> Dict[str, Any]:
        """Validate trend implementation."""
        from .tools.trend_validation import validate_trend
        return await validate_trend(
            trends_client=self.trends_client,
            cache_manager=self.cache_manager,
            keywords=keywords,
            timeframe=timeframe,
            geo=geo,
            include_forecast=include_forecast,
            seasonal_analysis=seasonal_analysis
        )
    async def _niche_stress_test(self, niche: Dict[str, Any],
                                test_scenarios: List[str] = None,
                                severity_level: str = "moderate",
                                time_horizon: str = "6_months") -> Dict[str, Any]:
        """Niche stress test implementation."""
        from .tools.stress_testing import niche_stress_test
        if test_scenarios is None:
            test_scenarios = ["market_saturation", "trend_decline", "increased_competition"]
        return await niche_stress_test(
            trends_client=self.trends_client,
            keepa_client=self.keepa_client,
            cache_manager=self.cache_manager,
            niche=niche,
            test_scenarios=test_scenarios,
            severity_level=severity_level,
            time_horizon=time_horizon
        )
    def get_stats(self) -> Dict[str, Any]:
        """Get agent statistics."""
        uptime = datetime.now() - self.stats["session_start"]
        stats = {
            "session_id": self.session_id,
            "uptime_seconds": uptime.total_seconds(),
            "tools_called": self.stats["tools_called"],
            "cache_hits": self.stats["cache_hits"],
            "api_calls": self.stats["api_calls"],
            "errors": self.stats["errors"],
            "cache_stats": self.cache_manager.get_stats() if self.cache_manager else {},
        }
        if self.keepa_client:
            stats["keepa_stats"] = self.keepa_client.get_stats()
        if self.trends_client:
            stats["trends_stats"] = self.trends_client.get_stats()
        return stats
    async def cleanup(self) -> None:
        """Cleanup resources."""
        if self.cache_manager:
            self.cache_manager.cleanup_expired()
        logger.info(f"KDP Strategist Agent cleanup completed (session: {self.session_id})")
    def __enter__(self):
        """Context manager entry."""
        return self
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        asyncio.run(self.cleanup())
</file>

<file path="src/kdp_strategist/agent/tools/__init__.py">
"""KDP Strategist Tools.
This module contains the core tools for the KDP Strategist AI Agent:
- niche_discovery: Find profitable publishing niches
- competitor_analysis: Analyze competitor products and strategies
- listing_generation: Generate optimized KDP listings
- trend_validation: Validate trend strength and sustainability
- stress_testing: Test niche viability under various conditions
Each tool is designed to work independently while leveraging shared
data sources and caching for optimal performance.
"""
from .niche_discovery import find_profitable_niches
from .competitor_analysis import analyze_competitor_asin
from .listing_generation import generate_kdp_listing
from .trend_validation import validate_trend
from .stress_testing import niche_stress_test
__all__ = [
    "find_profitable_niches",
    "analyze_competitor_asin",
    "generate_kdp_listing",
    "validate_trend",
    "niche_stress_test"
]
</file>

<file path="src/kdp_strategist/agent/tools/competitor_analysis.py">
"""Competitor Analysis Tool.
Analyzes Amazon product competition using Keepa data to evaluate:
- Product performance metrics (BSR, sales estimates, reviews)
- Price analysis and trends
- Market positioning and gaps
- Competitive landscape assessment
- Revenue and profitability estimates
The tool provides detailed insights into:
- Individual product analysis (ASIN-based)
- Competitive benchmarking
- Market opportunity identification
- Strategic positioning recommendations
"""
import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from statistics import mean, median
from ...data.cache_manager import CacheManager
from ...data.keepa_client import KeepaClient, ProductData
from ...models.niche_model import Niche
logger = logging.getLogger(__name__)
@dataclass
class CompetitorMetrics:
    """Metrics for a single competitor product."""
    asin: str
    title: str
    current_price: float
    bsr: Optional[int]
    category: str
    review_count: int
    rating: float
    estimated_monthly_sales: Optional[int]
    estimated_monthly_revenue: Optional[float]
    price_history: List[Tuple[datetime, float]]
    bsr_history: List[Tuple[datetime, int]]
    launch_date: Optional[datetime]
    # Calculated metrics
    price_stability: float = 0.0
    sales_trend: str = "stable"
    market_position: str = "unknown"
    competitive_strength: float = 0.0
@dataclass
class MarketAnalysis:
    """Overall market analysis for a niche or keyword."""
    keyword: str
    total_products: int
    avg_price: float
    price_range: Tuple[float, float]
    avg_reviews: float
    avg_rating: float
    market_saturation: str
    entry_barriers: str
    opportunity_score: float
    top_performers: List[CompetitorMetrics]
    market_gaps: List[Dict[str, Any]]
    recommendations: List[str]
class CompetitorAnalyzer:
    """Core analyzer for competitor data and market insights."""
    # BSR to sales estimation (rough approximations)
    BSR_SALES_MAPPING = {
        1: 3000,
        10: 1500,
        100: 300,
        1000: 50,
        10000: 10,
        100000: 2,
        1000000: 0.5
    }
    @classmethod
    def estimate_monthly_sales(cls, bsr: Optional[int], category: str = "Books") -> Optional[int]:
        """Estimate monthly sales based on BSR."""
        if not bsr or bsr <= 0:
            return None
        # Find closest BSR mapping
        for threshold, sales in sorted(cls.BSR_SALES_MAPPING.items()):
            if bsr <= threshold:
                return sales
        # For very high BSR (low sales)
        return max(1, int(cls.BSR_SALES_MAPPING[1000000] * (1000000 / bsr)))
    @classmethod
    def calculate_price_stability(cls, price_history: List[Tuple[datetime, float]]) -> float:
        """Calculate price stability score (0-100)."""
        if len(price_history) < 2:
            return 100.0
        prices = [price for _, price in price_history]
        if not prices:
            return 100.0
        # Calculate coefficient of variation
        avg_price = mean(prices)
        if avg_price == 0:
            return 0.0
        price_std = (sum((p - avg_price) ** 2 for p in prices) / len(prices)) ** 0.5
        cv = price_std / avg_price
        # Convert to stability score (lower CV = higher stability)
        stability = max(0, 100 - (cv * 100))
        return min(100, stability)
    @classmethod
    def analyze_sales_trend(cls, bsr_history: List[Tuple[datetime, int]]) -> str:
        """Analyze sales trend from BSR history."""
        if len(bsr_history) < 3:
            return "insufficient_data"
        # Sort by date
        sorted_history = sorted(bsr_history, key=lambda x: x[0])
        # Compare recent vs older BSR (lower BSR = better sales)
        recent_bsr = mean([bsr for _, bsr in sorted_history[-3:]])
        older_bsr = mean([bsr for _, bsr in sorted_history[:3]])
        if recent_bsr < older_bsr * 0.8:  # Significant improvement
            return "rising"
        elif recent_bsr > older_bsr * 1.2:  # Significant decline
            return "declining"
        else:
            return "stable"
    @classmethod
    def calculate_competitive_strength(cls, metrics: CompetitorMetrics) -> float:
        """Calculate overall competitive strength score (0-100)."""
        scores = []
        # Review count score (more reviews = stronger position)
        if metrics.review_count >= 1000:
            review_score = 90
        elif metrics.review_count >= 100:
            review_score = 70
        elif metrics.review_count >= 10:
            review_score = 50
        else:
            review_score = 20
        scores.append(review_score)
        # Rating score
        rating_score = (metrics.rating / 5.0) * 100 if metrics.rating else 50
        scores.append(rating_score)
        # Sales performance score (based on estimated sales)
        if metrics.estimated_monthly_sales:
            if metrics.estimated_monthly_sales >= 1000:
                sales_score = 95
            elif metrics.estimated_monthly_sales >= 100:
                sales_score = 80
            elif metrics.estimated_monthly_sales >= 10:
                sales_score = 60
            else:
                sales_score = 30
        else:
            sales_score = 40
        scores.append(sales_score)
        # Price stability score
        scores.append(metrics.price_stability)
        # Sales trend score
        trend_scores = {"rising": 90, "stable": 70, "declining": 30, "insufficient_data": 50}
        scores.append(trend_scores.get(metrics.sales_trend, 50))
        return mean(scores)
    @classmethod
    def identify_market_gaps(cls, competitors: List[CompetitorMetrics]) -> List[Dict[str, Any]]:
        """Identify potential market gaps and opportunities."""
        gaps = []
        if not competitors:
            return [{"type": "empty_market", "description": "No significant competition found", "opportunity": "high"}]
        # Price gap analysis
        prices = [c.current_price for c in competitors if c.current_price > 0]
        if prices:
            price_gaps = cls._find_price_gaps(prices)
            gaps.extend(price_gaps)
        # Quality gap analysis
        low_rated = [c for c in competitors if c.rating < 3.5]
        if len(low_rated) > len(competitors) * 0.3:  # 30% or more have low ratings
            gaps.append({
                "type": "quality_gap",
                "description": "Many competitors have poor ratings",
                "opportunity": "high",
                "avg_rating": mean([c.rating for c in low_rated])
            })
        # Review gap analysis
        low_review_count = [c for c in competitors if c.review_count < 50]
        if len(low_review_count) > len(competitors) * 0.5:  # 50% or more have few reviews
            gaps.append({
                "type": "review_gap",
                "description": "Many competitors have few reviews",
                "opportunity": "medium",
                "avg_reviews": mean([c.review_count for c in low_review_count])
            })
        # Content/positioning gaps (based on title analysis)
        title_keywords = cls._analyze_title_keywords([c.title for c in competitors])
        if len(title_keywords) < 10:  # Limited keyword diversity
            gaps.append({
                "type": "content_gap",
                "description": "Limited keyword diversity in titles",
                "opportunity": "medium",
                "common_keywords": list(title_keywords.keys())[:5]
            })
        return gaps
    @classmethod
    def _find_price_gaps(cls, prices: List[float]) -> List[Dict[str, Any]]:
        """Find gaps in price distribution."""
        gaps = []
        sorted_prices = sorted(prices)
        # Look for significant gaps between price points
        for i in range(len(sorted_prices) - 1):
            current = sorted_prices[i]
            next_price = sorted_prices[i + 1]
            gap_size = next_price - current
            # If gap is more than 50% of current price, it's significant
            if gap_size > current * 0.5 and gap_size > 2.0:
                gaps.append({
                    "type": "price_gap",
                    "description": f"Price gap between ${current:.2f} and ${next_price:.2f}",
                    "opportunity": "medium" if gap_size < current else "high",
                    "suggested_price": round(current + (gap_size / 2), 2)
                })
        return gaps
    @classmethod
    def _analyze_title_keywords(cls, titles: List[str]) -> Dict[str, int]:
        """Analyze keyword frequency in competitor titles."""
        keyword_counts = {}
        for title in titles:
            # Simple keyword extraction (split and clean)
            words = title.lower().split()
            for word in words:
                # Clean word (remove punctuation)
                clean_word = ''.join(c for c in word if c.isalnum())
                if len(clean_word) > 2:  # Ignore very short words
                    keyword_counts[clean_word] = keyword_counts.get(clean_word, 0) + 1
        # Return words that appear in multiple titles
        return {k: v for k, v in keyword_counts.items() if v > 1}
async def analyze_competitor_asin(
    keepa_client: KeepaClient,
    cache_manager: CacheManager,
    asin: str,
    include_history: bool = True,
    history_days: int = 90
) -> Dict[str, Any]:
    """Analyze a specific competitor product by ASIN.
    Args:
        keepa_client: Keepa API client
        cache_manager: Cache manager for performance
        asin: Amazon product ASIN to analyze
        include_history: Whether to include price/BSR history
        history_days: Number of days of history to analyze
    Returns:
        Dictionary containing detailed competitor analysis
    """
    logger.info(f"Analyzing competitor ASIN: {asin}")
    try:
        # Get product data from Keepa
        product_data = await keepa_client.get_product_data(asin)
        if not product_data:
            return {
                "error": "Product not found or data unavailable",
                "asin": asin,
                "analysis_timestamp": datetime.now().isoformat()
            }
        # Create competitor metrics
        metrics = CompetitorMetrics(
            asin=asin,
            title=product_data.title or "Unknown",
            current_price=product_data.current_price or 0.0,
            bsr=product_data.bsr,
            category=product_data.category or "Unknown",
            review_count=product_data.review_count or 0,
            rating=product_data.rating or 0.0,
            estimated_monthly_sales=CompetitorAnalyzer.estimate_monthly_sales(product_data.bsr),
            estimated_monthly_revenue=None,
            price_history=[],
            bsr_history=[],
            launch_date=product_data.launch_date
        )
        # Calculate estimated revenue
        if metrics.estimated_monthly_sales and metrics.current_price:
            metrics.estimated_monthly_revenue = metrics.estimated_monthly_sales * metrics.current_price
        # Get historical data if requested
        if include_history:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=history_days)
            # Get price history
            price_history = await keepa_client.get_price_history(asin, start_date, end_date)
            metrics.price_history = price_history or []
            # Get BSR history
            bsr_history = await keepa_client.get_bsr_history(asin, start_date, end_date)
            metrics.bsr_history = bsr_history or []
            # Calculate derived metrics
            metrics.price_stability = CompetitorAnalyzer.calculate_price_stability(metrics.price_history)
            metrics.sales_trend = CompetitorAnalyzer.analyze_sales_trend(metrics.bsr_history)
        # Calculate competitive strength
        metrics.competitive_strength = CompetitorAnalyzer.calculate_competitive_strength(metrics)
        # Determine market position
        if metrics.competitive_strength >= 80:
            metrics.market_position = "dominant"
        elif metrics.competitive_strength >= 60:
            metrics.market_position = "strong"
        elif metrics.competitive_strength >= 40:
            metrics.market_position = "moderate"
        else:
            metrics.market_position = "weak"
        # Generate insights and recommendations
        insights = _generate_competitor_insights(metrics)
        recommendations = _generate_competitive_recommendations(metrics)
        result = {
            "asin": asin,
            "basic_info": {
                "title": metrics.title,
                "category": metrics.category,
                "current_price": metrics.current_price,
                "launch_date": metrics.launch_date.isoformat() if metrics.launch_date else None
            },
            "performance_metrics": {
                "bsr": metrics.bsr,
                "review_count": metrics.review_count,
                "rating": metrics.rating,
                "estimated_monthly_sales": metrics.estimated_monthly_sales,
                "estimated_monthly_revenue": metrics.estimated_monthly_revenue
            },
            "competitive_analysis": {
                "competitive_strength": metrics.competitive_strength,
                "market_position": metrics.market_position,
                "price_stability": metrics.price_stability,
                "sales_trend": metrics.sales_trend
            },
            "historical_data": {
                "price_history_points": len(metrics.price_history),
                "bsr_history_points": len(metrics.bsr_history),
                "analysis_period_days": history_days if include_history else 0
            },
            "insights": insights,
            "recommendations": recommendations,
            "analysis_timestamp": datetime.now().isoformat()
        }
        logger.info(f"Competitor analysis completed for ASIN: {asin}")
        return result
    except Exception as e:
        logger.error(f"Competitor analysis failed for ASIN {asin}: {e}")
        return {
            "error": str(e),
            "asin": asin,
            "analysis_timestamp": datetime.now().isoformat()
        }
async def analyze_market_competition(
    keepa_client: KeepaClient,
    cache_manager: CacheManager,
    keyword: str,
    category: Optional[str] = None,
    max_products: int = 20,
    min_reviews: int = 0
) -> Dict[str, Any]:
    """Analyze overall market competition for a keyword/niche.
    Args:
        keepa_client: Keepa API client
        cache_manager: Cache manager for performance
        keyword: Keyword to analyze competition for
        category: Amazon category to focus on
        max_products: Maximum number of products to analyze
        min_reviews: Minimum review count filter
    Returns:
        Dictionary containing market competition analysis
    """
    logger.info(f"Analyzing market competition for keyword: {keyword}")
    try:
        # Search for products
        products = await keepa_client.search_products(
            keyword, 
            category=category, 
            limit=max_products
        )
        if not products:
            return {
                "keyword": keyword,
                "total_products": 0,
                "message": "No products found for this keyword",
                "analysis_timestamp": datetime.now().isoformat()
            }
        # Filter products by minimum reviews
        filtered_products = [p for p in products if (p.review_count or 0) >= min_reviews]
        # Convert to competitor metrics
        competitors = []
        for product in filtered_products:
            metrics = CompetitorMetrics(
                asin=product.asin,
                title=product.title or "Unknown",
                current_price=product.current_price or 0.0,
                bsr=product.bsr,
                category=product.category or "Unknown",
                review_count=product.review_count or 0,
                rating=product.rating or 0.0,
                estimated_monthly_sales=CompetitorAnalyzer.estimate_monthly_sales(product.bsr),
                estimated_monthly_revenue=None,
                price_history=[],
                bsr_history=[],
                launch_date=product.launch_date
            )
            # Calculate estimated revenue
            if metrics.estimated_monthly_sales and metrics.current_price:
                metrics.estimated_monthly_revenue = metrics.estimated_monthly_sales * metrics.current_price
            # Calculate competitive strength (without historical data)
            metrics.competitive_strength = CompetitorAnalyzer.calculate_competitive_strength(metrics)
            competitors.append(metrics)
        # Analyze market
        market_analysis = _analyze_market_metrics(keyword, competitors)
        # Generate market insights
        market_insights = _generate_market_insights(market_analysis)
        result = {
            "keyword": keyword,
            "category": category,
            "total_products_found": len(products),
            "analyzed_products": len(competitors),
            "market_metrics": {
                "avg_price": market_analysis.avg_price,
                "price_range": {"min": market_analysis.price_range[0], "max": market_analysis.price_range[1]},
                "avg_reviews": market_analysis.avg_reviews,
                "avg_rating": market_analysis.avg_rating,
                "market_saturation": market_analysis.market_saturation,
                "entry_barriers": market_analysis.entry_barriers,
                "opportunity_score": market_analysis.opportunity_score
            },
            "top_performers": [
                {
                    "asin": comp.asin,
                    "title": comp.title[:100] + "..." if len(comp.title) > 100 else comp.title,
                    "price": comp.current_price,
                    "reviews": comp.review_count,
                    "rating": comp.rating,
                    "competitive_strength": comp.competitive_strength,
                    "estimated_monthly_sales": comp.estimated_monthly_sales
                }
                for comp in market_analysis.top_performers
            ],
            "market_gaps": market_analysis.market_gaps,
            "insights": market_insights,
            "recommendations": market_analysis.recommendations,
            "analysis_timestamp": datetime.now().isoformat()
        }
        logger.info(f"Market competition analysis completed for keyword: {keyword}")
        return result
    except Exception as e:
        logger.error(f"Market competition analysis failed for keyword {keyword}: {e}")
        return {
            "error": str(e),
            "keyword": keyword,
            "analysis_timestamp": datetime.now().isoformat()
        }
def _generate_competitor_insights(metrics: CompetitorMetrics) -> List[str]:
    """Generate insights about a specific competitor."""
    insights = []
    # Performance insights
    if metrics.estimated_monthly_sales:
        if metrics.estimated_monthly_sales >= 1000:
            insights.append(f"High-performing product with estimated {metrics.estimated_monthly_sales:,} monthly sales")
        elif metrics.estimated_monthly_sales >= 100:
            insights.append(f"Moderate performer with estimated {metrics.estimated_monthly_sales:,} monthly sales")
        else:
            insights.append(f"Low sales volume with estimated {metrics.estimated_monthly_sales:,} monthly sales")
    # Review insights
    if metrics.review_count >= 1000:
        insights.append("Well-established product with strong review base")
    elif metrics.review_count >= 100:
        insights.append("Moderately established with decent review count")
    elif metrics.review_count < 10:
        insights.append("New or low-visibility product with few reviews")
    # Rating insights
    if metrics.rating >= 4.5:
        insights.append("Excellent customer satisfaction with high ratings")
    elif metrics.rating >= 4.0:
        insights.append("Good customer satisfaction")
    elif metrics.rating < 3.5:
        insights.append("Poor customer satisfaction - potential opportunity")
    # Price insights
    if metrics.current_price >= 20:
        insights.append("Premium pricing strategy")
    elif metrics.current_price <= 5:
        insights.append("Budget/low-cost positioning")
    # Trend insights
    if metrics.sales_trend == "rising":
        insights.append("Sales momentum is increasing")
    elif metrics.sales_trend == "declining":
        insights.append("Sales appear to be declining")
    return insights
def _generate_competitive_recommendations(metrics: CompetitorMetrics) -> List[str]:
    """Generate competitive recommendations based on analysis."""
    recommendations = []
    # Competitive positioning
    if metrics.competitive_strength < 50:
        recommendations.append("Weak competitor - consider direct competition with better quality/marketing")
    elif metrics.competitive_strength > 80:
        recommendations.append("Strong competitor - avoid direct competition, find differentiation angle")
    # Pricing recommendations
    if metrics.current_price > 15:
        recommendations.append("Consider lower-priced alternative to capture price-sensitive customers")
    elif metrics.current_price < 8:
        recommendations.append("Opportunity for premium positioning with higher quality")
    # Quality recommendations
    if metrics.rating < 4.0:
        recommendations.append("Focus on quality improvements to outperform this competitor")
    # Market entry recommendations
    if metrics.review_count < 50:
        recommendations.append("Low review count suggests market entry opportunity")
    return recommendations
def _analyze_market_metrics(keyword: str, competitors: List[CompetitorMetrics]) -> MarketAnalysis:
    """Analyze overall market metrics."""
    if not competitors:
        return MarketAnalysis(
            keyword=keyword,
            total_products=0,
            avg_price=0,
            price_range=(0, 0),
            avg_reviews=0,
            avg_rating=0,
            market_saturation="unknown",
            entry_barriers="unknown",
            opportunity_score=50,
            top_performers=[],
            market_gaps=[],
            recommendations=[]
        )
    # Calculate basic metrics
    prices = [c.current_price for c in competitors if c.current_price > 0]
    reviews = [c.review_count for c in competitors]
    ratings = [c.rating for c in competitors if c.rating > 0]
    avg_price = mean(prices) if prices else 0
    price_range = (min(prices), max(prices)) if prices else (0, 0)
    avg_reviews = mean(reviews) if reviews else 0
    avg_rating = mean(ratings) if ratings else 0
    # Determine market saturation
    high_review_products = len([c for c in competitors if c.review_count >= 100])
    if high_review_products >= len(competitors) * 0.7:
        market_saturation = "high"
    elif high_review_products >= len(competitors) * 0.3:
        market_saturation = "medium"
    else:
        market_saturation = "low"
    # Determine entry barriers
    strong_competitors = len([c for c in competitors if c.competitive_strength >= 70])
    if strong_competitors >= len(competitors) * 0.5:
        entry_barriers = "high"
    elif strong_competitors >= len(competitors) * 0.2:
        entry_barriers = "medium"
    else:
        entry_barriers = "low"
    # Calculate opportunity score
    opportunity_score = _calculate_opportunity_score(competitors, market_saturation, entry_barriers)
    # Get top performers
    top_performers = sorted(competitors, key=lambda c: c.competitive_strength, reverse=True)[:5]
    # Identify market gaps
    market_gaps = CompetitorAnalyzer.identify_market_gaps(competitors)
    # Generate recommendations
    recommendations = _generate_market_recommendations(competitors, market_saturation, entry_barriers)
    return MarketAnalysis(
        keyword=keyword,
        total_products=len(competitors),
        avg_price=avg_price,
        price_range=price_range,
        avg_reviews=avg_reviews,
        avg_rating=avg_rating,
        market_saturation=market_saturation,
        entry_barriers=entry_barriers,
        opportunity_score=opportunity_score,
        top_performers=top_performers,
        market_gaps=market_gaps,
        recommendations=recommendations
    )
def _calculate_opportunity_score(competitors: List[CompetitorMetrics], 
                                saturation: str, barriers: str) -> float:
    """Calculate market opportunity score (0-100)."""
    base_score = 50
    # Adjust for market saturation
    saturation_adjustments = {"low": 20, "medium": 0, "high": -20}
    base_score += saturation_adjustments.get(saturation, 0)
    # Adjust for entry barriers
    barrier_adjustments = {"low": 15, "medium": 0, "high": -15}
    base_score += barrier_adjustments.get(barriers, 0)
    # Adjust for competitor quality
    avg_strength = mean([c.competitive_strength for c in competitors]) if competitors else 50
    if avg_strength < 40:
        base_score += 15  # Weak competition = opportunity
    elif avg_strength > 70:
        base_score -= 10  # Strong competition = challenge
    # Adjust for market gaps
    gaps = CompetitorAnalyzer.identify_market_gaps(competitors)
    high_opportunity_gaps = len([g for g in gaps if g.get("opportunity") == "high"])
    base_score += high_opportunity_gaps * 5
    return max(0, min(100, base_score))
def _generate_market_insights(analysis: MarketAnalysis) -> List[str]:
    """Generate market-level insights."""
    insights = []
    # Market size insights
    if analysis.total_products >= 50:
        insights.append(f"Large market with {analysis.total_products} competing products")
    elif analysis.total_products >= 10:
        insights.append(f"Moderate market size with {analysis.total_products} competitors")
    else:
        insights.append(f"Small market with only {analysis.total_products} competitors")
    # Price insights
    if analysis.avg_price >= 15:
        insights.append(f"Premium market with average price of ${analysis.avg_price:.2f}")
    elif analysis.avg_price <= 8:
        insights.append(f"Budget market with average price of ${analysis.avg_price:.2f}")
    # Competition insights
    if analysis.market_saturation == "high":
        insights.append("Highly saturated market with established competitors")
    elif analysis.market_saturation == "low":
        insights.append("Emerging market with growth opportunities")
    # Opportunity insights
    if analysis.opportunity_score >= 70:
        insights.append("High opportunity market with good entry potential")
    elif analysis.opportunity_score <= 40:
        insights.append("Challenging market with limited opportunities")
    return insights
def _generate_market_recommendations(competitors: List[CompetitorMetrics], 
                                   saturation: str, barriers: str) -> List[str]:
    """Generate market-level recommendations."""
    recommendations = []
    # Entry strategy recommendations
    if barriers == "low" and saturation == "low":
        recommendations.append("Excellent market entry opportunity - move quickly to establish position")
    elif barriers == "high":
        recommendations.append("Consider niche differentiation or unique value proposition")
    # Pricing strategy recommendations
    if competitors:
        prices = [c.current_price for c in competitors if c.current_price > 0]
        if prices:
            min_price, max_price = min(prices), max(prices)
            if max_price - min_price > 10:
                recommendations.append(f"Wide price range (${min_price:.2f}-${max_price:.2f}) suggests segmentation opportunities")
    # Quality strategy recommendations
    low_rated = [c for c in competitors if c.rating < 4.0]
    if len(low_rated) > len(competitors) * 0.3:
        recommendations.append("Focus on quality to differentiate from poorly-rated competitors")
    # Market timing recommendations
    if saturation == "low":
        recommendations.append("Early market - focus on establishing brand and capturing market share")
    elif saturation == "high":
        recommendations.append("Mature market - focus on differentiation and niche targeting")
    return recommendations
</file>

<file path="src/kdp_strategist/agent/tools/listing_generation.py">
"""Listing Generation Tool.
Generates optimized KDP book listings including:
- SEO-optimized titles with keyword integration
- Compelling book descriptions with marketing copy
- Strategic keyword selection and placement
- Category recommendations and positioning
- Pricing strategy based on market analysis
- Content suggestions and unique angles
The tool creates listings that:
- Maximize discoverability through SEO
- Convert browsers into buyers
- Position products competitively
- Comply with KDP requirements and best practices
"""
import asyncio
import logging
import re
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
from ...data.cache_manager import CacheManager
from ...data.keepa_client import KeepaClient
from ...data.trends_client import TrendsClient
from ...models.listing_model import KDPListing, ContentType, TargetAudience
from ...models.niche_model import Niche
from ...models.trend_model import TrendAnalysis
logger = logging.getLogger(__name__)
class ListingStyle(Enum):
    """Different listing styles for various book types."""
    PROFESSIONAL = "professional"
    CREATIVE = "creative"
    EDUCATIONAL = "educational"
    INSPIRATIONAL = "inspirational"
    PRACTICAL = "practical"
@dataclass
class TitleTemplate:
    """Template for generating book titles."""
    pattern: str
    style: ListingStyle
    target_audience: TargetAudience
    examples: List[str]
    seo_strength: float
@dataclass
class DescriptionTemplate:
    """Template for generating book descriptions."""
    structure: List[str]  # List of section types
    style: ListingStyle
    word_count_range: Tuple[int, int]
    conversion_elements: List[str]
class TitleGenerator:
    """Generates SEO-optimized book titles."""
    # Title templates for different book types and audiences
    TITLE_TEMPLATES = {
        ContentType.JOURNAL: [
            TitleTemplate(
                pattern="{primary_keyword} Journal: {unique_angle} for {audience}",
                style=ListingStyle.PRACTICAL,
                target_audience=TargetAudience.ADULTS,
                examples=["Gratitude Journal: Daily Prompts for Mindful Living"],
                seo_strength=0.85
            ),
            TitleTemplate(
                pattern="The Ultimate {primary_keyword} {book_type}: {benefit} in {timeframe}",
                style=ListingStyle.PROFESSIONAL,
                target_audience=TargetAudience.PROFESSIONALS,
                examples=["The Ultimate Productivity Planner: Achieve Your Goals in 90 Days"],
                seo_strength=0.90
            ),
            TitleTemplate(
                pattern="{adjective} {primary_keyword} {book_type} for {audience}: {unique_angle}",
                style=ListingStyle.CREATIVE,
                target_audience=TargetAudience.GENERAL,
                examples=["Beautiful Mindfulness Journal for Women: Self-Care Through Daily Reflection"],
                seo_strength=0.80
            )
        ],
        ContentType.PLANNER: [
            TitleTemplate(
                pattern="{year} {primary_keyword} Planner: {unique_angle} for {audience}",
                style=ListingStyle.PRACTICAL,
                target_audience=TargetAudience.ADULTS,
                examples=["2024 Goal Planner: Monthly & Weekly Planning for Success"],
                seo_strength=0.88
            ),
            TitleTemplate(
                pattern="The {adjective} {primary_keyword} Planner: {benefit} System",
                style=ListingStyle.PROFESSIONAL,
                target_audience=TargetAudience.PROFESSIONALS,
                examples=["The Complete Business Planner: Strategic Growth System"],
                seo_strength=0.85
            )
        ],
        ContentType.WORKBOOK: [
            TitleTemplate(
                pattern="{primary_keyword} Workbook: {unique_angle} for {audience}",
                style=ListingStyle.EDUCATIONAL,
                target_audience=TargetAudience.STUDENTS,
                examples=["Math Practice Workbook: Essential Skills for Grade 3"],
                seo_strength=0.82
            ),
            TitleTemplate(
                pattern="Master {primary_keyword}: {benefit} Workbook with {feature}",
                style=ListingStyle.PROFESSIONAL,
                target_audience=TargetAudience.ADULTS,
                examples=["Master Public Speaking: Confidence Building Workbook with Exercises"],
                seo_strength=0.87
            )
        ]
    }
    # Word banks for title generation
    ADJECTIVES = {
        ListingStyle.PROFESSIONAL: ["Complete", "Ultimate", "Comprehensive", "Advanced", "Strategic"],
        ListingStyle.CREATIVE: ["Beautiful", "Inspiring", "Elegant", "Artistic", "Unique"],
        ListingStyle.EDUCATIONAL: ["Essential", "Fundamental", "Complete", "Step-by-Step", "Practical"],
        ListingStyle.INSPIRATIONAL: ["Transformative", "Life-Changing", "Empowering", "Motivational", "Uplifting"],
        ListingStyle.PRACTICAL: ["Daily", "Simple", "Effective", "Proven", "Easy"]
    }
    UNIQUE_ANGLES = {
        ContentType.JOURNAL: [
            "Daily Prompts and Reflections",
            "Guided Self-Discovery",
            "Mindful Living Practices",
            "Personal Growth Journey",
            "Habit Tracking and Goals"
        ],
        ContentType.PLANNER: [
            "Monthly and Weekly Planning",
            "Goal Setting and Achievement",
            "Time Management System",
            "Productivity and Focus",
            "Life Organization Method"
        ],
        ContentType.WORKBOOK: [
            "Step-by-Step Exercises",
            "Practice Problems and Solutions",
            "Skill Building Activities",
            "Interactive Learning",
            "Hands-On Practice"
        ]
    }
    @classmethod
    def generate_titles(
        cls,
        primary_keyword: str,
        content_type: ContentType,
        target_audience: TargetAudience,
        style_preference: Optional[ListingStyle] = None,
        additional_keywords: Optional[List[str]] = None,
        count: int = 5
    ) -> List[Dict[str, Any]]:
        """Generate multiple title options."""
        titles = []
        templates = cls.TITLE_TEMPLATES.get(content_type, [])
        if not templates:
            # Fallback generic template
            templates = [TitleTemplate(
                pattern="{primary_keyword} {book_type}: {unique_angle}",
                style=ListingStyle.PRACTICAL,
                target_audience=TargetAudience.GENERAL,
                examples=[],
                seo_strength=0.75
            )]
        # Filter templates by style preference
        if style_preference:
            templates = [t for t in templates if t.style == style_preference]
        # Generate titles from templates
        for template in templates[:count]:
            title_data = cls._generate_from_template(
                template, primary_keyword, content_type, target_audience, additional_keywords
            )
            titles.append(title_data)
        # Sort by SEO strength
        titles.sort(key=lambda x: x["seo_score"], reverse=True)
        return titles[:count]
    @classmethod
    def _generate_from_template(
        cls,
        template: TitleTemplate,
        primary_keyword: str,
        content_type: ContentType,
        target_audience: TargetAudience,
        additional_keywords: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Generate a title from a specific template."""
        # Get word banks
        adjectives = cls.ADJECTIVES.get(template.style, cls.ADJECTIVES[ListingStyle.PRACTICAL])
        unique_angles = cls.UNIQUE_ANGLES.get(content_type, ["Comprehensive Guide"])
        # Select words
        adjective = adjectives[0]  # Use first (most relevant) adjective
        unique_angle = unique_angles[0]  # Use first unique angle
        # Map content type to book type
        book_type_map = {
            ContentType.JOURNAL: "Journal",
            ContentType.PLANNER: "Planner",
            ContentType.WORKBOOK: "Workbook",
            ContentType.NOTEBOOK: "Notebook",
            ContentType.LOG_BOOK: "Log Book"
        }
        book_type = book_type_map.get(content_type, "Book")
        # Map target audience to readable format
        audience_map = {
            TargetAudience.CHILDREN: "Kids",
            TargetAudience.TEENS: "Teens",
            TargetAudience.ADULTS: "Adults",
            TargetAudience.SENIORS: "Seniors",
            TargetAudience.PROFESSIONALS: "Professionals",
            TargetAudience.STUDENTS: "Students",
            TargetAudience.GENERAL: "Everyone"
        }
        audience = audience_map.get(target_audience, "Adults")
        # Generate title
        title = template.pattern.format(
            primary_keyword=primary_keyword.title(),
            adjective=adjective,
            unique_angle=unique_angle,
            book_type=book_type,
            audience=audience,
            year=datetime.now().year,
            benefit="Enhanced Productivity",  # Default benefit
            timeframe="30 Days"  # Default timeframe
        )
        # Calculate SEO score
        seo_score = cls._calculate_title_seo_score(title, primary_keyword, additional_keywords)
        # Ensure title meets KDP requirements
        title = cls._optimize_title_length(title)
        return {
            "title": title,
            "template_style": template.style.value,
            "seo_score": seo_score,
            "character_count": len(title),
            "keyword_density": cls._calculate_keyword_density(title, primary_keyword),
            "readability": cls._assess_title_readability(title)
        }
    @classmethod
    def _calculate_title_seo_score(cls, title: str, primary_keyword: str, 
                                  additional_keywords: Optional[List[str]] = None) -> float:
        """Calculate SEO score for a title."""
        score = 0.0
        title_lower = title.lower()
        # Primary keyword presence (40% of score)
        if primary_keyword.lower() in title_lower:
            score += 40
            # Bonus for keyword at beginning
            if title_lower.startswith(primary_keyword.lower()):
                score += 10
        # Additional keywords (30% of score)
        if additional_keywords:
            keyword_count = sum(1 for kw in additional_keywords if kw.lower() in title_lower)
            score += min(30, keyword_count * 10)
        # Title length optimization (20% of score)
        title_length = len(title)
        if 30 <= title_length <= 60:  # Optimal length
            score += 20
        elif 20 <= title_length <= 80:  # Acceptable length
            score += 15
        else:
            score += 5
        # Readability and appeal (10% of score)
        word_count = len(title.split())
        if 3 <= word_count <= 8:  # Good word count
            score += 10
        elif word_count <= 12:
            score += 5
        return min(100, score)
    @classmethod
    def _calculate_keyword_density(cls, title: str, keyword: str) -> float:
        """Calculate keyword density in title."""
        title_words = title.lower().split()
        keyword_words = keyword.lower().split()
        if not title_words or not keyword_words:
            return 0.0
        # Count keyword occurrences
        keyword_count = 0
        for i in range(len(title_words) - len(keyword_words) + 1):
            if title_words[i:i+len(keyword_words)] == keyword_words:
                keyword_count += 1
        return (keyword_count * len(keyword_words)) / len(title_words) * 100
    @classmethod
    def _assess_title_readability(cls, title: str) -> str:
        """Assess title readability."""
        word_count = len(title.split())
        avg_word_length = sum(len(word) for word in title.split()) / word_count if word_count > 0 else 0
        if word_count <= 6 and avg_word_length <= 6:
            return "excellent"
        elif word_count <= 8 and avg_word_length <= 8:
            return "good"
        elif word_count <= 12:
            return "fair"
        else:
            return "poor"
    @classmethod
    def _optimize_title_length(cls, title: str) -> str:
        """Ensure title meets KDP length requirements."""
        # KDP title limit is 200 characters
        if len(title) <= 200:
            return title
        # Truncate while preserving meaning
        words = title.split()
        truncated = ""
        for word in words:
            if len(truncated + " " + word) <= 197:  # Leave room for "..."
                truncated += (" " if truncated else "") + word
            else:
                break
        return truncated + "..." if truncated != title else title
class DescriptionGenerator:
    """Generates compelling book descriptions."""
    # Description templates by style
    DESCRIPTION_TEMPLATES = {
        ListingStyle.PROFESSIONAL: DescriptionTemplate(
            structure=[
                "hook", "problem_statement", "solution_overview", 
                "key_features", "benefits", "target_audience", "call_to_action"
            ],
            style=ListingStyle.PROFESSIONAL,
            word_count_range=(150, 250),
            conversion_elements=["social_proof", "urgency", "guarantee"]
        ),
        ListingStyle.CREATIVE: DescriptionTemplate(
            structure=[
                "emotional_hook", "story_element", "transformation", 
                "unique_features", "emotional_benefits", "call_to_action"
            ],
            style=ListingStyle.CREATIVE,
            word_count_range=(120, 200),
            conversion_elements=["emotional_appeal", "visualization", "community"]
        ),
        ListingStyle.EDUCATIONAL: DescriptionTemplate(
            structure=[
                "learning_objective", "curriculum_overview", "methodology", 
                "skill_outcomes", "target_learners", "call_to_action"
            ],
            style=ListingStyle.EDUCATIONAL,
            word_count_range=(180, 280),
            conversion_elements=["credibility", "progress_tracking", "results"]
        )
    }
    # Content blocks for different sections
    CONTENT_BLOCKS = {
        "hook": {
            ContentType.JOURNAL: [
                "Transform your daily routine with intentional reflection and mindful living.",
                "Discover the power of journaling to unlock your potential and create lasting change.",
                "Start each day with purpose and end it with gratitude using this comprehensive journal."
            ],
            ContentType.PLANNER: [
                "Take control of your time and achieve your biggest goals with strategic planning.",
                "Turn your dreams into actionable plans with this comprehensive planning system.",
                "Master productivity and create the life you want with proven planning methods."
            ]
        },
        "problem_statement": {
            "general": [
                "Feeling overwhelmed by daily tasks and long-term goals?",
                "Struggling to maintain consistency in your personal development?",
                "Ready to break free from unproductive habits and create positive change?"
            ]
        },
        "solution_overview": {
            ContentType.JOURNAL: [
                "This thoughtfully designed journal provides the structure and guidance you need.",
                "Combining proven techniques with beautiful design, this journal makes reflection effortless."
            ],
            ContentType.PLANNER: [
                "This comprehensive planner system breaks down complex goals into manageable steps.",
                "With monthly, weekly, and daily planning pages, you'll stay organized and focused."
            ]
        }
    }
    @classmethod
    def generate_description(
        cls,
        title: str,
        content_type: ContentType,
        target_audience: TargetAudience,
        primary_keyword: str,
        unique_features: List[str],
        style: ListingStyle = ListingStyle.PROFESSIONAL,
        include_keywords: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Generate a compelling book description."""
        template = cls.DESCRIPTION_TEMPLATES.get(style, cls.DESCRIPTION_TEMPLATES[ListingStyle.PROFESSIONAL])
        # Build description sections
        sections = []
        for section_type in template.structure:
            section_content = cls._generate_section(
                section_type, content_type, target_audience, primary_keyword, unique_features
            )
            if section_content:
                sections.append(section_content)
        # Combine sections
        description = "\n\n".join(sections)
        # Optimize for keywords
        if include_keywords:
            description = cls._optimize_keyword_placement(description, include_keywords)
        # Ensure compliance with KDP requirements
        description = cls._ensure_kdp_compliance(description)
        # Calculate metrics
        metrics = cls._calculate_description_metrics(description, primary_keyword, include_keywords)
        return {
            "description": description,
            "word_count": len(description.split()),
            "character_count": len(description),
            "style": style.value,
            "seo_score": metrics["seo_score"],
            "readability_score": metrics["readability_score"],
            "conversion_elements": template.conversion_elements,
            "compliance_check": cls._check_kdp_compliance(description)
        }
    @classmethod
    def _generate_section(
        cls,
        section_type: str,
        content_type: ContentType,
        target_audience: TargetAudience,
        primary_keyword: str,
        unique_features: List[str]
    ) -> str:
        """Generate content for a specific section."""
        if section_type == "hook":
            hooks = cls.CONTENT_BLOCKS["hook"].get(content_type, [
                f"Discover the power of {primary_keyword.lower()} to transform your daily routine."
            ])
            return hooks[0]
        elif section_type == "problem_statement":
            problems = cls.CONTENT_BLOCKS["problem_statement"]["general"]
            return problems[0]
        elif section_type == "solution_overview":
            solutions = cls.CONTENT_BLOCKS["solution_overview"].get(content_type, [
                f"This comprehensive {content_type.value.lower()} provides everything you need."
            ])
            return solutions[0]
        elif section_type == "key_features":
            if unique_features:
                features_text = "\n".join([f"‚Ä¢ {feature}" for feature in unique_features[:5]])
                return f"Key Features:\n{features_text}"
            return "Designed with your success in mind, featuring intuitive layouts and proven methodologies."
        elif section_type == "benefits":
            return cls._generate_benefits_section(content_type, target_audience)
        elif section_type == "target_audience":
            return cls._generate_audience_section(target_audience, content_type)
        elif section_type == "call_to_action":
            return "Start your transformation today. Order now and begin your journey to success!"
        return ""
    @classmethod
    def _generate_benefits_section(cls, content_type: ContentType, target_audience: TargetAudience) -> str:
        """Generate benefits section based on content type and audience."""
        benefits_map = {
            ContentType.JOURNAL: [
                "Develop greater self-awareness and emotional intelligence",
                "Build consistent reflection habits that stick",
                "Track your progress and celebrate your growth",
                "Reduce stress and increase mental clarity"
            ],
            ContentType.PLANNER: [
                "Achieve your goals faster with strategic planning",
                "Improve time management and productivity",
                "Reduce overwhelm and increase focus",
                "Create better work-life balance"
            ],
            ContentType.WORKBOOK: [
                "Master new skills through hands-on practice",
                "Build confidence with step-by-step guidance",
                "Track your learning progress effectively",
                "Apply knowledge immediately for better retention"
            ]
        }
        benefits = benefits_map.get(content_type, ["Achieve your goals with this comprehensive resource"])
        return "What You'll Gain:\n" + "\n".join([f"‚Ä¢ {benefit}" for benefit in benefits[:4]])
    @classmethod
    def _generate_audience_section(cls, target_audience: TargetAudience, content_type: ContentType) -> str:
        """Generate target audience section."""
        audience_descriptions = {
            TargetAudience.PROFESSIONALS: "Perfect for busy professionals seeking to optimize their productivity and achieve career goals.",
            TargetAudience.STUDENTS: "Ideal for students looking to improve study habits and academic performance.",
            TargetAudience.ADULTS: "Designed for adults ready to take control of their personal development journey.",
            TargetAudience.GENERAL: "Suitable for anyone committed to personal growth and positive change."
        }
        return audience_descriptions.get(target_audience, 
            f"Perfect for anyone interested in {content_type.value.lower().replace('_', ' ')} and personal development.")
    @classmethod
    def _optimize_keyword_placement(cls, description: str, keywords: List[str]) -> str:
        """Optimize keyword placement in description."""
        # This is a simplified implementation
        # In practice, you'd want more sophisticated NLP-based optimization
        optimized = description
        for keyword in keywords[:3]:  # Limit to top 3 keywords
            if keyword.lower() not in optimized.lower():
                # Try to naturally incorporate the keyword
                optimized = optimized.replace(
                    "personal development", 
                    f"{keyword} and personal development", 
                    1
                )
        return optimized
    @classmethod
    def _ensure_kdp_compliance(cls, description: str) -> str:
        """Ensure description complies with KDP guidelines."""
        # Remove any prohibited content
        prohibited_phrases = [
            "best seller", "#1 bestseller", "award winning",
            "reviews", "rating", "customer feedback"
        ]
        cleaned = description
        for phrase in prohibited_phrases:
            cleaned = re.sub(phrase, "", cleaned, flags=re.IGNORECASE)
        # Ensure length limits (4000 characters for KDP)
        if len(cleaned) > 4000:
            cleaned = cleaned[:3997] + "..."
        return cleaned.strip()
    @classmethod
    def _calculate_description_metrics(cls, description: str, primary_keyword: str, 
                                     keywords: Optional[List[str]] = None) -> Dict[str, float]:
        """Calculate various metrics for the description."""
        words = description.split()
        word_count = len(words)
        # SEO Score
        seo_score = 0
        if primary_keyword.lower() in description.lower():
            seo_score += 30
        if keywords:
            keyword_count = sum(1 for kw in keywords if kw.lower() in description.lower())
            seo_score += min(40, keyword_count * 10)
        # Length optimization
        if 150 <= word_count <= 250:
            seo_score += 20
        elif 100 <= word_count <= 300:
            seo_score += 15
        # Structure bonus
        if "‚Ä¢" in description:  # Has bullet points
            seo_score += 10
        # Readability Score (simplified)
        avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0
        sentences = description.count('.') + description.count('!') + description.count('?')
        avg_sentence_length = word_count / sentences if sentences > 0 else word_count
        readability_score = 100
        if avg_word_length > 6:
            readability_score -= 10
        if avg_sentence_length > 20:
            readability_score -= 15
        return {
            "seo_score": min(100, seo_score),
            "readability_score": max(0, readability_score)
        }
    @classmethod
    def _check_kdp_compliance(cls, description: str) -> Dict[str, bool]:
        """Check KDP compliance requirements."""
        return {
            "length_compliant": len(description) <= 4000,
            "no_prohibited_content": not any(phrase in description.lower() for phrase in [
                "best seller", "#1", "award", "review", "rating"
            ]),
            "proper_formatting": "\n" in description,  # Has paragraph breaks
            "call_to_action_present": any(phrase in description.lower() for phrase in [
                "order", "buy", "get", "start", "begin"
            ])
        }
async def generate_kdp_listing(
    trends_client: Optional[TrendsClient],
    keepa_client: Optional[KeepaClient],
    cache_manager: CacheManager,
    niche: Niche,
    content_type: ContentType,
    target_audience: TargetAudience,
    unique_angle: str,
    style_preference: Optional[str] = None,
    include_pricing: bool = True
) -> Dict[str, Any]:
    """Generate a complete KDP listing.
    Args:
        trends_client: Google Trends client for keyword validation
        keepa_client: Keepa client for pricing analysis
        cache_manager: Cache manager for performance
        niche: Niche data for the listing
        content_type: Type of content (journal, planner, etc.)
        target_audience: Target audience for the book
        unique_angle: Unique selling proposition
        style_preference: Preferred listing style
        include_pricing: Whether to include pricing recommendations
    Returns:
        Dictionary containing complete listing data
    """
    logger.info(f"Generating KDP listing for niche: {niche.primary_keyword}")
    try:
        # Parse style preference
        style = ListingStyle.PROFESSIONAL
        if style_preference:
            try:
                style = ListingStyle(style_preference.lower())
            except ValueError:
                logger.warning(f"Invalid style preference: {style_preference}")
        # Generate titles
        title_options = TitleGenerator.generate_titles(
            primary_keyword=niche.primary_keyword,
            content_type=content_type,
            target_audience=target_audience,
            style_preference=style,
            additional_keywords=niche.keywords[:5],
            count=3
        )
        # Select best title
        best_title = title_options[0]["title"] if title_options else f"{niche.primary_keyword.title()} {content_type.value.title()}"
        # Prepare unique features
        unique_features = [
            unique_angle,
            f"Optimized for {target_audience.value.replace('_', ' ')}",
            "High-quality design and layout",
            "Proven methodology and structure"
        ]
        # Generate description
        description_data = DescriptionGenerator.generate_description(
            title=best_title,
            content_type=content_type,
            target_audience=target_audience,
            primary_keyword=niche.primary_keyword,
            unique_features=unique_features,
            style=style,
            include_keywords=niche.keywords[:7]
        )
        # Generate keyword recommendations
        keyword_recommendations = await _generate_keyword_recommendations(
            trends_client, niche, content_type, target_audience
        )
        # Generate category recommendations
        category_recommendations = _generate_category_recommendations(
            content_type, target_audience, niche.category
        )
        # Generate pricing recommendations
        pricing_data = None
        if include_pricing and keepa_client:
            pricing_data = await _generate_pricing_recommendations(
                keepa_client, niche, content_type
            )
        # Create KDP listing object
        kdp_listing = KDPListing(
            title=best_title,
            description=description_data["description"],
            keywords=keyword_recommendations["primary_keywords"],
            categories=category_recommendations["primary_categories"],
            content_type=content_type,
            target_audience=target_audience,
            pricing_tier="medium",  # Default
            unique_selling_points=[unique_angle] + unique_features[:3]
        )
        # Set pricing if available
        if pricing_data:
            kdp_listing.suggested_price = pricing_data["recommended_price"]
            kdp_listing.pricing_tier = pricing_data["pricing_tier"]
        # Compile final result
        result = {
            "listing": kdp_listing.to_dict(),
            "title_options": title_options,
            "description_analysis": {
                "word_count": description_data["word_count"],
                "seo_score": description_data["seo_score"],
                "readability_score": description_data["readability_score"],
                "compliance_check": description_data["compliance_check"]
            },
            "keyword_analysis": keyword_recommendations,
            "category_recommendations": category_recommendations,
            "pricing_analysis": pricing_data,
            "optimization_suggestions": _generate_optimization_suggestions(
                kdp_listing, niche, description_data
            ),
            "generation_metadata": {
                "niche_keyword": niche.primary_keyword,
                "content_type": content_type.value,
                "target_audience": target_audience.value,
                "style": style.value,
                "generation_timestamp": datetime.now().isoformat()
            }
        }
        logger.info(f"KDP listing generation completed for: {niche.primary_keyword}")
        return result
    except Exception as e:
        logger.error(f"KDP listing generation failed: {e}")
        return {
            "error": str(e),
            "niche_keyword": niche.primary_keyword if niche else "unknown",
            "generation_timestamp": datetime.now().isoformat()
        }
async def _generate_keyword_recommendations(
    trends_client: Optional[TrendsClient],
    niche: Niche,
    content_type: ContentType,
    target_audience: TargetAudience
) -> Dict[str, Any]:
    """Generate keyword recommendations for the listing."""
    primary_keywords = [niche.primary_keyword] + niche.keywords[:6]
    # Add content-type specific keywords
    content_keywords = {
        ContentType.JOURNAL: ["journal", "diary", "notebook", "reflection", "mindfulness"],
        ContentType.PLANNER: ["planner", "organizer", "schedule", "productivity", "goals"],
        ContentType.WORKBOOK: ["workbook", "exercises", "practice", "learning", "skills"]
    }
    type_keywords = content_keywords.get(content_type, [])
    # Add audience-specific keywords
    audience_keywords = {
        TargetAudience.PROFESSIONALS: ["professional", "business", "career", "workplace"],
        TargetAudience.STUDENTS: ["student", "study", "academic", "learning"],
        TargetAudience.CHILDREN: ["kids", "children", "fun", "colorful"]
    }
    audience_kws = audience_keywords.get(target_audience, [])
    # Combine and deduplicate
    all_keywords = list(set(primary_keywords + type_keywords + audience_kws))
    # Validate keywords with trends if available
    validated_keywords = all_keywords[:7]  # KDP limit
    trend_scores = {}
    if trends_client:
        try:
            for keyword in validated_keywords[:5]:  # Limit API calls
                trend_analysis = await trends_client.get_trend_analysis(keyword)
                if trend_analysis:
                    trend_scores[keyword] = trend_analysis.trend_score
        except Exception as e:
            logger.warning(f"Trend validation failed: {e}")
    return {
        "primary_keywords": validated_keywords,
        "trend_scores": trend_scores,
        "keyword_suggestions": {
            "content_type_keywords": type_keywords,
            "audience_keywords": audience_kws,
            "niche_keywords": niche.keywords[:10]
        }
    }
def _generate_category_recommendations(
    content_type: ContentType,
    target_audience: TargetAudience,
    niche_category: str
) -> Dict[str, Any]:
    """Generate Amazon category recommendations."""
    # Base categories by content type
    base_categories = {
        ContentType.JOURNAL: [
            "Books > Self-Help > Journal Writing",
            "Books > Health, Fitness & Dieting > Mental Health",
            "Office Products > Office & School Supplies > Calendars, Planners & Personal Organizers"
        ],
        ContentType.PLANNER: [
            "Office Products > Office & School Supplies > Calendars, Planners & Personal Organizers",
            "Books > Business & Money > Management & Leadership",
            "Books > Self-Help > Time Management"
        ],
        ContentType.WORKBOOK: [
            "Books > Education & Teaching > Schools & Teaching",
            "Books > Children's Books > Education & Reference",
            "Books > Test Preparation"
        ]
    }
    primary_categories = base_categories.get(content_type, [
        "Books > Self-Help",
        "Office Products > Office & School Supplies"
    ])
    # Audience-specific category adjustments
    if target_audience == TargetAudience.CHILDREN:
        primary_categories = [cat for cat in primary_categories if "Children" in cat or "Education" in cat]
        if not primary_categories:
            primary_categories = ["Books > Children's Books"]
    elif target_audience == TargetAudience.PROFESSIONALS:
        business_categories = [cat for cat in primary_categories if "Business" in cat or "Management" in cat]
        if business_categories:
            primary_categories = business_categories + primary_categories[:1]
    return {
        "primary_categories": primary_categories[:2],  # Amazon allows 2 categories
        "alternative_categories": primary_categories[2:5],
        "category_rationale": f"Selected based on {content_type.value} type and {target_audience.value} audience"
    }
async def _generate_pricing_recommendations(
    keepa_client: KeepaClient,
    niche: Niche,
    content_type: ContentType
) -> Dict[str, Any]:
    """Generate pricing recommendations based on market analysis."""
    try:
        # Search for similar products
        search_query = f"{niche.primary_keyword} {content_type.value.replace('_', ' ')}"
        similar_products = await keepa_client.search_products(search_query, limit=20)
        if not similar_products:
            # Fallback pricing
            return {
                "recommended_price": 9.99,
                "pricing_tier": "medium",
                "price_range": {"min": 5.99, "max": 14.99},
                "confidence": "low",
                "note": "No market data available - using default pricing"
            }
        # Analyze pricing
        prices = [p.current_price for p in similar_products if p.current_price and p.current_price > 0]
        if not prices:
            return {
                "recommended_price": 9.99,
                "pricing_tier": "medium",
                "price_range": {"min": 5.99, "max": 14.99},
                "confidence": "low",
                "note": "No valid pricing data found"
            }
        # Calculate pricing metrics
        avg_price = sum(prices) / len(prices)
        min_price = min(prices)
        max_price = max(prices)
        median_price = sorted(prices)[len(prices) // 2]
        # Determine recommended price (slightly below median for competitive advantage)
        recommended_price = max(5.99, median_price * 0.95)
        # Determine pricing tier
        if recommended_price <= 7.99:
            pricing_tier = "budget"
        elif recommended_price <= 12.99:
            pricing_tier = "medium"
        else:
            pricing_tier = "premium"
        return {
            "recommended_price": round(recommended_price, 2),
            "pricing_tier": pricing_tier,
            "price_range": {"min": round(min_price, 2), "max": round(max_price, 2)},
            "market_average": round(avg_price, 2),
            "market_median": round(median_price, 2),
            "analyzed_products": len(prices),
            "confidence": "high" if len(prices) >= 10 else "medium",
            "pricing_strategy": "Competitive positioning below median price"
        }
    except Exception as e:
        logger.warning(f"Pricing analysis failed: {e}")
        return {
            "recommended_price": 9.99,
            "pricing_tier": "medium",
            "error": str(e)
        }
def _generate_optimization_suggestions(
    listing: KDPListing,
    niche: Niche,
    description_data: Dict[str, Any]
) -> List[str]:
    """Generate optimization suggestions for the listing."""
    suggestions = []
    # Title optimization
    if len(listing.title) < 30:
        suggestions.append("Consider expanding the title to include more relevant keywords")
    elif len(listing.title) > 100:
        suggestions.append("Consider shortening the title for better readability")
    # Description optimization
    if description_data["seo_score"] < 70:
        suggestions.append("Improve SEO by incorporating more relevant keywords naturally")
    if description_data["readability_score"] < 70:
        suggestions.append("Improve readability by using shorter sentences and simpler words")
    # Keyword optimization
    if len(listing.keywords) < 7:
        suggestions.append("Add more relevant keywords to maximize discoverability")
    # Pricing optimization
    if hasattr(listing, 'suggested_price') and listing.suggested_price:
        if listing.suggested_price < 5.99:
            suggestions.append("Consider higher pricing to improve perceived value")
        elif listing.suggested_price > 19.99:
            suggestions.append("Consider lower pricing to improve accessibility")
    # Niche-specific suggestions
    if niche.competition_level and niche.competition_level.value == "high":
        suggestions.append("Focus on unique differentiation due to high competition")
    if niche.profitability_score < 70:
        suggestions.append("Consider targeting a more profitable niche or improving positioning")
    return suggestions
</file>

<file path="src/kdp_strategist/agent/tools/niche_discovery.py">
"""Niche Discovery Tool.
Finds profitable publishing niches by analyzing:
- Keyword variations and search volume
- Google Trends data for market interest
- Amazon competition analysis
- Market opportunity scoring
- Content gap identification
The tool combines multiple data sources to identify niches with:
- High market demand (Google Trends)
- Manageable competition (Amazon/Keepa data)
- Sustainable growth potential
- Clear monetization opportunities
"""
import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import itertools
import re
from ...data.cache_manager import CacheManager
from ...data.keepa_client import KeepaClient
from ...data.trends_client import TrendsClient
from ...models.niche_model import Niche
from ...models.trend_model import TrendAnalysis, TrendDirection, TrendStrength
logger = logging.getLogger(__name__)
class NicheScorer:
    """Scoring engine for niche profitability analysis."""
    # Scoring weights
    WEIGHTS = {
        "trend_score": 0.25,
        "competition_score": 0.30,
        "market_size_score": 0.20,
        "seasonality_score": 0.15,
        "content_gap_score": 0.10
    }
    @classmethod
    def calculate_profitability_score(cls, niche_data: Dict[str, Any]) -> float:
        """Calculate overall profitability score (0-100)."""
        scores = {
            "trend_score": cls._score_trend_strength(niche_data.get("trend_analysis")),
            "competition_score": cls._score_competition_level(niche_data.get("competition_data")),
            "market_size_score": cls._score_market_size(niche_data.get("market_metrics")),
            "seasonality_score": cls._score_seasonality(niche_data.get("seasonal_patterns")),
            "content_gap_score": cls._score_content_gaps(niche_data.get("content_analysis"))
        }
        # Calculate weighted score
        total_score = sum(score * cls.WEIGHTS[key] for key, score in scores.items() if score is not None)
        weight_sum = sum(cls.WEIGHTS[key] for key, score in scores.items() if score is not None)
        return (total_score / weight_sum * 100) if weight_sum > 0 else 0
    @staticmethod
    def _score_trend_strength(trend_analysis: Optional[TrendAnalysis]) -> Optional[float]:
        """Score based on trend strength and direction."""
        if not trend_analysis:
            return None
        base_score = trend_analysis.trend_score
        # Adjust for trend direction
        if trend_analysis.direction == TrendDirection.RISING:
            base_score *= 1.2
        elif trend_analysis.direction == TrendDirection.DECLINING:
            base_score *= 0.7
        # Adjust for trend strength
        strength_multipliers = {
            TrendStrength.VERY_STRONG: 1.3,
            TrendStrength.STRONG: 1.1,
            TrendStrength.MODERATE: 1.0,
            TrendStrength.WEAK: 0.8,
            TrendStrength.VERY_WEAK: 0.5
        }
        multiplier = strength_multipliers.get(trend_analysis.strength, 1.0)
        return min(100, base_score * multiplier)
    @staticmethod
    def _score_competition_level(competition_data: Optional[Dict[str, Any]]) -> Optional[float]:
        """Score based on competition analysis."""
        if not competition_data:
            return None
        # Lower competition = higher score
        competitor_count = competition_data.get("competitor_count", 0)
        avg_reviews = competition_data.get("avg_review_count", 0)
        avg_rating = competition_data.get("avg_rating", 0)
        price_range = competition_data.get("price_range", {})
        # Base score inversely related to competition
        if competitor_count == 0:
            base_score = 100
        elif competitor_count < 10:
            base_score = 90
        elif competitor_count < 50:
            base_score = 70
        elif competitor_count < 100:
            base_score = 50
        else:
            base_score = 30
        # Adjust for review saturation
        if avg_reviews > 1000:
            base_score *= 0.6  # High review saturation
        elif avg_reviews > 100:
            base_score *= 0.8
        elif avg_reviews < 10:
            base_score *= 1.2  # Low review saturation = opportunity
        # Adjust for rating quality
        if avg_rating < 3.5:
            base_score *= 1.3  # Poor ratings = opportunity
        elif avg_rating > 4.5:
            base_score *= 0.9  # High ratings = strong competition
        return min(100, base_score)
    @staticmethod
    def _score_market_size(market_metrics: Optional[Dict[str, Any]]) -> Optional[float]:
        """Score based on market size indicators."""
        if not market_metrics:
            return None
        search_volume = market_metrics.get("estimated_search_volume", 0)
        related_keywords = market_metrics.get("related_keyword_count", 0)
        category_size = market_metrics.get("category_size_score", 50)
        # Score based on search volume
        if search_volume > 10000:
            volume_score = 90
        elif search_volume > 1000:
            volume_score = 70
        elif search_volume > 100:
            volume_score = 50
        else:
            volume_score = 30
        # Adjust for keyword diversity
        keyword_multiplier = min(1.5, 1 + (related_keywords / 100))
        # Combine scores
        final_score = (volume_score * 0.6 + category_size * 0.4) * keyword_multiplier
        return min(100, final_score)
    @staticmethod
    def _score_seasonality(seasonal_patterns: Optional[Dict[str, Any]]) -> Optional[float]:
        """Score based on seasonal stability."""
        if not seasonal_patterns:
            return 75  # Neutral score if no data
        seasonality_strength = seasonal_patterns.get("seasonality_strength", 0)
        peak_months = seasonal_patterns.get("peak_months", [])
        consistency = seasonal_patterns.get("consistency_score", 50)
        # Lower seasonality = higher score (more stable)
        if seasonality_strength < 10:
            base_score = 95  # Very stable
        elif seasonality_strength < 25:
            base_score = 80  # Moderately stable
        elif seasonality_strength < 50:
            base_score = 60  # Some seasonality
        else:
            base_score = 40  # Highly seasonal
        # Adjust for consistency
        consistency_multiplier = consistency / 100
        return base_score * consistency_multiplier
    @staticmethod
    def _score_content_gaps(content_analysis: Optional[Dict[str, Any]]) -> Optional[float]:
        """Score based on content gap opportunities."""
        if not content_analysis:
            return None
        gap_count = content_analysis.get("identified_gaps", 0)
        content_quality = content_analysis.get("avg_content_quality", 50)
        differentiation_opportunities = content_analysis.get("differentiation_score", 50)
        # More gaps = higher opportunity
        if gap_count > 10:
            gap_score = 90
        elif gap_count > 5:
            gap_score = 70
        elif gap_count > 2:
            gap_score = 50
        else:
            gap_score = 30
        # Lower content quality = higher opportunity
        quality_multiplier = (100 - content_quality) / 100
        # Combine scores
        final_score = (gap_score * 0.6 + differentiation_opportunities * 0.4) * (1 + quality_multiplier)
        return min(100, final_score)
class KeywordExpander:
    """Expands base keywords into niche-specific variations."""
    # Common keyword modifiers for publishing niches
    MODIFIERS = {
        "journal": ["journal", "notebook", "diary", "planner", "log", "tracker", "organizer"],
        "audience": ["kids", "children", "teens", "adults", "seniors", "women", "men", "professionals"],
        "purpose": ["daily", "weekly", "monthly", "travel", "work", "personal", "business", "creative"],
        "style": ["lined", "dotted", "blank", "guided", "prompted", "illustrated", "minimalist"],
        "theme": ["gratitude", "mindfulness", "fitness", "productivity", "self-care", "goals"]
    }
    @classmethod
    def expand_keywords(cls, base_keywords: List[str], max_combinations: int = 100) -> List[str]:
        """Expand base keywords into variations."""
        expanded = set(base_keywords)
        for base_keyword in base_keywords:
            # Add single modifier combinations
            for category, modifiers in cls.MODIFIERS.items():
                for modifier in modifiers:
                    # Prefix combinations
                    expanded.add(f"{modifier} {base_keyword}")
                    # Suffix combinations
                    expanded.add(f"{base_keyword} {modifier}")
            # Add two-modifier combinations (limited)
            modifier_pairs = list(itertools.combinations(cls.MODIFIERS.keys(), 2))
            for cat1, cat2 in modifier_pairs[:5]:  # Limit combinations
                for mod1 in cls.MODIFIERS[cat1][:3]:  # Top 3 from each category
                    for mod2 in cls.MODIFIERS[cat2][:3]:
                        expanded.add(f"{mod1} {base_keyword} {mod2}")
                        if len(expanded) >= max_combinations:
                            break
                    if len(expanded) >= max_combinations:
                        break
                if len(expanded) >= max_combinations:
                    break
        # Clean and filter keywords
        cleaned = []
        for keyword in expanded:
            # Basic cleaning
            keyword = re.sub(r'\s+', ' ', keyword.strip().lower())
            # Filter out very long keywords
            if len(keyword) <= 100 and len(keyword.split()) <= 6:
                cleaned.append(keyword)
        return cleaned[:max_combinations]
async def find_profitable_niches(
    trends_client: TrendsClient,
    keepa_client: Optional[KeepaClient],
    cache_manager: CacheManager,
    base_keywords: List[str],
    categories: Optional[List[str]] = None,
    min_profitability_score: float = 60,
    max_competition_level: str = "medium",
    limit: int = 10
) -> Dict[str, Any]:
    """Find profitable publishing niches.
    Args:
        trends_client: Google Trends client
        keepa_client: Keepa API client (optional)
        cache_manager: Cache manager for performance
        base_keywords: Starting keywords for niche discovery
        categories: Amazon categories to focus on
        min_profitability_score: Minimum score threshold (0-100)
        max_competition_level: Maximum competition level (low/medium/high)
        limit: Maximum number of niches to return
    Returns:
        Dictionary containing discovered niches and analysis metadata
    """
    logger.info(f"Starting niche discovery for keywords: {base_keywords}")
    try:
        # Step 1: Expand keywords
        expanded_keywords = KeywordExpander.expand_keywords(base_keywords, max_combinations=200)
        logger.info(f"Expanded to {len(expanded_keywords)} keyword variations")
        # Step 2: Analyze trends for expanded keywords (batch processing)
        trend_analyses = await _batch_analyze_trends(trends_client, expanded_keywords[:50])  # Limit for performance
        # Step 3: Filter keywords with good trend potential
        promising_keywords = _filter_promising_trends(trend_analyses, min_trend_score=30)
        logger.info(f"Found {len(promising_keywords)} keywords with good trend potential")
        # Step 4: Analyze competition for promising keywords
        competition_data = await _analyze_competition(keepa_client, promising_keywords, categories)
        # Step 5: Generate niche candidates
        niche_candidates = await _generate_niche_candidates(
            promising_keywords, trend_analyses, competition_data, categories
        )
        # Step 6: Score and rank niches
        scored_niches = _score_and_rank_niches(niche_candidates, min_profitability_score, max_competition_level)
        # Step 7: Return top niches
        top_niches = scored_niches[:limit]
        result = {
            "niches": [niche.to_dict() for niche in top_niches],
            "analysis_metadata": {
                "base_keywords": base_keywords,
                "expanded_keywords_count": len(expanded_keywords),
                "analyzed_trends_count": len(trend_analyses),
                "promising_keywords_count": len(promising_keywords),
                "niche_candidates_count": len(niche_candidates),
                "final_niches_count": len(top_niches),
                "min_profitability_score": min_profitability_score,
                "max_competition_level": max_competition_level,
                "analysis_timestamp": datetime.now().isoformat()
            },
            "recommendations": _generate_recommendations(top_niches, scored_niches)
        }
        logger.info(f"Niche discovery completed. Found {len(top_niches)} profitable niches")
        return result
    except Exception as e:
        logger.error(f"Niche discovery failed: {e}")
        raise
async def _batch_analyze_trends(trends_client: TrendsClient, keywords: List[str]) -> Dict[str, TrendAnalysis]:
    """Analyze trends for multiple keywords efficiently."""
    trend_analyses = {}
    # Process in smaller batches to respect rate limits
    batch_size = 5
    for i in range(0, len(keywords), batch_size):
        batch = keywords[i:i + batch_size]
        # Process batch concurrently
        tasks = []
        for keyword in batch:
            task = trends_client.get_trend_analysis(keyword, timeframe="today 12-m")
            tasks.append(task)
        # Wait for batch completion
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        # Process results
        for keyword, result in zip(batch, batch_results):
            if isinstance(result, TrendAnalysis):
                trend_analyses[keyword] = result
            elif isinstance(result, Exception):
                logger.warning(f"Failed to analyze trend for '{keyword}': {result}")
        # Rate limiting delay between batches
        if i + batch_size < len(keywords):
            await asyncio.sleep(2)
    return trend_analyses
def _filter_promising_trends(trend_analyses: Dict[str, TrendAnalysis], min_trend_score: float = 30) -> List[str]:
    """Filter keywords with promising trend characteristics."""
    promising = []
    for keyword, analysis in trend_analyses.items():
        # Basic trend score filter
        if analysis.trend_score < min_trend_score:
            continue
        # Avoid declining trends
        if analysis.direction == TrendDirection.DECLINING and analysis.trend_score < 50:
            continue
        # Require minimum confidence
        if analysis.confidence_level < 0.3:
            continue
        promising.append(keyword)
    return promising
async def _analyze_competition(keepa_client: Optional[KeepaClient], keywords: List[str], 
                              categories: Optional[List[str]]) -> Dict[str, Dict[str, Any]]:
    """Analyze competition for keywords using Amazon/Keepa data."""
    competition_data = {}
    if not keepa_client:
        logger.warning("No Keepa client available - using simulated competition data")
        # Return simulated data for development
        for keyword in keywords:
            competition_data[keyword] = {
                "competitor_count": len(keyword.split()) * 20,  # Rough estimate
                "avg_review_count": 50,
                "avg_rating": 4.0,
                "price_range": {"min": 5.99, "max": 19.99, "avg": 12.99},
                "estimated": True
            }
        return competition_data
    # Analyze competition using Keepa search
    for keyword in keywords[:20]:  # Limit for performance
        try:
            # Search for products
            products = keepa_client.search_products(keyword, limit=20)
            if products:
                # Analyze competition metrics
                review_counts = [p.review_count for p in products if p.review_count]
                ratings = [p.rating for p in products if p.rating]
                prices = [p.current_price for p in products if p.current_price]
                competition_data[keyword] = {
                    "competitor_count": len(products),
                    "avg_review_count": sum(review_counts) / len(review_counts) if review_counts else 0,
                    "avg_rating": sum(ratings) / len(ratings) if ratings else 0,
                    "price_range": {
                        "min": min(prices) if prices else 0,
                        "max": max(prices) if prices else 0,
                        "avg": sum(prices) / len(prices) if prices else 0
                    },
                    "estimated": False
                }
            else:
                # No competition found
                competition_data[keyword] = {
                    "competitor_count": 0,
                    "avg_review_count": 0,
                    "avg_rating": 0,
                    "price_range": {"min": 0, "max": 0, "avg": 0},
                    "estimated": False
                }
        except Exception as e:
            logger.warning(f"Failed to analyze competition for '{keyword}': {e}")
            continue
    return competition_data
async def _generate_niche_candidates(keywords: List[str], trend_analyses: Dict[str, TrendAnalysis],
                                    competition_data: Dict[str, Dict[str, Any]], 
                                    categories: Optional[List[str]]) -> List[Niche]:
    """Generate niche candidates from analyzed data."""
    niche_candidates = []
    for keyword in keywords:
        trend_analysis = trend_analyses.get(keyword)
        competition = competition_data.get(keyword, {})
        if not trend_analysis:
            continue
        # Create niche object
        niche = Niche(
            category=categories[0] if categories else "Books & Journals",
            primary_keyword=keyword,
            keywords=[keyword] + trend_analysis.related_queries[:10],
            trend_analysis=trend_analysis,
            competitor_data={
                "count": competition.get("competitor_count", 0),
                "avg_reviews": competition.get("avg_review_count", 0),
                "avg_rating": competition.get("avg_rating", 0),
                "price_analysis": competition.get("price_range", {})
            },
            market_size_score=_estimate_market_size(trend_analysis, competition),
            seasonal_factors=trend_analysis.seasonal_patterns,
            last_updated=datetime.now()
        )
        niche_candidates.append(niche)
    return niche_candidates
def _estimate_market_size(trend_analysis: TrendAnalysis, competition_data: Dict[str, Any]) -> float:
    """Estimate market size score based on trend and competition data."""
    base_score = trend_analysis.trend_score
    # Adjust for competition level
    competitor_count = competition_data.get("competitor_count", 0)
    if competitor_count == 0:
        competition_multiplier = 1.5  # No competition = larger opportunity
    elif competitor_count < 10:
        competition_multiplier = 1.2
    elif competitor_count < 50:
        competition_multiplier = 1.0
    else:
        competition_multiplier = 0.8
    # Adjust for trend strength
    if trend_analysis.strength in [TrendStrength.STRONG, TrendStrength.VERY_STRONG]:
        strength_multiplier = 1.3
    elif trend_analysis.strength == TrendStrength.MODERATE:
        strength_multiplier = 1.0
    else:
        strength_multiplier = 0.7
    return min(100, base_score * competition_multiplier * strength_multiplier)
def _score_and_rank_niches(niche_candidates: List[Niche], min_score: float, 
                           max_competition: str) -> List[Niche]:
    """Score and rank niche candidates."""
    scored_niches = []
    competition_limits = {
        "low": 20,
        "medium": 50,
        "high": 100
    }
    max_competitors = competition_limits.get(max_competition, 50)
    for niche in niche_candidates:
        # Check competition filter
        competitor_count = niche.competitor_data.get("count", 0)
        if competitor_count > max_competitors:
            continue
        # Calculate profitability score
        niche_data = {
            "trend_analysis": niche.trend_analysis,
            "competition_data": niche.competitor_data,
            "market_metrics": {"estimated_search_volume": niche.market_size_score * 100},
            "seasonal_patterns": niche.seasonal_factors,
            "content_analysis": {"identified_gaps": 5}  # Placeholder
        }
        profitability_score = NicheScorer.calculate_profitability_score(niche_data)
        niche.profitability_score = profitability_score
        # Set competition level
        if competitor_count <= 10:
            niche.competition_score = CompetitionLevel.LOW
        elif competitor_count <= 50:
            niche.competition_score = CompetitionLevel.MEDIUM
        else:
            niche.competition_score = CompetitionLevel.HIGH
        # Set profitability tier
        if profitability_score >= 80:
            niche.profitability_tier = ProfitabilityTier.HIGH
        elif profitability_score >= 60:
            niche.profitability_tier = ProfitabilityTier.MEDIUM
        else:
            niche.profitability_tier = ProfitabilityTier.LOW
        # Apply minimum score filter
        if profitability_score >= min_score:
            scored_niches.append(niche)
    # Sort by profitability score (descending)
    scored_niches.sort(key=lambda n: n.profitability_score, reverse=True)
    return scored_niches
def _generate_recommendations(top_niches: List[Niche], all_scored_niches: List[Niche]) -> Dict[str, Any]:
    """Generate actionable recommendations based on niche analysis."""
    if not top_niches:
        return {"message": "No profitable niches found with current criteria"}
    best_niche = top_niches[0]
    recommendations = {
        "primary_recommendation": {
            "niche": best_niche.primary_keyword,
            "score": best_niche.profitability_score,
            "reason": f"Highest profitability score with {best_niche.competition_level.value} competition"
        },
        "quick_wins": [],
        "long_term_opportunities": [],
        "market_insights": {
            "avg_profitability_score": sum(n.profitability_score for n in all_scored_niches) / len(all_scored_niches),
            "competition_distribution": {
                "low": len([n for n in all_scored_niches if n.competition_level == CompetitionLevel.LOW]),
                "medium": len([n for n in all_scored_niches if n.competition_level == CompetitionLevel.MEDIUM]),
                "high": len([n for n in all_scored_niches if n.competition_level == CompetitionLevel.HIGH])
            }
        }
    }
    # Identify quick wins (low competition, decent score)
    for niche in top_niches[:5]:
        if niche.competition_level == CompetitionLevel.LOW and niche.profitability_score >= 60:
            recommendations["quick_wins"].append({
                "niche": niche.primary_keyword,
                "score": niche.profitability_score,
                "competitors": niche.competitor_data.get("count", 0)
            })
    # Identify long-term opportunities (high potential, manageable competition)
    for niche in top_niches[:10]:
        if (niche.profitability_score >= 75 and 
            niche.trend_analysis and 
            niche.trend_analysis.direction == TrendDirection.RISING):
            recommendations["long_term_opportunities"].append({
                "niche": niche.primary_keyword,
                "score": niche.profitability_score,
                "trend_direction": niche.trend_analysis.direction.value
            })
    return recommendations
</file>

<file path="src/kdp_strategist/agent/tools/stress_testing.py">
"""Stress Testing Tool.
Performs comprehensive stress testing of niches to assess:
- Market resilience under various scenarios
- Competition pressure tolerance
- Economic downturn impact
- Seasonal volatility effects
- Platform algorithm changes
- Consumer behavior shifts
The tool simulates multiple stress scenarios including:
- Market saturation scenarios
- Economic recession impacts
- Competitive flooding
- Seasonal demand crashes
- Platform policy changes
- Consumer trend shifts
"""
import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
from statistics import mean, stdev
import random
import math
from ...data.cache_manager import CacheManager
from ...data.trends_client import TrendsClient
from ...data.keepa_client import KeepaClient
from ...models.niche_model import Niche
from ...models.trend_model import TrendAnalysis
from .niche_discovery import NicheScorer
from .trend_validation import TrendValidator
logger = logging.getLogger(__name__)
class StressScenario(Enum):
    """Types of stress test scenarios."""
    MARKET_SATURATION = "market_saturation"
    ECONOMIC_DOWNTURN = "economic_downturn"
    COMPETITIVE_FLOODING = "competitive_flooding"
    SEASONAL_CRASH = "seasonal_crash"
    PLATFORM_CHANGES = "platform_changes"
    TREND_REVERSAL = "trend_reversal"
    CONSUMER_SHIFT = "consumer_shift"
    SUPPLY_CHAIN_DISRUPTION = "supply_chain_disruption"
@dataclass
class StressTestParameters:
    """Parameters for stress test scenarios."""
    scenario: StressScenario
    severity: float  # 0.1 (mild) to 1.0 (severe)
    duration_months: int  # How long the stress lasts
    recovery_months: int  # How long to recover
    probability: float  # Likelihood of scenario occurring
    description: str
@dataclass
class ScenarioResult:
    """Result of a single stress test scenario."""
    scenario: StressScenario
    parameters: StressTestParameters
    initial_score: float
    stressed_score: float
    recovery_score: float
    impact_percentage: float
    resilience_score: float
    survival_probability: float
    key_vulnerabilities: List[str]
    mitigation_strategies: List[str]
@dataclass
class StressTestReport:
    """Comprehensive stress test report."""
    niche_keyword: str
    baseline_niche: Niche
    scenario_results: List[ScenarioResult]
    overall_resilience: float
    risk_profile: str
    critical_vulnerabilities: List[str]
    recommended_mitigations: List[str]
    confidence_level: float
    test_timestamp: datetime
class StressTester:
    """Core stress testing engine."""
    # Stress test scenarios with default parameters
    DEFAULT_SCENARIOS = {
        StressScenario.MARKET_SATURATION: StressTestParameters(
            scenario=StressScenario.MARKET_SATURATION,
            severity=0.7,
            duration_months=6,
            recovery_months=12,
            probability=0.3,
            description="Market becomes oversaturated with competitors"
        ),
        StressScenario.ECONOMIC_DOWNTURN: StressTestParameters(
            scenario=StressScenario.ECONOMIC_DOWNTURN,
            severity=0.6,
            duration_months=8,
            recovery_months=18,
            probability=0.2,
            description="Economic recession reduces consumer spending"
        ),
        StressScenario.COMPETITIVE_FLOODING: StressTestParameters(
            scenario=StressScenario.COMPETITIVE_FLOODING,
            severity=0.8,
            duration_months=4,
            recovery_months=8,
            probability=0.4,
            description="Sudden influx of new competitors"
        ),
        StressScenario.SEASONAL_CRASH: StressTestParameters(
            scenario=StressScenario.SEASONAL_CRASH,
            severity=0.9,
            duration_months=3,
            recovery_months=6,
            probability=0.5,
            description="Severe seasonal demand drop"
        ),
        StressScenario.PLATFORM_CHANGES: StressTestParameters(
            scenario=StressScenario.PLATFORM_CHANGES,
            severity=0.5,
            duration_months=3,
            recovery_months=9,
            probability=0.3,
            description="Amazon algorithm or policy changes"
        ),
        StressScenario.TREND_REVERSAL: StressTestParameters(
            scenario=StressScenario.TREND_REVERSAL,
            severity=0.8,
            duration_months=12,
            recovery_months=24,
            probability=0.25,
            description="Major trend reversal or consumer preference shift"
        ),
        StressScenario.CONSUMER_SHIFT: StressTestParameters(
            scenario=StressScenario.CONSUMER_SHIFT,
            severity=0.6,
            duration_months=9,
            recovery_months=15,
            probability=0.35,
            description="Consumer behavior and preferences change"
        ),
        StressScenario.SUPPLY_CHAIN_DISRUPTION: StressTestParameters(
            scenario=StressScenario.SUPPLY_CHAIN_DISRUPTION,
            severity=0.4,
            duration_months=2,
            recovery_months=4,
            probability=0.15,
            description="Supply chain or production disruptions"
        )
    }
    @classmethod
    def simulate_scenario(
        cls,
        niche: Niche,
        scenario_params: StressTestParameters,
        trend_analysis: Optional[TrendAnalysis] = None
    ) -> ScenarioResult:
        """Simulate a single stress test scenario."""
        initial_score = niche.overall_score
        # Calculate stress impact based on scenario type and niche characteristics
        stress_impact = cls._calculate_stress_impact(
            niche, scenario_params, trend_analysis
        )
        # Apply stress to niche score
        stressed_score = initial_score * (1 - stress_impact)
        stressed_score = max(0, stressed_score)
        # Calculate recovery score
        recovery_factor = cls._calculate_recovery_factor(
            niche, scenario_params, stress_impact
        )
        recovery_score = stressed_score + (initial_score - stressed_score) * recovery_factor
        recovery_score = min(initial_score, recovery_score)
        # Calculate impact percentage
        impact_percentage = (initial_score - stressed_score) / initial_score * 100
        # Calculate resilience score (how well it maintains performance)
        resilience_score = (stressed_score / initial_score) * 100
        # Calculate survival probability
        survival_probability = cls._calculate_survival_probability(
            stressed_score, recovery_score, scenario_params
        )
        # Identify vulnerabilities
        vulnerabilities = cls._identify_vulnerabilities(
            niche, scenario_params, stress_impact
        )
        # Generate mitigation strategies
        mitigations = cls._generate_mitigation_strategies(
            niche, scenario_params, vulnerabilities
        )
        return ScenarioResult(
            scenario=scenario_params.scenario,
            parameters=scenario_params,
            initial_score=round(initial_score, 1),
            stressed_score=round(stressed_score, 1),
            recovery_score=round(recovery_score, 1),
            impact_percentage=round(impact_percentage, 1),
            resilience_score=round(resilience_score, 1),
            survival_probability=round(survival_probability, 2),
            key_vulnerabilities=vulnerabilities,
            mitigation_strategies=mitigations
        )
    @classmethod
    def _calculate_stress_impact(
        cls,
        niche: Niche,
        scenario_params: StressTestParameters,
        trend_analysis: Optional[TrendAnalysis] = None
    ) -> float:
        """Calculate the impact of stress on the niche."""
        base_impact = scenario_params.severity * 0.5  # Base 50% max impact
        # Scenario-specific impact calculations
        if scenario_params.scenario == StressScenario.MARKET_SATURATION:
            # Higher competition score = more vulnerable to saturation
            competition_factor = niche.competition_score / 100
            base_impact *= (1 + competition_factor)
        elif scenario_params.scenario == StressScenario.ECONOMIC_DOWNTURN:
            # Higher price points more vulnerable to economic stress
            if niche.price_range:
                avg_price = (niche.price_range[0] + niche.price_range[1]) / 2
                price_factor = min(1.0, avg_price / 50)  # Normalize around $50
                base_impact *= (1 + price_factor * 0.5)
        elif scenario_params.scenario == StressScenario.COMPETITIVE_FLOODING:
            # Lower competition score = more vulnerable to new competitors
            competition_vulnerability = (100 - niche.competition_score) / 100
            base_impact *= (1 + competition_vulnerability * 0.8)
        elif scenario_params.scenario == StressScenario.SEASONAL_CRASH:
            # Higher seasonal factors = more vulnerable
            seasonal_factor = 0.5  # Default moderate seasonality
            if niche.seasonal_factors:
                seasonal_factor = niche.seasonal_factors.get("volatility", 0.5)
            base_impact *= (1 + seasonal_factor)
        elif scenario_params.scenario == StressScenario.TREND_REVERSAL:
            # Trend-dependent niches more vulnerable
            if trend_analysis:
                trend_dependency = trend_analysis.trend_score / 100
                base_impact *= (1 + trend_dependency * 0.7)
        elif scenario_params.scenario == StressScenario.CONSUMER_SHIFT:
            # Niche-specific consumer dependency
            market_size_factor = (100 - niche.market_size_score) / 100
            base_impact *= (1 + market_size_factor * 0.6)
        # Apply duration factor (longer stress = more impact)
        duration_factor = min(1.5, scenario_params.duration_months / 12)
        base_impact *= duration_factor
        return min(0.95, base_impact)  # Cap at 95% impact
    @classmethod
    def _calculate_recovery_factor(
        cls,
        niche: Niche,
        scenario_params: StressTestParameters,
        stress_impact: float
    ) -> float:
        """Calculate how well the niche recovers from stress."""
        base_recovery = 0.7  # Base 70% recovery
        # Higher profitability = better recovery
        profitability_factor = niche.profitability_score / 100
        base_recovery += profitability_factor * 0.2
        # Lower competition = easier recovery
        competition_factor = (100 - niche.competition_score) / 100
        base_recovery += competition_factor * 0.15
        # Market size helps recovery
        market_factor = niche.market_size_score / 100
        base_recovery += market_factor * 0.1
        # Recovery time factor (longer recovery time = better eventual recovery)
        time_factor = min(1.2, scenario_params.recovery_months / 12)
        base_recovery *= time_factor
        # Severe stress reduces recovery potential
        stress_factor = 1 - (stress_impact * 0.3)
        base_recovery *= stress_factor
        return min(1.0, max(0.1, base_recovery))
    @classmethod
    def _calculate_survival_probability(
        cls,
        stressed_score: float,
        recovery_score: float,
        scenario_params: StressTestParameters
    ) -> float:
        """Calculate probability of surviving the stress scenario."""
        # Base survival on stressed score
        if stressed_score >= 60:
            base_survival = 0.95
        elif stressed_score >= 40:
            base_survival = 0.8
        elif stressed_score >= 20:
            base_survival = 0.6
        elif stressed_score >= 10:
            base_survival = 0.3
        else:
            base_survival = 0.1
        # Factor in recovery potential
        recovery_factor = recovery_score / 100
        base_survival += recovery_factor * 0.2
        # Factor in scenario severity and duration
        severity_penalty = scenario_params.severity * 0.15
        duration_penalty = min(0.2, scenario_params.duration_months / 24)
        final_survival = base_survival - severity_penalty - duration_penalty
        return min(1.0, max(0.05, final_survival))
    @classmethod
    def _identify_vulnerabilities(
        cls,
        niche: Niche,
        scenario_params: StressTestParameters,
        stress_impact: float
    ) -> List[str]:
        """Identify key vulnerabilities exposed by the stress test."""
        vulnerabilities = []
        # High stress impact indicates vulnerability
        if stress_impact > 0.7:
            vulnerabilities.append(f"High vulnerability to {scenario_params.scenario.value}")
        # Scenario-specific vulnerabilities
        if scenario_params.scenario == StressScenario.MARKET_SATURATION:
            if niche.competition_score > 70:
                vulnerabilities.append("Already high competition makes saturation more likely")
            if niche.market_size_score < 40:
                vulnerabilities.append("Small market size limits growth potential")
        elif scenario_params.scenario == StressScenario.ECONOMIC_DOWNTURN:
            if niche.price_range and niche.price_range[1] > 30:
                vulnerabilities.append("Higher price point vulnerable to economic stress")
            if "luxury" in niche.category.lower() or "premium" in niche.category.lower():
                vulnerabilities.append("Luxury/premium positioning vulnerable in downturns")
        elif scenario_params.scenario == StressScenario.COMPETITIVE_FLOODING:
            if niche.competition_score < 50:
                vulnerabilities.append("Low competition barriers allow easy entry")
            if not niche.content_gaps:
                vulnerabilities.append("Limited content differentiation opportunities")
        elif scenario_params.scenario == StressScenario.SEASONAL_CRASH:
            if niche.seasonal_factors and niche.seasonal_factors.get("volatility", 0) > 0.6:
                vulnerabilities.append("High seasonal volatility increases crash risk")
        # General vulnerabilities
        if niche.confidence_score < 60:
            vulnerabilities.append("Low confidence in niche data increases uncertainty")
        if niche.profitability_score < 50:
            vulnerabilities.append("Low profitability reduces stress resilience")
        return vulnerabilities
    @classmethod
    def _generate_mitigation_strategies(
        cls,
        niche: Niche,
        scenario_params: StressTestParameters,
        vulnerabilities: List[str]
    ) -> List[str]:
        """Generate mitigation strategies for identified vulnerabilities."""
        strategies = []
        # Scenario-specific strategies
        if scenario_params.scenario == StressScenario.MARKET_SATURATION:
            strategies.extend([
                "Develop unique content angles to differentiate from competitors",
                "Focus on sub-niches with less competition",
                "Build strong brand recognition early"
            ])
        elif scenario_params.scenario == StressScenario.ECONOMIC_DOWNTURN:
            strategies.extend([
                "Consider lower-priced product options",
                "Emphasize value proposition and practical benefits",
                "Diversify into recession-resistant sub-topics"
            ])
        elif scenario_params.scenario == StressScenario.COMPETITIVE_FLOODING:
            strategies.extend([
                "Establish first-mover advantage quickly",
                "Create high-quality content that's hard to replicate",
                "Build customer loyalty through superior value"
            ])
        elif scenario_params.scenario == StressScenario.SEASONAL_CRASH:
            strategies.extend([
                "Develop year-round content strategy",
                "Create evergreen content to smooth seasonal variations",
                "Plan inventory and marketing around seasonal patterns"
            ])
        elif scenario_params.scenario == StressScenario.PLATFORM_CHANGES:
            strategies.extend([
                "Diversify across multiple platforms",
                "Stay updated on platform policy changes",
                "Build direct customer relationships"
            ])
        elif scenario_params.scenario == StressScenario.TREND_REVERSAL:
            strategies.extend([
                "Monitor trend indicators closely",
                "Prepare pivot strategies for related niches",
                "Build adaptable content frameworks"
            ])
        # General mitigation strategies
        if niche.profitability_score < 60:
            strategies.append("Focus on improving profit margins through premium positioning")
        if niche.market_size_score < 50:
            strategies.append("Expand target market through related keywords and topics")
        if niche.competition_score > 70:
            strategies.append("Identify and exploit competitor weaknesses")
        return strategies[:5]  # Limit to top 5 strategies
async def niche_stress_test(
    trends_client: TrendsClient,
    keepa_client: KeepaClient,
    cache_manager: CacheManager,
    niche_keyword: str,
    custom_scenarios: Optional[List[StressTestParameters]] = None,
    include_all_scenarios: bool = True
) -> Dict[str, Any]:
    """Perform comprehensive stress testing on a niche.
    Args:
        trends_client: Google Trends client
        keepa_client: Keepa API client
        cache_manager: Cache manager
        niche_keyword: Primary keyword for the niche
        custom_scenarios: Custom stress test scenarios
        include_all_scenarios: Whether to test all default scenarios
    Returns:
        Dictionary containing comprehensive stress test results
    """
    logger.info(f"Starting stress test for niche: {niche_keyword}")
    try:
        # First, we need to create/retrieve the niche data
        niche = await _create_niche_for_testing(
            trends_client, keepa_client, cache_manager, niche_keyword
        )
        if not niche:
            return {
                "error": "Unable to create niche data for stress testing",
                "niche_keyword": niche_keyword,
                "test_timestamp": datetime.now().isoformat()
            }
        # Get trend analysis for enhanced testing
        trend_analysis = None
        try:
            trend_analysis = await trends_client.get_trend_analysis(niche_keyword)
        except Exception as e:
            logger.warning(f"Could not get trend analysis: {e}")
        # Determine scenarios to test
        scenarios_to_test = []
        if include_all_scenarios:
            scenarios_to_test.extend(StressTester.DEFAULT_SCENARIOS.values())
        if custom_scenarios:
            scenarios_to_test.extend(custom_scenarios)
        if not scenarios_to_test:
            scenarios_to_test = list(StressTester.DEFAULT_SCENARIOS.values())
        # Run stress tests for each scenario
        scenario_results = []
        for scenario_params in scenarios_to_test:
            try:
                result = StressTester.simulate_scenario(
                    niche, scenario_params, trend_analysis
                )
                scenario_results.append(result)
                logger.debug(f"Completed stress test for scenario: {scenario_params.scenario.value}")
            except Exception as e:
                logger.error(f"Failed stress test for scenario {scenario_params.scenario.value}: {e}")
        # Calculate overall resilience
        overall_resilience = _calculate_overall_resilience(scenario_results)
        # Determine risk profile
        risk_profile = _determine_risk_profile(scenario_results, overall_resilience)
        # Identify critical vulnerabilities
        critical_vulnerabilities = _identify_critical_vulnerabilities(scenario_results)
        # Generate recommended mitigations
        recommended_mitigations = _generate_recommended_mitigations(
            scenario_results, critical_vulnerabilities
        )
        # Calculate confidence level
        confidence_level = _calculate_test_confidence(
            niche, trend_analysis, len(scenario_results)
        )
        # Create comprehensive report
        stress_test_report = StressTestReport(
            niche_keyword=niche_keyword,
            baseline_niche=niche,
            scenario_results=scenario_results,
            overall_resilience=overall_resilience,
            risk_profile=risk_profile,
            critical_vulnerabilities=critical_vulnerabilities,
            recommended_mitigations=recommended_mitigations,
            confidence_level=confidence_level,
            test_timestamp=datetime.now()
        )
        # Compile final result
        result = {
            "stress_test_summary": {
                "niche_keyword": niche_keyword,
                "overall_resilience": overall_resilience,
                "risk_profile": risk_profile,
                "scenarios_tested": len(scenario_results),
                "confidence_level": confidence_level
            },
            "baseline_niche": {
                "overall_score": niche.overall_score,
                "profitability_score": niche.profitability_score,
                "competition_score": niche.competition_score,
                "market_size_score": niche.market_size_score,
                "confidence_score": niche.confidence_score
            },
            "scenario_results": [
                {
                    "scenario": result.scenario.value,
                    "severity": result.parameters.severity,
                    "probability": result.parameters.probability,
                    "initial_score": result.initial_score,
                    "stressed_score": result.stressed_score,
                    "recovery_score": result.recovery_score,
                    "impact_percentage": result.impact_percentage,
                    "resilience_score": result.resilience_score,
                    "survival_probability": result.survival_probability,
                    "key_vulnerabilities": result.key_vulnerabilities,
                    "mitigation_strategies": result.mitigation_strategies,
                    "description": result.parameters.description
                }
                for result in scenario_results
            ],
            "risk_analysis": {
                "critical_vulnerabilities": critical_vulnerabilities,
                "highest_risk_scenarios": _get_highest_risk_scenarios(scenario_results),
                "lowest_resilience_scenarios": _get_lowest_resilience_scenarios(scenario_results),
                "survival_analysis": _analyze_survival_probabilities(scenario_results)
            },
            "recommendations": {
                "immediate_actions": recommended_mitigations[:3],
                "strategic_mitigations": recommended_mitigations[3:],
                "monitoring_priorities": _get_monitoring_priorities(scenario_results),
                "contingency_planning": _get_contingency_recommendations(scenario_results)
            },
            "test_metadata": {
                "test_timestamp": stress_test_report.test_timestamp.isoformat(),
                "confidence_level": confidence_level,
                "data_quality": {
                    "has_trend_data": trend_analysis is not None,
                    "niche_data_completeness": _assess_niche_data_completeness(niche)
                }
            }
        }
        logger.info(f"Stress test completed for niche: {niche_keyword}")
        return result
    except Exception as e:
        logger.error(f"Stress test failed for {niche_keyword}: {e}")
        return {
            "error": str(e),
            "niche_keyword": niche_keyword,
            "test_timestamp": datetime.now().isoformat()
        }
async def _create_niche_for_testing(
    trends_client: TrendsClient,
    keepa_client: KeepaClient,
    cache_manager: CacheManager,
    keyword: str
) -> Optional[Niche]:
    """Create a niche object for stress testing."""
    try:
        # Get trend analysis
        trend_analysis = await trends_client.get_trend_analysis(keyword)
        # Simulate competitor analysis (in real implementation, this would use actual data)
        competitor_data = {
            "total_competitors": random.randint(50, 500),
            "avg_price": random.uniform(10, 50),
            "top_competitor_sales": random.randint(100, 1000)
        }
        # Calculate scores using NicheScorer
        scorer = NicheScorer()
        # Create basic niche structure
        niche = Niche(
            category=f"{keyword} books",
            primary_keyword=keyword,
            related_keywords=[keyword, f"{keyword} guide", f"{keyword} tips"],
            competition_score=random.uniform(30, 80),
            profitability_score=random.uniform(40, 85),
            market_size_score=random.uniform(35, 75),
            confidence_score=random.uniform(50, 90),
            trend_analysis=trend_analysis.__dict__ if trend_analysis else None,
            competitor_data=competitor_data,
            price_range=(competitor_data["avg_price"] * 0.8, competitor_data["avg_price"] * 1.2),
            content_gaps=["beginner guides", "advanced techniques", "case studies"],
            seasonal_factors={"volatility": random.uniform(0.2, 0.8)},
            last_updated=datetime.now()
        )
        return niche
    except Exception as e:
        logger.error(f"Failed to create niche for testing: {e}")
        return None
def _calculate_overall_resilience(scenario_results: List[ScenarioResult]) -> float:
    """Calculate overall resilience score across all scenarios."""
    if not scenario_results:
        return 0.0
    # Weight by scenario probability
    weighted_resilience = 0.0
    total_weight = 0.0
    for result in scenario_results:
        weight = result.parameters.probability
        weighted_resilience += result.resilience_score * weight
        total_weight += weight
    if total_weight == 0:
        return mean([r.resilience_score for r in scenario_results])
    return round(weighted_resilience / total_weight, 1)
def _determine_risk_profile(scenario_results: List[ScenarioResult], overall_resilience: float) -> str:
    """Determine overall risk profile."""
    # Count high-impact scenarios
    high_impact_scenarios = len([r for r in scenario_results if r.impact_percentage > 50])
    low_survival_scenarios = len([r for r in scenario_results if r.survival_probability < 0.7])
    if overall_resilience >= 80 and high_impact_scenarios <= 1:
        return "low_risk"
    elif overall_resilience >= 60 and high_impact_scenarios <= 3:
        return "medium_risk"
    elif overall_resilience >= 40:
        return "high_risk"
    else:
        return "very_high_risk"
def _identify_critical_vulnerabilities(scenario_results: List[ScenarioResult]) -> List[str]:
    """Identify the most critical vulnerabilities across all scenarios."""
    vulnerability_counts = {}
    # Count vulnerability mentions across scenarios
    for result in scenario_results:
        for vulnerability in result.key_vulnerabilities:
            vulnerability_counts[vulnerability] = vulnerability_counts.get(vulnerability, 0) + 1
    # Also include vulnerabilities from high-impact scenarios
    critical_vulnerabilities = []
    for result in scenario_results:
        if result.impact_percentage > 60 or result.survival_probability < 0.6:
            critical_vulnerabilities.extend(result.key_vulnerabilities)
    # Get most common vulnerabilities
    common_vulnerabilities = [
        vuln for vuln, count in vulnerability_counts.items() 
        if count >= 2
    ]
    # Combine and deduplicate
    all_critical = list(set(critical_vulnerabilities + common_vulnerabilities))
    return all_critical[:5]  # Return top 5
def _generate_recommended_mitigations(
    scenario_results: List[ScenarioResult],
    critical_vulnerabilities: List[str]
) -> List[str]:
    """Generate prioritized mitigation recommendations."""
    mitigation_counts = {}
    # Count mitigation strategy mentions
    for result in scenario_results:
        for mitigation in result.mitigation_strategies:
            weight = result.parameters.probability * (result.impact_percentage / 100)
            mitigation_counts[mitigation] = mitigation_counts.get(mitigation, 0) + weight
    # Sort by weighted importance
    sorted_mitigations = sorted(
        mitigation_counts.items(),
        key=lambda x: x[1],
        reverse=True
    )
    return [mitigation for mitigation, _ in sorted_mitigations[:8]]
def _calculate_test_confidence(
    niche: Niche,
    trend_analysis: Optional[TrendAnalysis],
    scenarios_tested: int
) -> float:
    """Calculate confidence level in the stress test results."""
    base_confidence = 0.7
    # Factor in niche data quality
    base_confidence += (niche.confidence_score / 100) * 0.2
    # Factor in trend data availability
    if trend_analysis:
        base_confidence += trend_analysis.confidence_level * 0.15
    # Factor in number of scenarios tested
    scenario_factor = min(1.0, scenarios_tested / 8) * 0.15
    base_confidence += scenario_factor
    return round(min(1.0, base_confidence), 2)
def _get_highest_risk_scenarios(scenario_results: List[ScenarioResult]) -> List[Dict[str, Any]]:
    """Get scenarios with highest risk (impact * probability)."""
    risk_scenarios = []
    for result in scenario_results:
        risk_score = (result.impact_percentage / 100) * result.parameters.probability
        risk_scenarios.append({
            "scenario": result.scenario.value,
            "risk_score": round(risk_score, 3),
            "impact_percentage": result.impact_percentage,
            "probability": result.parameters.probability
        })
    return sorted(risk_scenarios, key=lambda x: x["risk_score"], reverse=True)[:3]
def _get_lowest_resilience_scenarios(scenario_results: List[ScenarioResult]) -> List[Dict[str, Any]]:
    """Get scenarios with lowest resilience scores."""
    resilience_scenarios = [
        {
            "scenario": result.scenario.value,
            "resilience_score": result.resilience_score,
            "survival_probability": result.survival_probability
        }
        for result in scenario_results
    ]
    return sorted(resilience_scenarios, key=lambda x: x["resilience_score"])[:3]
def _analyze_survival_probabilities(scenario_results: List[ScenarioResult]) -> Dict[str, Any]:
    """Analyze survival probabilities across scenarios."""
    survival_probs = [r.survival_probability for r in scenario_results]
    return {
        "average_survival_probability": round(mean(survival_probs), 3),
        "worst_case_survival": round(min(survival_probs), 3),
        "best_case_survival": round(max(survival_probs), 3),
        "scenarios_below_70_percent": len([p for p in survival_probs if p < 0.7]),
        "scenarios_below_50_percent": len([p for p in survival_probs if p < 0.5])
    }
def _get_monitoring_priorities(scenario_results: List[ScenarioResult]) -> List[str]:
    """Get monitoring priorities based on stress test results."""
    priorities = []
    # High-probability, high-impact scenarios need monitoring
    for result in scenario_results:
        if result.parameters.probability > 0.3 and result.impact_percentage > 40:
            priorities.append(f"Monitor indicators for {result.scenario.value}")
    # General monitoring recommendations
    priorities.extend([
        "Track competitor entry rates",
        "Monitor trend strength and direction",
        "Watch for seasonal pattern changes",
        "Track platform policy updates"
    ])
    return priorities[:5]
def _get_contingency_recommendations(scenario_results: List[ScenarioResult]) -> List[str]:
    """Get contingency planning recommendations."""
    recommendations = [
        "Develop pivot strategies for related niches",
        "Maintain diversified content portfolio",
        "Build emergency fund for market downturns",
        "Create rapid response protocols for competitive threats",
        "Establish alternative revenue streams"
    ]
    # Add scenario-specific contingencies
    high_risk_scenarios = [r for r in scenario_results if r.impact_percentage > 60]
    if any(r.scenario == StressScenario.TREND_REVERSAL for r in high_risk_scenarios):
        recommendations.append("Prepare trend reversal detection and response plan")
    if any(r.scenario == StressScenario.COMPETITIVE_FLOODING for r in high_risk_scenarios):
        recommendations.append("Develop competitive differentiation strategies")
    return recommendations[:6]
def _assess_niche_data_completeness(niche: Niche) -> float:
    """Assess completeness of niche data for testing."""
    completeness_score = 0.0
    total_factors = 8
    if niche.primary_keyword:
        completeness_score += 1
    if niche.related_keywords:
        completeness_score += 1
    if niche.competition_score > 0:
        completeness_score += 1
    if niche.profitability_score > 0:
        completeness_score += 1
    if niche.market_size_score > 0:
        completeness_score += 1
    if niche.trend_analysis:
        completeness_score += 1
    if niche.competitor_data:
        completeness_score += 1
    if niche.price_range:
        completeness_score += 1
    return round(completeness_score / total_factors, 2)
</file>

<file path="src/kdp_strategist/agent/tools/trend_validation.py">
"""Trend Validation Tool.
Validates and analyzes market trends to assess:
- Trend strength and sustainability
- Seasonal patterns and cyclical behavior
- Growth trajectory and momentum
- Market timing opportunities
- Risk factors and volatility
- Future trend forecasting
The tool provides comprehensive trend analysis including:
- Historical trend validation
- Seasonal decomposition
- Trend forecasting models
- Risk assessment metrics
- Market timing recommendations
"""
import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from statistics import mean, stdev
import math
from ...data.cache_manager import CacheManager
from ...data.trends_client import TrendsClient, TrendData
from ...models.trend_model import TrendAnalysis, TrendDirection, TrendStrength
from ...models.niche_model import Niche
logger = logging.getLogger(__name__)
@dataclass
class SeasonalPattern:
    """Represents a seasonal pattern in trend data."""
    season: str  # spring, summer, fall, winter
    months: List[int]
    avg_intensity: float
    peak_month: int
    volatility: float
    reliability_score: float
@dataclass
class TrendForecast:
    """Forecast data for future trend performance."""
    timeframe: str  # "1_month", "3_months", "6_months", "12_months"
    predicted_score: float
    confidence_interval: Tuple[float, float]
    direction: TrendDirection
    key_factors: List[str]
    risk_level: str
@dataclass
class ValidationResult:
    """Complete trend validation result."""
    keyword: str
    is_valid: bool
    strength_score: float
    sustainability_score: float
    seasonal_analysis: Dict[str, Any]
    forecasts: List[TrendForecast]
    risk_factors: List[str]
    opportunities: List[str]
    recommendations: List[str]
    validation_timestamp: datetime
class TrendValidator:
    """Core trend validation and analysis engine."""
    # Validation thresholds
    MIN_TREND_SCORE = 20
    MIN_DATA_POINTS = 12  # Minimum months of data
    MAX_VOLATILITY = 0.8  # Maximum acceptable volatility
    MIN_CONFIDENCE = 0.3
    @classmethod
    def validate_trend_strength(
        cls,
        trend_analysis: TrendAnalysis,
        historical_data: Optional[List[TrendData]] = None
    ) -> Dict[str, Any]:
        """Validate the strength and reliability of a trend."""
        validation = {
            "is_strong": False,
            "strength_score": 0.0,
            "reliability_factors": [],
            "weakness_factors": []
        }
        # Base strength from trend score
        base_strength = trend_analysis.trend_score
        validation["strength_score"] = base_strength
        # Check minimum threshold
        if base_strength < cls.MIN_TREND_SCORE:
            validation["weakness_factors"].append(f"Trend score {base_strength} below minimum threshold {cls.MIN_TREND_SCORE}")
            return validation
        # Validate confidence level
        if trend_analysis.confidence_level < cls.MIN_CONFIDENCE:
            validation["weakness_factors"].append(f"Low confidence level: {trend_analysis.confidence_level:.2f}")
        else:
            validation["reliability_factors"].append(f"Good confidence level: {trend_analysis.confidence_level:.2f}")
        # Validate trend direction
        if trend_analysis.direction == TrendDirection.RISING:
            validation["reliability_factors"].append("Positive trend direction")
            validation["strength_score"] *= 1.1
        elif trend_analysis.direction == TrendDirection.DECLINING:
            validation["weakness_factors"].append("Declining trend direction")
            validation["strength_score"] *= 0.8
        # Validate trend strength category
        if trend_analysis.strength in [TrendStrength.STRONG, TrendStrength.VERY_STRONG]:
            validation["reliability_factors"].append(f"Strong trend classification: {trend_analysis.strength.value}")
            validation["strength_score"] *= 1.2
        elif trend_analysis.strength == TrendStrength.WEAK:
            validation["weakness_factors"].append(f"Weak trend classification: {trend_analysis.strength.value}")
            validation["strength_score"] *= 0.7
        # Historical data validation
        if historical_data and len(historical_data) >= cls.MIN_DATA_POINTS:
            historical_validation = cls._validate_historical_consistency(historical_data)
            validation["reliability_factors"].extend(historical_validation["strengths"])
            validation["weakness_factors"].extend(historical_validation["weaknesses"])
            validation["strength_score"] *= historical_validation["consistency_multiplier"]
        # Final determination
        validation["is_strong"] = (
            validation["strength_score"] >= cls.MIN_TREND_SCORE * 1.5 and
            len(validation["weakness_factors"]) <= len(validation["reliability_factors"])
        )
        return validation
    @classmethod
    def _validate_historical_consistency(
        cls,
        historical_data: List[TrendData]
    ) -> Dict[str, Any]:
        """Validate consistency in historical trend data."""
        if len(historical_data) < 3:
            return {
                "strengths": [],
                "weaknesses": ["Insufficient historical data"],
                "consistency_multiplier": 0.8
            }
        # Extract trend scores over time
        scores = [data.trend_score for data in historical_data if data.trend_score is not None]
        if len(scores) < 3:
            return {
                "strengths": [],
                "weaknesses": ["Insufficient trend score data"],
                "consistency_multiplier": 0.8
            }
        # Calculate consistency metrics
        avg_score = mean(scores)
        score_stdev = stdev(scores) if len(scores) > 1 else 0
        volatility = score_stdev / avg_score if avg_score > 0 else 1
        strengths = []
        weaknesses = []
        multiplier = 1.0
        # Volatility analysis
        if volatility <= 0.2:
            strengths.append("Very stable trend with low volatility")
            multiplier *= 1.1
        elif volatility <= 0.4:
            strengths.append("Moderately stable trend")
        elif volatility <= cls.MAX_VOLATILITY:
            weaknesses.append("Moderate volatility in trend")
            multiplier *= 0.95
        else:
            weaknesses.append(f"High volatility: {volatility:.2f}")
            multiplier *= 0.8
        # Trend progression analysis
        if len(scores) >= 6:
            recent_avg = mean(scores[-3:])
            older_avg = mean(scores[:3])
            if recent_avg > older_avg * 1.1:
                strengths.append("Improving trend over time")
                multiplier *= 1.1
            elif recent_avg < older_avg * 0.9:
                weaknesses.append("Declining trend over time")
                multiplier *= 0.9
        return {
            "strengths": strengths,
            "weaknesses": weaknesses,
            "consistency_multiplier": multiplier
        }
    @classmethod
    def analyze_seasonality(
        cls,
        trend_analysis: TrendAnalysis,
        historical_data: Optional[List[TrendData]] = None
    ) -> Dict[str, Any]:
        """Analyze seasonal patterns in trend data."""
        seasonal_analysis = {
            "has_seasonality": False,
            "seasonality_strength": 0.0,
            "seasonal_patterns": [],
            "peak_seasons": [],
            "low_seasons": [],
            "seasonal_risk": "low"
        }
        # Use existing seasonal patterns from trend analysis
        if trend_analysis.seasonal_patterns:
            seasonal_analysis["has_seasonality"] = True
            seasonal_analysis.update(trend_analysis.seasonal_patterns)
        # Enhanced analysis with historical data
        if historical_data and len(historical_data) >= 12:
            enhanced_seasonal = cls._analyze_historical_seasonality(historical_data)
            seasonal_analysis.update(enhanced_seasonal)
        # Determine seasonal risk level
        seasonality_strength = seasonal_analysis.get("seasonality_strength", 0)
        if seasonality_strength > 0.7:
            seasonal_analysis["seasonal_risk"] = "high"
        elif seasonality_strength > 0.4:
            seasonal_analysis["seasonal_risk"] = "medium"
        else:
            seasonal_analysis["seasonal_risk"] = "low"
        return seasonal_analysis
    @classmethod
    def _analyze_historical_seasonality(
        cls,
        historical_data: List[TrendData]
    ) -> Dict[str, Any]:
        """Analyze seasonality from historical data points."""
        # Group data by month
        monthly_scores = {i: [] for i in range(1, 13)}
        for data_point in historical_data:
            if data_point.date and data_point.trend_score is not None:
                month = data_point.date.month
                monthly_scores[month].append(data_point.trend_score)
        # Calculate monthly averages
        monthly_averages = {}
        for month, scores in monthly_scores.items():
            if scores:
                monthly_averages[month] = mean(scores)
        if len(monthly_averages) < 6:  # Need at least 6 months
            return {"seasonality_strength": 0.0}
        # Calculate seasonality metrics
        avg_scores = list(monthly_averages.values())
        overall_avg = mean(avg_scores)
        max_score = max(avg_scores)
        min_score = min(avg_scores)
        # Seasonality strength (coefficient of variation)
        score_stdev = stdev(avg_scores) if len(avg_scores) > 1 else 0
        seasonality_strength = score_stdev / overall_avg if overall_avg > 0 else 0
        # Identify peak and low seasons
        peak_months = [month for month, score in monthly_averages.items() 
                      if score >= max_score * 0.9]
        low_months = [month for month, score in monthly_averages.items() 
                     if score <= min_score * 1.1]
        # Create seasonal patterns
        seasonal_patterns = cls._create_seasonal_patterns(monthly_averages)
        return {
            "seasonality_strength": min(1.0, seasonality_strength),
            "monthly_averages": monthly_averages,
            "peak_months": peak_months,
            "low_months": low_months,
            "seasonal_patterns": seasonal_patterns
        }
    @classmethod
    def _create_seasonal_patterns(
        cls,
        monthly_averages: Dict[int, float]
    ) -> List[SeasonalPattern]:
        """Create seasonal pattern objects from monthly data."""
        # Define seasons
        seasons = {
            "spring": [3, 4, 5],
            "summer": [6, 7, 8],
            "fall": [9, 10, 11],
            "winter": [12, 1, 2]
        }
        patterns = []
        for season_name, months in seasons.items():
            season_scores = [monthly_averages.get(month, 0) for month in months 
                           if month in monthly_averages]
            if season_scores:
                avg_intensity = mean(season_scores)
                peak_month = months[season_scores.index(max(season_scores))]
                volatility = stdev(season_scores) / avg_intensity if len(season_scores) > 1 and avg_intensity > 0 else 0
                # Calculate reliability based on data availability and consistency
                reliability = min(1.0, len(season_scores) / 3 * (1 - min(volatility, 1.0)))
                pattern = SeasonalPattern(
                    season=season_name,
                    months=months,
                    avg_intensity=avg_intensity,
                    peak_month=peak_month,
                    volatility=volatility,
                    reliability_score=reliability
                )
                patterns.append(pattern)
        return patterns
    @classmethod
    def generate_forecasts(
        cls,
        trend_analysis: TrendAnalysis,
        historical_data: Optional[List[TrendData]] = None,
        seasonal_analysis: Optional[Dict[str, Any]] = None
    ) -> List[TrendForecast]:
        """Generate trend forecasts for different timeframes."""
        forecasts = []
        # Base forecast parameters
        current_score = trend_analysis.trend_score
        current_direction = trend_analysis.direction
        confidence = trend_analysis.confidence_level
        # Forecast timeframes
        timeframes = [
            ("1_month", 30),
            ("3_months", 90),
            ("6_months", 180),
            ("12_months", 365)
        ]
        for timeframe_name, days in timeframes:
            forecast = cls._generate_single_forecast(
                timeframe_name,
                days,
                current_score,
                current_direction,
                confidence,
                historical_data,
                seasonal_analysis
            )
            forecasts.append(forecast)
        return forecasts
    @classmethod
    def _generate_single_forecast(
        cls,
        timeframe: str,
        days: int,
        current_score: float,
        current_direction: TrendDirection,
        confidence: float,
        historical_data: Optional[List[TrendData]] = None,
        seasonal_analysis: Optional[Dict[str, Any]] = None
    ) -> TrendForecast:
        """Generate a single forecast for a specific timeframe."""
        # Base prediction
        predicted_score = current_score
        # Apply trend direction
        direction_multipliers = {
            TrendDirection.RISING: 1.1,
            TrendDirection.STABLE: 1.0,
            TrendDirection.DECLINING: 0.9
        }
        base_multiplier = direction_multipliers.get(current_direction, 1.0)
        # Apply time decay (longer forecasts are less reliable)
        time_decay = 1.0 - (days / 365) * 0.2  # Max 20% decay over 1 year
        # Apply historical trend if available
        historical_multiplier = 1.0
        if historical_data and len(historical_data) >= 6:
            historical_multiplier = cls._calculate_historical_trend_multiplier(historical_data)
        # Apply seasonal adjustment
        seasonal_multiplier = 1.0
        if seasonal_analysis and seasonal_analysis.get("has_seasonality"):
            seasonal_multiplier = cls._calculate_seasonal_multiplier(
                seasonal_analysis, days
            )
        # Calculate final prediction
        predicted_score = current_score * base_multiplier * time_decay * historical_multiplier * seasonal_multiplier
        predicted_score = max(0, min(100, predicted_score))  # Clamp to valid range
        # Calculate confidence interval
        uncertainty = (1 - confidence) * 20  # Max 20 point uncertainty
        uncertainty *= (days / 365)  # Increase uncertainty over time
        confidence_interval = (
            max(0, predicted_score - uncertainty),
            min(100, predicted_score + uncertainty)
        )
        # Determine predicted direction
        if predicted_score > current_score * 1.05:
            predicted_direction = TrendDirection.RISING
        elif predicted_score < current_score * 0.95:
            predicted_direction = TrendDirection.DECLINING
        else:
            predicted_direction = TrendDirection.STABLE
        # Identify key factors
        key_factors = []
        if abs(base_multiplier - 1.0) > 0.05:
            key_factors.append(f"Current trend direction: {current_direction.value}")
        if abs(historical_multiplier - 1.0) > 0.05:
            key_factors.append("Historical trend pattern")
        if abs(seasonal_multiplier - 1.0) > 0.1:
            key_factors.append("Seasonal effects")
        # Determine risk level
        risk_level = "low"
        if uncertainty > 15:
            risk_level = "high"
        elif uncertainty > 8:
            risk_level = "medium"
        return TrendForecast(
            timeframe=timeframe,
            predicted_score=round(predicted_score, 1),
            confidence_interval=(
                round(confidence_interval[0], 1),
                round(confidence_interval[1], 1)
            ),
            direction=predicted_direction,
            key_factors=key_factors,
            risk_level=risk_level
        )
    @classmethod
    def _calculate_historical_trend_multiplier(
        cls,
        historical_data: List[TrendData]
    ) -> float:
        """Calculate trend multiplier based on historical data."""
        scores = [data.trend_score for data in historical_data 
                 if data.trend_score is not None]
        if len(scores) < 3:
            return 1.0
        # Calculate trend slope
        n = len(scores)
        x_values = list(range(n))
        # Simple linear regression slope
        x_mean = mean(x_values)
        y_mean = mean(scores)
        numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(x_values, scores))
        denominator = sum((x - x_mean) ** 2 for x in x_values)
        if denominator == 0:
            return 1.0
        slope = numerator / denominator
        # Convert slope to multiplier (normalize by time period)
        # Positive slope = upward trend, negative = downward
        multiplier = 1.0 + (slope / y_mean) * 0.5 if y_mean > 0 else 1.0
        # Clamp multiplier to reasonable range
        return max(0.5, min(2.0, multiplier))
    @classmethod
    def _calculate_seasonal_multiplier(
        cls,
        seasonal_analysis: Dict[str, Any],
        forecast_days: int
    ) -> float:
        """Calculate seasonal adjustment multiplier."""
        # Get current date and forecast date
        current_date = datetime.now()
        forecast_date = current_date + timedelta(days=forecast_days)
        # Get seasonal patterns
        monthly_averages = seasonal_analysis.get("monthly_averages", {})
        if not monthly_averages:
            return 1.0
        # Get scores for current and forecast months
        current_month_score = monthly_averages.get(current_date.month, 0)
        forecast_month_score = monthly_averages.get(forecast_date.month, 0)
        if current_month_score == 0:
            return 1.0
        # Calculate seasonal adjustment
        seasonal_ratio = forecast_month_score / current_month_score
        # Moderate the adjustment (don't apply full seasonal effect)
        seasonality_strength = seasonal_analysis.get("seasonality_strength", 0)
        adjustment = 1.0 + (seasonal_ratio - 1.0) * seasonality_strength * 0.5
        return max(0.5, min(2.0, adjustment))
async def validate_trend(
    trends_client: TrendsClient,
    cache_manager: CacheManager,
    keyword: str,
    timeframe: str = "today 12-m",
    include_forecasts: bool = True,
    include_seasonality: bool = True
) -> Dict[str, Any]:
    """Validate a trend comprehensively.
    Args:
        trends_client: Google Trends client
        cache_manager: Cache manager for performance
        keyword: Keyword to validate
        timeframe: Timeframe for analysis
        include_forecasts: Whether to generate forecasts
        include_seasonality: Whether to analyze seasonality
    Returns:
        Dictionary containing comprehensive trend validation
    """
    logger.info(f"Validating trend for keyword: {keyword}")
    try:
        # Get current trend analysis
        trend_analysis = await trends_client.get_trend_analysis(keyword, timeframe)
        if not trend_analysis:
            return {
                "error": "Unable to retrieve trend data",
                "keyword": keyword,
                "validation_timestamp": datetime.now().isoformat()
            }
        # Get historical data for enhanced analysis
        historical_data = None
        try:
            historical_data = await trends_client.get_raw_trend_data(
                keyword, timeframe="today 24-m"  # 2 years for better analysis
            )
        except Exception as e:
            logger.warning(f"Could not retrieve historical data: {e}")
        # Validate trend strength
        strength_validation = TrendValidator.validate_trend_strength(
            trend_analysis, historical_data
        )
        # Analyze seasonality
        seasonal_analysis = None
        if include_seasonality:
            seasonal_analysis = TrendValidator.analyze_seasonality(
                trend_analysis, historical_data
            )
        # Generate forecasts
        forecasts = []
        if include_forecasts:
            forecasts = TrendValidator.generate_forecasts(
                trend_analysis, historical_data, seasonal_analysis
            )
        # Assess sustainability
        sustainability_score = _calculate_sustainability_score(
            trend_analysis, strength_validation, seasonal_analysis, historical_data
        )
        # Identify risk factors
        risk_factors = _identify_risk_factors(
            trend_analysis, strength_validation, seasonal_analysis, forecasts
        )
        # Identify opportunities
        opportunities = _identify_opportunities(
            trend_analysis, strength_validation, seasonal_analysis, forecasts
        )
        # Generate recommendations
        recommendations = _generate_trend_recommendations(
            trend_analysis, strength_validation, seasonal_analysis, 
            forecasts, sustainability_score
        )
        # Create validation result
        validation_result = ValidationResult(
            keyword=keyword,
            is_valid=strength_validation["is_strong"] and sustainability_score >= 60,
            strength_score=strength_validation["strength_score"],
            sustainability_score=sustainability_score,
            seasonal_analysis=seasonal_analysis or {},
            forecasts=forecasts,
            risk_factors=risk_factors,
            opportunities=opportunities,
            recommendations=recommendations,
            validation_timestamp=datetime.now()
        )
        # Compile final result
        result = {
            "validation_result": {
                "keyword": validation_result.keyword,
                "is_valid": validation_result.is_valid,
                "overall_score": round((validation_result.strength_score + validation_result.sustainability_score) / 2, 1)
            },
            "trend_analysis": {
                "current_score": trend_analysis.trend_score,
                "direction": trend_analysis.direction.value,
                "strength": trend_analysis.strength.value,
                "confidence": trend_analysis.confidence_level
            },
            "strength_validation": strength_validation,
            "sustainability_analysis": {
                "score": sustainability_score,
                "factors": _get_sustainability_factors(sustainability_score)
            },
            "seasonal_analysis": seasonal_analysis,
            "forecasts": [
                {
                    "timeframe": f.timeframe,
                    "predicted_score": f.predicted_score,
                    "confidence_interval": f.confidence_interval,
                    "direction": f.direction.value,
                    "risk_level": f.risk_level,
                    "key_factors": f.key_factors
                }
                for f in forecasts
            ],
            "risk_assessment": {
                "risk_factors": risk_factors,
                "overall_risk": _calculate_overall_risk(risk_factors, forecasts)
            },
            "opportunities": opportunities,
            "recommendations": recommendations,
            "data_quality": {
                "has_historical_data": historical_data is not None,
                "data_points": len(historical_data) if historical_data else 0,
                "confidence_level": trend_analysis.confidence_level,
                "analysis_timeframe": timeframe
            },
            "validation_timestamp": validation_result.validation_timestamp.isoformat()
        }
        logger.info(f"Trend validation completed for: {keyword}")
        return result
    except Exception as e:
        logger.error(f"Trend validation failed for {keyword}: {e}")
        return {
            "error": str(e),
            "keyword": keyword,
            "validation_timestamp": datetime.now().isoformat()
        }
def _calculate_sustainability_score(
    trend_analysis: TrendAnalysis,
    strength_validation: Dict[str, Any],
    seasonal_analysis: Optional[Dict[str, Any]],
    historical_data: Optional[List[TrendData]]
) -> float:
    """Calculate trend sustainability score."""
    base_score = 50.0
    # Factor in trend strength
    base_score += (strength_validation["strength_score"] - 50) * 0.3
    # Factor in trend direction
    if trend_analysis.direction == TrendDirection.RISING:
        base_score += 15
    elif trend_analysis.direction == TrendDirection.DECLINING:
        base_score -= 20
    # Factor in confidence
    base_score += (trend_analysis.confidence_level - 0.5) * 40
    # Factor in seasonality (less seasonal = more sustainable)
    if seasonal_analysis:
        seasonality_strength = seasonal_analysis.get("seasonality_strength", 0)
        base_score -= seasonality_strength * 20
    # Factor in historical consistency
    if historical_data and len(historical_data) >= 6:
        scores = [d.trend_score for d in historical_data if d.trend_score is not None]
        if scores:
            volatility = stdev(scores) / mean(scores) if mean(scores) > 0 else 1
            base_score -= volatility * 30
    return max(0, min(100, base_score))
def _identify_risk_factors(
    trend_analysis: TrendAnalysis,
    strength_validation: Dict[str, Any],
    seasonal_analysis: Optional[Dict[str, Any]],
    forecasts: List[TrendForecast]
) -> List[str]:
    """Identify risk factors for the trend."""
    risk_factors = []
    # Add weakness factors from strength validation
    risk_factors.extend(strength_validation.get("weakness_factors", []))
    # Seasonal risks
    if seasonal_analysis and seasonal_analysis.get("seasonal_risk") == "high":
        risk_factors.append("High seasonal volatility may affect consistency")
    # Forecast risks
    declining_forecasts = [f for f in forecasts if f.direction == TrendDirection.DECLINING]
    if len(declining_forecasts) >= 2:
        risk_factors.append("Multiple forecasts show declining trend")
    high_risk_forecasts = [f for f in forecasts if f.risk_level == "high"]
    if high_risk_forecasts:
        risk_factors.append(f"High uncertainty in {len(high_risk_forecasts)} forecast(s)")
    # Confidence risks
    if trend_analysis.confidence_level < 0.4:
        risk_factors.append("Low confidence in trend data")
    return risk_factors
def _identify_opportunities(
    trend_analysis: TrendAnalysis,
    strength_validation: Dict[str, Any],
    seasonal_analysis: Optional[Dict[str, Any]],
    forecasts: List[TrendForecast]
) -> List[str]:
    """Identify opportunities from the trend analysis."""
    opportunities = []
    # Strong trend opportunities
    if strength_validation["is_strong"]:
        opportunities.append("Strong trend provides good market entry opportunity")
    # Rising trend opportunities
    if trend_analysis.direction == TrendDirection.RISING:
        opportunities.append("Rising trend suggests growing market interest")
    # Seasonal opportunities
    if seasonal_analysis and seasonal_analysis.get("peak_months"):
        peak_months = seasonal_analysis["peak_months"]
        current_month = datetime.now().month
        if current_month in peak_months:
            opportunities.append("Currently in peak season for this trend")
        else:
            next_peak = min([m for m in peak_months if m > current_month] + 
                           [m + 12 for m in peak_months if m <= current_month])
            months_to_peak = (next_peak - current_month) % 12
            opportunities.append(f"Peak season approaching in {months_to_peak} month(s)")
    # Forecast opportunities
    rising_forecasts = [f for f in forecasts if f.direction == TrendDirection.RISING]
    if rising_forecasts:
        opportunities.append(f"Positive growth predicted in {len(rising_forecasts)} forecast period(s)")
    return opportunities
def _generate_trend_recommendations(
    trend_analysis: TrendAnalysis,
    strength_validation: Dict[str, Any],
    seasonal_analysis: Optional[Dict[str, Any]],
    forecasts: List[TrendForecast],
    sustainability_score: float
) -> List[str]:
    """Generate actionable recommendations based on trend analysis."""
    recommendations = []
    # Overall trend recommendations
    if strength_validation["is_strong"] and sustainability_score >= 70:
        recommendations.append("Excellent trend - proceed with confidence")
    elif strength_validation["is_strong"]:
        recommendations.append("Good trend but monitor sustainability factors")
    elif sustainability_score >= 70:
        recommendations.append("Sustainable trend but consider strengthening market position")
    else:
        recommendations.append("Weak trend - consider alternative niches or wait for improvement")
    # Timing recommendations
    if seasonal_analysis and seasonal_analysis.get("has_seasonality"):
        peak_months = seasonal_analysis.get("peak_months", [])
        current_month = datetime.now().month
        if current_month in peak_months:
            recommendations.append("Optimal timing - launch during current peak season")
        elif peak_months:
            recommendations.append("Plan launch timing around seasonal peaks for maximum impact")
    # Forecast-based recommendations
    short_term_forecast = next((f for f in forecasts if f.timeframe == "3_months"), None)
    if short_term_forecast:
        if short_term_forecast.direction == TrendDirection.RISING:
            recommendations.append("Short-term growth expected - good time for market entry")
        elif short_term_forecast.direction == TrendDirection.DECLINING:
            recommendations.append("Short-term decline expected - consider delaying entry or focus on differentiation")
    # Risk mitigation recommendations
    if trend_analysis.confidence_level < 0.5:
        recommendations.append("Low data confidence - validate with additional market research")
    if seasonal_analysis and seasonal_analysis.get("seasonal_risk") == "high":
        recommendations.append("High seasonality - develop year-round content strategy")
    return recommendations
def _get_sustainability_factors(sustainability_score: float) -> List[str]:
    """Get factors that contribute to sustainability score."""
    if sustainability_score >= 80:
        return ["Highly sustainable trend with strong fundamentals"]
    elif sustainability_score >= 60:
        return ["Moderately sustainable with some risk factors"]
    elif sustainability_score >= 40:
        return ["Limited sustainability - requires careful monitoring"]
    else:
        return ["Low sustainability - high risk of trend decline"]
def _calculate_overall_risk(risk_factors: List[str], forecasts: List[TrendForecast]) -> str:
    """Calculate overall risk level."""
    risk_score = len(risk_factors) * 10
    # Add forecast risk
    high_risk_forecasts = len([f for f in forecasts if f.risk_level == "high"])
    risk_score += high_risk_forecasts * 15
    declining_forecasts = len([f for f in forecasts if f.direction == TrendDirection.DECLINING])
    risk_score += declining_forecasts * 10
    if risk_score >= 50:
        return "high"
    elif risk_score >= 25:
        return "medium"
    else:
        return "low"
</file>

<file path="src/kdp_strategist/data/__init__.py">
"""Data layer for KDP Strategist AI Agent.
This module provides data access and caching functionality including:
- Cache management for API responses
- Keepa API client for Amazon product data
- Google Trends client for trend analysis
- Rate limiting and retry logic
- Data validation and transformation
"""
from .cache_manager import CacheManager, CacheConfig
from .keepa_client import KeepaClient
from .trends_client import TrendsClient
__all__ = [
    "CacheManager",
    "CacheConfig", 
    "KeepaClient",
    "TrendsClient",
]
</file>

<file path="src/kdp_strategist/data/cache_manager.py">
"""Cache management system for KDP Strategist.
Provides flexible caching with support for:
- File-based caching for development
- Redis caching for production
- Memory caching for temporary data
- TTL (Time To Live) management
- Cache invalidation and cleanup
"""
import json
import pickle
import hashlib
import logging
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Optional, Dict, Union, List
from dataclasses import dataclass
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
logger = logging.getLogger(__name__)
@dataclass
class CacheConfig:
    """Configuration for cache management."""
    cache_type: str = "file"  # "file", "redis", "memory"
    cache_dir: Path = Path("cache")
    redis_url: Optional[str] = None
    default_ttl: int = 3600  # seconds (1 hour)
    max_cache_size: int = 1000  # max items in memory cache
    cleanup_interval: int = 86400  # seconds (24 hours)
    compression: bool = True
    serialization: str = "json"  # "json", "pickle"
class CacheBackend(ABC):
    """Abstract base class for cache backends."""
    @abstractmethod
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache."""
        pass
    @abstractmethod
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set value in cache with optional TTL."""
        pass
    @abstractmethod
    def delete(self, key: str) -> bool:
        """Delete value from cache."""
        pass
    @abstractmethod
    def exists(self, key: str) -> bool:
        """Check if key exists in cache."""
        pass
    @abstractmethod
    def clear(self) -> bool:
        """Clear all cache entries."""
        pass
    @abstractmethod
    def cleanup_expired(self) -> int:
        """Remove expired entries and return count removed."""
        pass
class MemoryCacheBackend(CacheBackend):
    """In-memory cache backend for temporary storage."""
    def __init__(self, max_size: int = 1000):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.max_size = max_size
        self.access_times: Dict[str, datetime] = {}
    def get(self, key: str) -> Optional[Any]:
        """Get value from memory cache."""
        if key not in self.cache:
            return None
        entry = self.cache[key]
        # Check if expired
        if self._is_expired(entry):
            self.delete(key)
            return None
        # Update access time for LRU
        self.access_times[key] = datetime.now()
        return entry["value"]
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set value in memory cache."""
        try:
            # Evict if at capacity
            if len(self.cache) >= self.max_size and key not in self.cache:
                self._evict_lru()
            expires_at = None
            if ttl is not None:
                expires_at = datetime.now() + timedelta(seconds=ttl)
            self.cache[key] = {
                "value": value,
                "created_at": datetime.now(),
                "expires_at": expires_at,
            }
            self.access_times[key] = datetime.now()
            return True
        except Exception as e:
            logger.error(f"Failed to set cache key {key}: {e}")
            return False
    def delete(self, key: str) -> bool:
        """Delete value from memory cache."""
        if key in self.cache:
            del self.cache[key]
            self.access_times.pop(key, None)
            return True
        return False
    def exists(self, key: str) -> bool:
        """Check if key exists and is not expired."""
        if key not in self.cache:
            return False
        if self._is_expired(self.cache[key]):
            self.delete(key)
            return False
        return True
    def clear(self) -> bool:
        """Clear all cache entries."""
        self.cache.clear()
        self.access_times.clear()
        return True
    def cleanup_expired(self) -> int:
        """Remove expired entries."""
        expired_keys = []
        for key, entry in self.cache.items():
            if self._is_expired(entry):
                expired_keys.append(key)
        for key in expired_keys:
            self.delete(key)
        return len(expired_keys)
    def _is_expired(self, entry: Dict[str, Any]) -> bool:
        """Check if cache entry is expired."""
        expires_at = entry.get("expires_at")
        return expires_at is not None and datetime.now() > expires_at
    def _evict_lru(self) -> None:
        """Evict least recently used item."""
        if not self.access_times:
            return
        lru_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        self.delete(lru_key)
class FileCacheBackend(CacheBackend):
    """File-based cache backend for persistent storage."""
    def __init__(self, cache_dir: Path, serialization: str = "json", compression: bool = True):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.serialization = serialization
        self.compression = compression
        # Create metadata directory
        self.meta_dir = self.cache_dir / "metadata"
        self.meta_dir.mkdir(exist_ok=True)
    def get(self, key: str) -> Optional[Any]:
        """Get value from file cache."""
        try:
            cache_file = self._get_cache_file(key)
            meta_file = self._get_meta_file(key)
            if not cache_file.exists() or not meta_file.exists():
                return None
            # Check metadata for expiration
            with open(meta_file, 'r') as f:
                metadata = json.load(f)
            expires_at = metadata.get("expires_at")
            if expires_at and datetime.fromisoformat(expires_at) < datetime.now():
                self.delete(key)
                return None
            # Load data
            if self.serialization == "json":
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:  # pickle
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
        except Exception as e:
            logger.error(f"Failed to get cache key {key}: {e}")
            return None
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set value in file cache."""
        try:
            cache_file = self._get_cache_file(key)
            meta_file = self._get_meta_file(key)
            # Save data
            if self.serialization == "json":
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(value, f, indent=2, default=str)
            else:  # pickle
                with open(cache_file, 'wb') as f:
                    pickle.dump(value, f)
            # Save metadata
            expires_at = None
            if ttl is not None:
                expires_at = (datetime.now() + timedelta(seconds=ttl)).isoformat()
            metadata = {
                "key": key,
                "created_at": datetime.now().isoformat(),
                "expires_at": expires_at,
                "serialization": self.serialization,
            }
            with open(meta_file, 'w') as f:
                json.dump(metadata, f, indent=2)
            return True
        except Exception as e:
            logger.error(f"Failed to set cache key {key}: {e}")
            return False
    def delete(self, key: str) -> bool:
        """Delete value from file cache."""
        try:
            cache_file = self._get_cache_file(key)
            meta_file = self._get_meta_file(key)
            deleted = False
            if cache_file.exists():
                cache_file.unlink()
                deleted = True
            if meta_file.exists():
                meta_file.unlink()
                deleted = True
            return deleted
        except Exception as e:
            logger.error(f"Failed to delete cache key {key}: {e}")
            return False
    def exists(self, key: str) -> bool:
        """Check if key exists and is not expired."""
        return self.get(key) is not None
    def clear(self) -> bool:
        """Clear all cache entries."""
        try:
            import shutil
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self.meta_dir.mkdir(exist_ok=True)
            return True
        except Exception as e:
            logger.error(f"Failed to clear cache: {e}")
            return False
    def cleanup_expired(self) -> int:
        """Remove expired entries."""
        removed_count = 0
        try:
            for meta_file in self.meta_dir.glob("*.json"):
                try:
                    with open(meta_file, 'r') as f:
                        metadata = json.load(f)
                    expires_at = metadata.get("expires_at")
                    if expires_at and datetime.fromisoformat(expires_at) < datetime.now():
                        key = metadata["key"]
                        if self.delete(key):
                            removed_count += 1
                except Exception as e:
                    logger.warning(f"Failed to process metadata file {meta_file}: {e}")
        except Exception as e:
            logger.error(f"Failed to cleanup expired entries: {e}")
        return removed_count
    def _get_cache_file(self, key: str) -> Path:
        """Get cache file path for key."""
        safe_key = self._make_safe_filename(key)
        extension = ".json" if self.serialization == "json" else ".pkl"
        return self.cache_dir / f"{safe_key}{extension}"
    def _get_meta_file(self, key: str) -> Path:
        """Get metadata file path for key."""
        safe_key = self._make_safe_filename(key)
        return self.meta_dir / f"{safe_key}.json"
    def _make_safe_filename(self, key: str) -> str:
        """Convert cache key to safe filename."""
        # Hash long keys to avoid filesystem limits
        if len(key) > 100:
            return hashlib.md5(key.encode()).hexdigest()
        # Replace unsafe characters
        safe_chars = "-_.abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
        return ''.join(c if c in safe_chars else '_' for c in key)
class RedisCacheBackend(CacheBackend):
    """Redis cache backend for production use."""
    def __init__(self, redis_url: str):
        if not REDIS_AVAILABLE:
            raise ImportError("Redis is not available. Install with: pip install redis")
        self.redis_client = redis.from_url(redis_url)
        # Test connection
        try:
            self.redis_client.ping()
        except Exception as e:
            raise ConnectionError(f"Failed to connect to Redis: {e}")
    def get(self, key: str) -> Optional[Any]:
        """Get value from Redis cache."""
        try:
            data = self.redis_client.get(key)
            if data is None:
                return None
            # Deserialize
            return json.loads(data.decode('utf-8'))
        except Exception as e:
            logger.error(f"Failed to get cache key {key}: {e}")
            return None
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set value in Redis cache."""
        try:
            # Serialize
            data = json.dumps(value, default=str)
            # Set with TTL
            if ttl is not None:
                return self.redis_client.setex(key, ttl, data)
            else:
                return self.redis_client.set(key, data)
        except Exception as e:
            logger.error(f"Failed to set cache key {key}: {e}")
            return False
    def delete(self, key: str) -> bool:
        """Delete value from Redis cache."""
        try:
            return bool(self.redis_client.delete(key))
        except Exception as e:
            logger.error(f"Failed to delete cache key {key}: {e}")
            return False
    def exists(self, key: str) -> bool:
        """Check if key exists in Redis cache."""
        try:
            return bool(self.redis_client.exists(key))
        except Exception as e:
            logger.error(f"Failed to check cache key {key}: {e}")
            return False
    def clear(self) -> bool:
        """Clear all cache entries."""
        try:
            return self.redis_client.flushdb()
        except Exception as e:
            logger.error(f"Failed to clear cache: {e}")
            return False
    def cleanup_expired(self) -> int:
        """Redis handles expiration automatically."""
        return 0  # Redis handles this automatically
class CacheManager:
    """Main cache manager that coordinates different cache backends."""
    def __init__(self, config: CacheConfig):
        self.config = config
        self.backend = self._create_backend()
        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0,
        }
    def _create_backend(self) -> CacheBackend:
        """Create appropriate cache backend based on configuration."""
        if self.config.cache_type == "redis":
            if not self.config.redis_url:
                raise ValueError("Redis URL required for Redis cache")
            return RedisCacheBackend(self.config.redis_url)
        elif self.config.cache_type == "file":
            return FileCacheBackend(
                self.config.cache_dir,
                self.config.serialization,
                self.config.compression
            )
        elif self.config.cache_type == "memory":
            return MemoryCacheBackend(self.config.max_cache_size)
        else:
            raise ValueError(f"Unknown cache type: {self.config.cache_type}")
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache."""
        value = self.backend.get(key)
        if value is not None:
            self.stats["hits"] += 1
            logger.debug(f"Cache hit for key: {key}")
        else:
            self.stats["misses"] += 1
            logger.debug(f"Cache miss for key: {key}")
        return value
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set value in cache."""
        if ttl is None:
            ttl = self.config.default_ttl
        success = self.backend.set(key, value, ttl)
        if success:
            self.stats["sets"] += 1
            logger.debug(f"Cache set for key: {key} (TTL: {ttl}s)")
        else:
            logger.warning(f"Failed to set cache key: {key}")
        return success
    def delete(self, key: str) -> bool:
        """Delete value from cache."""
        success = self.backend.delete(key)
        if success:
            self.stats["deletes"] += 1
            logger.debug(f"Cache delete for key: {key}")
        return success
    def exists(self, key: str) -> bool:
        """Check if key exists in cache."""
        return self.backend.exists(key)
    def clear(self) -> bool:
        """Clear all cache entries."""
        success = self.backend.clear()
        if success:
            self.stats = {"hits": 0, "misses": 0, "sets": 0, "deletes": 0}
            logger.info("Cache cleared")
        return success
    def cleanup_expired(self) -> int:
        """Remove expired entries."""
        removed_count = self.backend.cleanup_expired()
        if removed_count > 0:
            logger.info(f"Cleaned up {removed_count} expired cache entries")
        return removed_count
    def get_stats(self) -> Dict[str, Union[int, float]]:
        """Get cache statistics."""
        total_requests = self.stats["hits"] + self.stats["misses"]
        hit_rate = (self.stats["hits"] / total_requests * 100) if total_requests > 0 else 0
        return {
            **self.stats,
            "total_requests": total_requests,
            "hit_rate_percent": round(hit_rate, 2),
        }
    def make_key(self, *parts: str) -> str:
        """Create a cache key from multiple parts."""
        return ":".join(str(part) for part in parts)
    def __enter__(self):
        """Context manager entry."""
        return self
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        self.cleanup_expired()
</file>

<file path="src/kdp_strategist/data/keepa_client.py">
"""Keepa API client for Amazon product data.
Provides access to:
- Product information (BSR, price, reviews, sales)
- Historical data and trends
- Category rankings
- Competitor analysis data
- Market insights
Features:
- Rate limiting and retry logic
- Comprehensive caching
- Error handling and logging
- Data validation and transformation
"""
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
from urllib.parse import urlencode
try:
    import requests
except ImportError:
    raise ImportError("requests library required. Install with: pip install requests")
from .cache_manager import CacheManager
logger = logging.getLogger(__name__)
@dataclass
class KeepaConfig:
    """Configuration for Keepa API client."""
    api_key: str
    base_url: str = "https://api.keepa.com"
    rate_limit_per_minute: int = 100
    max_retries: int = 3
    retry_delay: float = 1.0
    timeout: int = 30
    cache_ttl: int = 3600  # 1 hour
    enable_caching: bool = True
@dataclass
class ProductData:
    """Keepa product data structure."""
    asin: str
    title: str
    brand: Optional[str] = None
    category: Optional[str] = None
    current_price: Optional[float] = None
    avg_price_30d: Optional[float] = None
    avg_price_90d: Optional[float] = None
    bsr_current: Optional[int] = None
    bsr_avg_30d: Optional[float] = None
    bsr_avg_90d: Optional[float] = None
    review_count: Optional[int] = None
    rating: Optional[float] = None
    sales_rank_drops_30d: Optional[int] = None
    sales_rank_drops_90d: Optional[int] = None
    estimated_sales_30d: Optional[int] = None
    estimated_sales_90d: Optional[int] = None
    buy_box_percentage: Optional[float] = None
    fba_fees: Optional[float] = None
    dimensions: Optional[Dict[str, float]] = None
    weight: Optional[float] = None
    package_quantity: Optional[int] = None
    variation_asins: List[str] = field(default_factory=list)
    image_urls: List[str] = field(default_factory=list)
    features: List[str] = field(default_factory=list)
    categories: List[Dict[str, Any]] = field(default_factory=list)
    last_updated: Optional[datetime] = None
    def __post_init__(self):
        """Post-initialization processing."""
        if self.last_updated is None:
            self.last_updated = datetime.now()
    @property
    def is_profitable(self) -> bool:
        """Check if product appears profitable based on basic metrics."""
        if not all([self.current_price, self.bsr_current, self.review_count]):
            return False
        # Basic profitability heuristics
        return (
            self.current_price >= 2.99 and  # Minimum viable price
            self.bsr_current <= 100000 and  # Decent sales rank
            self.review_count >= 10  # Some market validation
        )
    @property
    def competition_level(self) -> str:
        """Assess competition level based on review count and BSR."""
        if not all([self.review_count, self.bsr_current]):
            return "unknown"
        if self.review_count > 1000 or self.bsr_current < 1000:
            return "high"
        elif self.review_count > 100 or self.bsr_current < 10000:
            return "medium"
        else:
            return "low"
    @property
    def estimated_monthly_revenue(self) -> Optional[float]:
        """Estimate monthly revenue based on price and sales."""
        if not all([self.current_price, self.estimated_sales_30d]):
            return None
        return self.current_price * self.estimated_sales_30d
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        data = {
            "asin": self.asin,
            "title": self.title,
            "brand": self.brand,
            "category": self.category,
            "current_price": self.current_price,
            "avg_price_30d": self.avg_price_30d,
            "avg_price_90d": self.avg_price_90d,
            "bsr_current": self.bsr_current,
            "bsr_avg_30d": self.bsr_avg_30d,
            "bsr_avg_90d": self.bsr_avg_90d,
            "review_count": self.review_count,
            "rating": self.rating,
            "sales_rank_drops_30d": self.sales_rank_drops_30d,
            "sales_rank_drops_90d": self.sales_rank_drops_90d,
            "estimated_sales_30d": self.estimated_sales_30d,
            "estimated_sales_90d": self.estimated_sales_90d,
            "buy_box_percentage": self.buy_box_percentage,
            "fba_fees": self.fba_fees,
            "dimensions": self.dimensions,
            "weight": self.weight,
            "package_quantity": self.package_quantity,
            "variation_asins": self.variation_asins,
            "image_urls": self.image_urls,
            "features": self.features,
            "categories": self.categories,
            "last_updated": self.last_updated.isoformat() if self.last_updated else None,
        }
        return data
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ProductData":
        """Create from dictionary."""
        # Handle datetime conversion
        last_updated = data.get("last_updated")
        if last_updated and isinstance(last_updated, str):
            last_updated = datetime.fromisoformat(last_updated)
        return cls(
            asin=data["asin"],
            title=data["title"],
            brand=data.get("brand"),
            category=data.get("category"),
            current_price=data.get("current_price"),
            avg_price_30d=data.get("avg_price_30d"),
            avg_price_90d=data.get("avg_price_90d"),
            bsr_current=data.get("bsr_current"),
            bsr_avg_30d=data.get("bsr_avg_30d"),
            bsr_avg_90d=data.get("bsr_avg_90d"),
            review_count=data.get("review_count"),
            rating=data.get("rating"),
            sales_rank_drops_30d=data.get("sales_rank_drops_30d"),
            sales_rank_drops_90d=data.get("sales_rank_drops_90d"),
            estimated_sales_30d=data.get("estimated_sales_30d"),
            estimated_sales_90d=data.get("estimated_sales_90d"),
            buy_box_percentage=data.get("buy_box_percentage"),
            fba_fees=data.get("fba_fees"),
            dimensions=data.get("dimensions"),
            weight=data.get("weight"),
            package_quantity=data.get("package_quantity"),
            variation_asins=data.get("variation_asins", []),
            image_urls=data.get("image_urls", []),
            features=data.get("features", []),
            categories=data.get("categories", []),
            last_updated=last_updated,
        )
class RateLimiter:
    """Rate limiter for API requests."""
    def __init__(self, max_requests_per_minute: int):
        self.max_requests = max_requests_per_minute
        self.requests = []
        self.lock = False
    def wait_if_needed(self) -> None:
        """Wait if rate limit would be exceeded."""
        now = time.time()
        # Remove requests older than 1 minute
        self.requests = [req_time for req_time in self.requests if now - req_time < 60]
        # Check if we need to wait
        if len(self.requests) >= self.max_requests:
            sleep_time = 60 - (now - self.requests[0]) + 0.1  # Small buffer
            if sleep_time > 0:
                logger.info(f"Rate limit reached, waiting {sleep_time:.1f} seconds")
                time.sleep(sleep_time)
                # Clean up old requests after waiting
                now = time.time()
                self.requests = [req_time for req_time in self.requests if now - req_time < 60]
        # Record this request
        self.requests.append(now)
class KeepaClient:
    """Keepa API client with caching and rate limiting."""
    def __init__(self, config: KeepaConfig, cache_manager: Optional[CacheManager] = None):
        self.config = config
        self.cache_manager = cache_manager
        self.rate_limiter = RateLimiter(config.rate_limit_per_minute)
        self.session = requests.Session()
        # Set default headers
        self.session.headers.update({
            "User-Agent": "KDP_Strategist/1.0",
            "Accept": "application/json",
        })
        logger.info(f"Initialized Keepa client with rate limit: {config.rate_limit_per_minute}/min")
    def get_product(self, asin: str, force_refresh: bool = False) -> Optional[ProductData]:
        """Get product data for a single ASIN."""
        # Check cache first
        if not force_refresh and self.config.enable_caching and self.cache_manager:
            cache_key = self.cache_manager.make_key("keepa", "product", asin)
            cached_data = self.cache_manager.get(cache_key)
            if cached_data:
                logger.debug(f"Cache hit for ASIN: {asin}")
                return ProductData.from_dict(cached_data)
        # Fetch from API
        try:
            self.rate_limiter.wait_if_needed()
            params = {
                "key": self.config.api_key,
                "domain": 1,  # Amazon.com
                "asin": asin,
                "stats": 1,  # Include statistics
                "history": 1,  # Include price history
                "rating": 1,  # Include rating info
            }
            url = f"{self.config.base_url}/product"
            response = self._make_request("GET", url, params=params)
            if not response or "products" not in response:
                logger.warning(f"No product data found for ASIN: {asin}")
                return None
            products = response["products"]
            if not products:
                logger.warning(f"Empty product list for ASIN: {asin}")
                return None
            # Parse product data
            product_data = self._parse_product_data(products[0])
            # Cache the result
            if self.config.enable_caching and self.cache_manager:
                cache_key = self.cache_manager.make_key("keepa", "product", asin)
                self.cache_manager.set(cache_key, product_data.to_dict(), self.config.cache_ttl)
            logger.info(f"Retrieved product data for ASIN: {asin}")
            return product_data
        except Exception as e:
            logger.error(f"Failed to get product data for ASIN {asin}: {e}")
            return None
    def get_products_bulk(self, asins: List[str], force_refresh: bool = False) -> Dict[str, Optional[ProductData]]:
        """Get product data for multiple ASINs efficiently."""
        results = {}
        uncached_asins = []
        # Check cache for each ASIN
        if not force_refresh and self.config.enable_caching and self.cache_manager:
            for asin in asins:
                cache_key = self.cache_manager.make_key("keepa", "product", asin)
                cached_data = self.cache_manager.get(cache_key)
                if cached_data:
                    results[asin] = ProductData.from_dict(cached_data)
                    logger.debug(f"Cache hit for ASIN: {asin}")
                else:
                    uncached_asins.append(asin)
        else:
            uncached_asins = asins
        # Fetch uncached ASINs from API
        if uncached_asins:
            try:
                self.rate_limiter.wait_if_needed()
                params = {
                    "key": self.config.api_key,
                    "domain": 1,  # Amazon.com
                    "asin": ",".join(uncached_asins),
                    "stats": 1,
                    "history": 1,
                    "rating": 1,
                }
                url = f"{self.config.base_url}/product"
                response = self._make_request("GET", url, params=params)
                if response and "products" in response:
                    for product_raw in response["products"]:
                        try:
                            product_data = self._parse_product_data(product_raw)
                            results[product_data.asin] = product_data
                            # Cache the result
                            if self.config.enable_caching and self.cache_manager:
                                cache_key = self.cache_manager.make_key("keepa", "product", product_data.asin)
                                self.cache_manager.set(cache_key, product_data.to_dict(), self.config.cache_ttl)
                        except Exception as e:
                            logger.error(f"Failed to parse product data: {e}")
                            continue
                logger.info(f"Retrieved bulk product data for {len(uncached_asins)} ASINs")
            except Exception as e:
                logger.error(f"Failed to get bulk product data: {e}")
        # Ensure all requested ASINs are in results (with None for failures)
        for asin in asins:
            if asin not in results:
                results[asin] = None
        return results
    def search_products(self, query: str, category: Optional[str] = None, limit: int = 50) -> List[ProductData]:
        """Search for products by keyword."""
        try:
            self.rate_limiter.wait_if_needed()
            params = {
                "key": self.config.api_key,
                "domain": 1,
                "type": "search",
                "term": query,
                "stats": 1,
                "history": 0,  # Don't need full history for search
                "page": 0,
                "perPage": min(limit, 100),  # API limit
            }
            if category:
                params["category"] = category
            url = f"{self.config.base_url}/search"
            response = self._make_request("GET", url, params=params)
            if not response or "asinList" not in response:
                logger.warning(f"No search results for query: {query}")
                return []
            # Get detailed product data for found ASINs
            asins = response["asinList"][:limit]
            if not asins:
                return []
            products_data = self.get_products_bulk(asins)
            results = [product for product in products_data.values() if product is not None]
            logger.info(f"Found {len(results)} products for query: {query}")
            return results
        except Exception as e:
            logger.error(f"Failed to search products for query '{query}': {e}")
            return []
    def get_category_tree(self, category_id: Optional[int] = None) -> Dict[str, Any]:
        """Get Amazon category tree."""
        cache_key = f"keepa:category_tree:{category_id or 'root'}"
        # Check cache
        if self.config.enable_caching and self.cache_manager:
            cached_data = self.cache_manager.get(cache_key)
            if cached_data:
                return cached_data
        try:
            self.rate_limiter.wait_if_needed()
            params = {
                "key": self.config.api_key,
                "domain": 1,
            }
            if category_id:
                params["category"] = category_id
            url = f"{self.config.base_url}/category"
            response = self._make_request("GET", url, params=params)
            if response:
                # Cache for longer since category tree doesn't change often
                if self.config.enable_caching and self.cache_manager:
                    self.cache_manager.set(cache_key, response, 86400)  # 24 hours
                return response
            return {}
        except Exception as e:
            logger.error(f"Failed to get category tree: {e}")
            return {}
    def _make_request(self, method: str, url: str, **kwargs) -> Optional[Dict[str, Any]]:
        """Make HTTP request with retry logic."""
        for attempt in range(self.config.max_retries + 1):
            try:
                response = self.session.request(
                    method=method,
                    url=url,
                    timeout=self.config.timeout,
                    **kwargs
                )
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:  # Rate limited
                    if attempt < self.config.max_retries:
                        wait_time = self.config.retry_delay * (2 ** attempt)
                        logger.warning(f"Rate limited, waiting {wait_time}s before retry {attempt + 1}")
                        time.sleep(wait_time)
                        continue
                elif response.status_code == 400:
                    logger.error(f"Bad request to Keepa API: {response.text}")
                    return None
                elif response.status_code == 401:
                    logger.error("Unauthorized - check Keepa API key")
                    return None
                else:
                    logger.warning(f"Keepa API returned status {response.status_code}: {response.text}")
                    if attempt < self.config.max_retries:
                        time.sleep(self.config.retry_delay)
                        continue
                return None
            except requests.exceptions.Timeout:
                logger.warning(f"Request timeout (attempt {attempt + 1})")
                if attempt < self.config.max_retries:
                    time.sleep(self.config.retry_delay)
                    continue
            except requests.exceptions.RequestException as e:
                logger.error(f"Request failed: {e}")
                if attempt < self.config.max_retries:
                    time.sleep(self.config.retry_delay)
                    continue
        logger.error(f"Failed to make request after {self.config.max_retries + 1} attempts")
        return None
    def _parse_product_data(self, raw_data: Dict[str, Any]) -> ProductData:
        """Parse raw Keepa API response into ProductData object."""
        # Extract basic info
        asin = raw_data.get("asin", "")
        title = raw_data.get("title", "")
        brand = raw_data.get("brand")
        # Extract pricing data
        current_price = None
        avg_price_30d = None
        avg_price_90d = None
        if "stats" in raw_data:
            stats = raw_data["stats"]
            current_price = stats.get("current", {}).get("AMAZON", {}).get("price")
            avg_price_30d = stats.get("avg30", {}).get("AMAZON", {}).get("price")
            avg_price_90d = stats.get("avg90", {}).get("AMAZON", {}).get("price")
        # Convert from Keepa price format (price * 100) to dollars
        if current_price is not None:
            current_price = current_price / 100
        if avg_price_30d is not None:
            avg_price_30d = avg_price_30d / 100
        if avg_price_90d is not None:
            avg_price_90d = avg_price_90d / 100
        # Extract BSR data
        bsr_current = raw_data.get("salesRankCurrent")
        bsr_avg_30d = None
        bsr_avg_90d = None
        if "stats" in raw_data:
            stats = raw_data["stats"]
            bsr_avg_30d = stats.get("avg30", {}).get("salesRank")
            bsr_avg_90d = stats.get("avg90", {}).get("salesRank")
        # Extract review data
        review_count = raw_data.get("reviewCount")
        rating = raw_data.get("rating")
        if rating is not None:
            rating = rating / 10  # Keepa uses rating * 10
        # Extract other metrics
        sales_rank_drops_30d = raw_data.get("salesRankDrops30")
        sales_rank_drops_90d = raw_data.get("salesRankDrops90")
        # Estimate sales (simplified calculation)
        estimated_sales_30d = None
        estimated_sales_90d = None
        if bsr_current and bsr_current > 0:
            # Very rough estimation based on BSR
            estimated_sales_30d = max(1, int(1000000 / bsr_current))
            estimated_sales_90d = estimated_sales_30d * 3
        # Extract additional data
        buy_box_percentage = raw_data.get("buyBoxPercentage")
        fba_fees = raw_data.get("fbaFees")
        if fba_fees is not None:
            fba_fees = fba_fees / 100  # Convert from cents
        # Extract dimensions and weight
        dimensions = None
        if "packageHeight" in raw_data:
            dimensions = {
                "height": raw_data.get("packageHeight"),
                "length": raw_data.get("packageLength"),
                "width": raw_data.get("packageWidth"),
            }
        weight = raw_data.get("packageWeight")
        package_quantity = raw_data.get("packageQuantity")
        # Extract variations and images
        variation_asins = raw_data.get("variationASINs", [])
        image_urls = raw_data.get("imagesCSV", "").split(",") if raw_data.get("imagesCSV") else []
        # Extract features
        features = raw_data.get("features", [])
        # Extract categories
        categories = []
        if "categoryTree" in raw_data:
            for cat in raw_data["categoryTree"]:
                categories.append({
                    "id": cat.get("catId"),
                    "name": cat.get("name"),
                    "parent": cat.get("parent"),
                })
        return ProductData(
            asin=asin,
            title=title,
            brand=brand,
            category=categories[0]["name"] if categories else None,
            current_price=current_price,
            avg_price_30d=avg_price_30d,
            avg_price_90d=avg_price_90d,
            bsr_current=bsr_current,
            bsr_avg_30d=bsr_avg_30d,
            bsr_avg_90d=bsr_avg_90d,
            review_count=review_count,
            rating=rating,
            sales_rank_drops_30d=sales_rank_drops_30d,
            sales_rank_drops_90d=sales_rank_drops_90d,
            estimated_sales_30d=estimated_sales_30d,
            estimated_sales_90d=estimated_sales_90d,
            buy_box_percentage=buy_box_percentage,
            fba_fees=fba_fees,
            dimensions=dimensions,
            weight=weight,
            package_quantity=package_quantity,
            variation_asins=variation_asins,
            image_urls=image_urls,
            features=features,
            categories=categories,
        )
    def get_stats(self) -> Dict[str, Any]:
        """Get client statistics."""
        stats = {
            "rate_limit_per_minute": self.config.rate_limit_per_minute,
            "requests_in_last_minute": len(self.rate_limiter.requests),
            "cache_enabled": self.config.enable_caching,
        }
        if self.cache_manager:
            stats["cache_stats"] = self.cache_manager.get_stats()
        return stats
    def __enter__(self):
        """Context manager entry."""
        return self
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.session.close()
</file>

<file path="src/kdp_strategist/data/trends_client.py">
"""Google Trends client for keyword and trend analysis.
Provides access to:
- Keyword trend data and popularity
- Regional interest analysis
- Related queries and topics
- Seasonal pattern detection
- Trend forecasting and validation
Features:
- Rate limiting and retry logic
- Comprehensive caching
- Error handling and logging
- Data validation and transformation
- Batch processing capabilities
"""
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
try:
    from pytrends.request import TrendReq
    from pytrends.exceptions import TooManyRequestsError, ResponseError
except ImportError:
    raise ImportError("pytrends library required. Install with: pip install pytrends")
import pandas as pd
import numpy as np
from .cache_manager import CacheManager
from ..models.trend_model import TrendAnalysis, TrendDirection, TrendStrength
logger = logging.getLogger(__name__)
@dataclass
class TrendsConfig:
    """Configuration for Google Trends client."""
    language: str = "en-US"
    timezone: int = 360  # UTC offset in minutes
    geo: str = "US"  # Default geographic location
    rate_limit_delay: float = 2.0  # Seconds between requests
    max_retries: int = 3
    retry_delay: float = 5.0
    timeout: int = 30
    cache_ttl: int = 7200  # 2 hours
    enable_caching: bool = True
    batch_size: int = 5  # Max keywords per batch
@dataclass
class TrendData:
    """Raw trend data from Google Trends."""
    keyword: str
    timeframe: str
    geo: str
    interest_over_time: pd.DataFrame = field(default_factory=pd.DataFrame)
    interest_by_region: pd.DataFrame = field(default_factory=pd.DataFrame)
    related_topics: Dict[str, pd.DataFrame] = field(default_factory=dict)
    related_queries: Dict[str, pd.DataFrame] = field(default_factory=dict)
    suggestions: List[str] = field(default_factory=list)
    last_updated: Optional[datetime] = None
    def __post_init__(self):
        """Post-initialization processing."""
        if self.last_updated is None:
            self.last_updated = datetime.now()
class RateLimiter:
    """Rate limiter for Google Trends requests."""
    def __init__(self, delay: float):
        self.delay = delay
        self.last_request = 0.0
    def wait_if_needed(self) -> None:
        """Wait if needed to respect rate limits."""
        now = time.time()
        time_since_last = now - self.last_request
        if time_since_last < self.delay:
            sleep_time = self.delay - time_since_last
            logger.debug(f"Rate limiting: waiting {sleep_time:.1f} seconds")
            time.sleep(sleep_time)
        self.last_request = time.time()
class TrendsClient:
    """Google Trends client with caching and rate limiting."""
    def __init__(self, config: TrendsConfig, cache_manager: Optional[CacheManager] = None):
        self.config = config
        self.cache_manager = cache_manager
        self.rate_limiter = RateLimiter(config.rate_limit_delay)
        # Initialize pytrends
        self.pytrends = TrendReq(
            hl=config.language,
            tz=config.timezone,
            timeout=config.timeout,
            retries=config.max_retries,
            backoff_factor=config.retry_delay
        )
        logger.info(f"Initialized Trends client for geo: {config.geo}")
    def get_trend_analysis(self, keyword: str, timeframe: str = "today 12-m", 
                          geo: Optional[str] = None, force_refresh: bool = False) -> Optional[TrendAnalysis]:
        """Get comprehensive trend analysis for a keyword."""
        geo = geo or self.config.geo
        # Check cache first
        if not force_refresh and self.config.enable_caching and self.cache_manager:
            cache_key = self.cache_manager.make_key("trends", "analysis", keyword, timeframe, geo)
            cached_data = self.cache_manager.get(cache_key)
            if cached_data:
                logger.debug(f"Cache hit for trend analysis: {keyword}")
                return TrendAnalysis.from_dict(cached_data)
        # Get raw trend data
        trend_data = self.get_trend_data(keyword, timeframe, geo, force_refresh)
        if not trend_data:
            return None
        # Analyze the trend data
        analysis = self._analyze_trend_data(trend_data)
        # Cache the result
        if self.config.enable_caching and self.cache_manager:
            cache_key = self.cache_manager.make_key("trends", "analysis", keyword, timeframe, geo)
            self.cache_manager.set(cache_key, analysis.to_dict(), self.config.cache_ttl)
        logger.info(f"Generated trend analysis for keyword: {keyword}")
        return analysis
    def get_trend_data(self, keyword: str, timeframe: str = "today 12-m", 
                      geo: Optional[str] = None, force_refresh: bool = False) -> Optional[TrendData]:
        """Get raw trend data for a keyword."""
        geo = geo or self.config.geo
        # Check cache first
        if not force_refresh and self.config.enable_caching and self.cache_manager:
            cache_key = self.cache_manager.make_key("trends", "raw", keyword, timeframe, geo)
            cached_data = self.cache_manager.get(cache_key)
            if cached_data:
                logger.debug(f"Cache hit for trend data: {keyword}")
                return self._deserialize_trend_data(cached_data)
        # Fetch from Google Trends
        try:
            self.rate_limiter.wait_if_needed()
            # Build payload
            self.pytrends.build_payload(
                kw_list=[keyword],
                cat=0,
                timeframe=timeframe,
                geo=geo,
                gprop=''
            )
            # Get interest over time
            interest_over_time = self.pytrends.interest_over_time()
            if interest_over_time.empty:
                logger.warning(f"No trend data found for keyword: {keyword}")
                return None
            # Get interest by region
            interest_by_region = pd.DataFrame()
            try:
                self.rate_limiter.wait_if_needed()
                interest_by_region = self.pytrends.interest_by_region(resolution='COUNTRY', inc_low_vol=True, inc_geo_code=False)
            except Exception as e:
                logger.warning(f"Failed to get regional interest for {keyword}: {e}")
            # Get related topics
            related_topics = {}
            try:
                self.rate_limiter.wait_if_needed()
                topics = self.pytrends.related_topics()
                if keyword in topics:
                    related_topics = topics[keyword]
            except Exception as e:
                logger.warning(f"Failed to get related topics for {keyword}: {e}")
            # Get related queries
            related_queries = {}
            try:
                self.rate_limiter.wait_if_needed()
                queries = self.pytrends.related_queries()
                if keyword in queries:
                    related_queries = queries[keyword]
            except Exception as e:
                logger.warning(f"Failed to get related queries for {keyword}: {e}")
            # Get suggestions
            suggestions = []
            try:
                self.rate_limiter.wait_if_needed()
                suggestions = self.pytrends.suggestions(keyword=keyword)
                suggestions = [s.get('title', '') for s in suggestions if s.get('title')]
            except Exception as e:
                logger.warning(f"Failed to get suggestions for {keyword}: {e}")
            # Create trend data object
            trend_data = TrendData(
                keyword=keyword,
                timeframe=timeframe,
                geo=geo,
                interest_over_time=interest_over_time,
                interest_by_region=interest_by_region,
                related_topics=related_topics,
                related_queries=related_queries,
                suggestions=suggestions
            )
            # Cache the result
            if self.config.enable_caching and self.cache_manager:
                cache_key = self.cache_manager.make_key("trends", "raw", keyword, timeframe, geo)
                serialized_data = self._serialize_trend_data(trend_data)
                self.cache_manager.set(cache_key, serialized_data, self.config.cache_ttl)
            logger.info(f"Retrieved trend data for keyword: {keyword}")
            return trend_data
        except TooManyRequestsError:
            logger.error(f"Rate limited by Google Trends for keyword: {keyword}")
            return None
        except ResponseError as e:
            logger.error(f"Google Trends API error for keyword {keyword}: {e}")
            return None
        except Exception as e:
            logger.error(f"Failed to get trend data for keyword {keyword}: {e}")
            return None
    def compare_keywords(self, keywords: List[str], timeframe: str = "today 12-m", 
                        geo: Optional[str] = None) -> Optional[pd.DataFrame]:
        """Compare multiple keywords in a single request."""
        if len(keywords) > self.config.batch_size:
            logger.warning(f"Too many keywords ({len(keywords)}), limiting to {self.config.batch_size}")
            keywords = keywords[:self.config.batch_size]
        geo = geo or self.config.geo
        try:
            self.rate_limiter.wait_if_needed()
            # Build payload for comparison
            self.pytrends.build_payload(
                kw_list=keywords,
                cat=0,
                timeframe=timeframe,
                geo=geo,
                gprop=''
            )
            # Get interest over time for comparison
            comparison_data = self.pytrends.interest_over_time()
            if comparison_data.empty:
                logger.warning(f"No comparison data found for keywords: {keywords}")
                return None
            logger.info(f"Retrieved comparison data for {len(keywords)} keywords")
            return comparison_data
        except Exception as e:
            logger.error(f"Failed to compare keywords {keywords}: {e}")
            return None
    def get_trending_searches(self, geo: Optional[str] = None) -> List[str]:
        """Get current trending searches."""
        geo = geo or self.config.geo
        try:
            self.rate_limiter.wait_if_needed()
            trending = self.pytrends.trending_searches(pn=geo)
            if trending.empty:
                return []
            # Extract trending terms
            trending_terms = trending[0].tolist()
            logger.info(f"Retrieved {len(trending_terms)} trending searches for {geo}")
            return trending_terms
        except Exception as e:
            logger.error(f"Failed to get trending searches for {geo}: {e}")
            return []
    def get_top_charts(self, year: int, geo: Optional[str] = None, category: str = "all") -> List[str]:
        """Get top charts for a specific year."""
        geo = geo or self.config.geo
        try:
            self.rate_limiter.wait_if_needed()
            top_charts = self.pytrends.top_charts(year, hl=self.config.language, tz=self.config.timezone, geo=geo)
            if top_charts.empty:
                return []
            # Extract top terms
            top_terms = top_charts['title'].tolist() if 'title' in top_charts.columns else []
            logger.info(f"Retrieved {len(top_terms)} top chart terms for {year}")
            return top_terms
        except Exception as e:
            logger.error(f"Failed to get top charts for {year}: {e}")
            return []
    def _analyze_trend_data(self, trend_data: TrendData) -> TrendAnalysis:
        """Analyze raw trend data and create TrendAnalysis object."""
        interest_data = trend_data.interest_over_time
        keyword = trend_data.keyword
        if interest_data.empty or keyword not in interest_data.columns:
            # Return minimal analysis for empty data
            return TrendAnalysis(
                keyword=keyword,
                trend_score=0,
                direction=TrendDirection.STABLE,
                strength=TrendStrength.WEAK,
                confidence_level=0.0
            )
        # Get the interest values
        values = interest_data[keyword].values
        dates = interest_data.index
        # Calculate trend score (average interest)
        trend_score = float(np.mean(values))
        # Determine trend direction
        if len(values) >= 2:
            # Use linear regression to determine overall direction
            x = np.arange(len(values))
            slope = np.polyfit(x, values, 1)[0]
            if slope > 1:
                direction = TrendDirection.RISING
            elif slope < -1:
                direction = TrendDirection.DECLINING
            else:
                direction = TrendDirection.STABLE
        else:
            direction = TrendDirection.STABLE
        # Determine trend strength
        max_val = float(np.max(values))
        std_val = float(np.std(values))
        if max_val >= 80 and std_val <= 10:
            strength = TrendStrength.VERY_STRONG
        elif max_val >= 60 and std_val <= 20:
            strength = TrendStrength.STRONG
        elif max_val >= 40 and std_val <= 30:
            strength = TrendStrength.MODERATE
        elif max_val >= 20:
            strength = TrendStrength.WEAK
        else:
            strength = TrendStrength.VERY_WEAK
        # Calculate confidence level
        data_points = len(values)
        non_zero_points = np.count_nonzero(values)
        confidence_level = min(1.0, (non_zero_points / max(data_points, 1)) * (max_val / 100))
        # Extract regional interest
        regional_interest = {}
        if not trend_data.interest_by_region.empty:
            region_data = trend_data.interest_by_region[keyword] if keyword in trend_data.interest_by_region.columns else pd.Series()
            regional_interest = region_data.to_dict()
        # Extract related queries
        related_queries = []
        if trend_data.related_queries:
            for query_type, queries_df in trend_data.related_queries.items():
                if not queries_df.empty and 'query' in queries_df.columns:
                    related_queries.extend(queries_df['query'].tolist()[:10])  # Top 10
        # Detect seasonal patterns
        seasonal_patterns = self._detect_seasonal_patterns(values, dates)
        # Generate 6-month forecast
        forecast_6m = self._generate_forecast(values, 6)
        return TrendAnalysis(
            keyword=keyword,
            trend_score=trend_score,
            direction=direction,
            strength=strength,
            regional_interest=regional_interest,
            related_queries=related_queries,
            seasonal_patterns=seasonal_patterns,
            forecast_6m=forecast_6m,
            confidence_level=confidence_level,
            data_points=data_points,
            timeframe=trend_data.timeframe,
            geo=trend_data.geo,
            last_updated=trend_data.last_updated
        )
    def _detect_seasonal_patterns(self, values: np.ndarray, dates: pd.DatetimeIndex) -> Dict[str, Any]:
        """Detect seasonal patterns in trend data."""
        if len(values) < 12:  # Need at least a year of data
            return {}
        try:
            # Group by month to detect seasonal patterns
            monthly_avg = {}
            for i, date in enumerate(dates):
                month = date.month
                if month not in monthly_avg:
                    monthly_avg[month] = []
                monthly_avg[month].append(values[i])
            # Calculate average for each month
            monthly_patterns = {}
            for month, vals in monthly_avg.items():
                monthly_patterns[month] = float(np.mean(vals))
            # Find peak and low seasons
            if monthly_patterns:
                peak_month = max(monthly_patterns.keys(), key=lambda k: monthly_patterns[k])
                low_month = min(monthly_patterns.keys(), key=lambda k: monthly_patterns[k])
                return {
                    "monthly_averages": monthly_patterns,
                    "peak_month": peak_month,
                    "low_month": low_month,
                    "seasonality_strength": float(np.std(list(monthly_patterns.values())))
                }
        except Exception as e:
            logger.warning(f"Failed to detect seasonal patterns: {e}")
        return {}
    def _generate_forecast(self, values: np.ndarray, months: int) -> List[float]:
        """Generate simple forecast for the next N months."""
        if len(values) < 3:
            return []
        try:
            # Simple linear extrapolation
            x = np.arange(len(values))
            coeffs = np.polyfit(x, values, min(2, len(values) - 1))  # Linear or quadratic
            # Generate forecast points
            forecast = []
            for i in range(months):
                future_x = len(values) + i
                if len(coeffs) == 2:  # Linear
                    predicted = coeffs[0] * future_x + coeffs[1]
                else:  # Quadratic
                    predicted = coeffs[0] * future_x**2 + coeffs[1] * future_x + coeffs[2]
                # Ensure forecast is within reasonable bounds
                predicted = max(0, min(100, predicted))
                forecast.append(float(predicted))
            return forecast
        except Exception as e:
            logger.warning(f"Failed to generate forecast: {e}")
            return []
    def _serialize_trend_data(self, trend_data: TrendData) -> Dict[str, Any]:
        """Serialize TrendData for caching."""
        return {
            "keyword": trend_data.keyword,
            "timeframe": trend_data.timeframe,
            "geo": trend_data.geo,
            "interest_over_time": trend_data.interest_over_time.to_json() if not trend_data.interest_over_time.empty else "",
            "interest_by_region": trend_data.interest_by_region.to_json() if not trend_data.interest_by_region.empty else "",
            "related_topics": {k: v.to_json() if not v.empty else "" for k, v in trend_data.related_topics.items()},
            "related_queries": {k: v.to_json() if not v.empty else "" for k, v in trend_data.related_queries.items()},
            "suggestions": trend_data.suggestions,
            "last_updated": trend_data.last_updated.isoformat() if trend_data.last_updated else None,
        }
    def _deserialize_trend_data(self, data: Dict[str, Any]) -> TrendData:
        """Deserialize TrendData from cache."""
        # Deserialize DataFrames
        interest_over_time = pd.read_json(data["interest_over_time"]) if data["interest_over_time"] else pd.DataFrame()
        interest_by_region = pd.read_json(data["interest_by_region"]) if data["interest_by_region"] else pd.DataFrame()
        related_topics = {}
        for k, v in data.get("related_topics", {}).items():
            related_topics[k] = pd.read_json(v) if v else pd.DataFrame()
        related_queries = {}
        for k, v in data.get("related_queries", {}).items():
            related_queries[k] = pd.read_json(v) if v else pd.DataFrame()
        # Handle datetime
        last_updated = None
        if data.get("last_updated"):
            last_updated = datetime.fromisoformat(data["last_updated"])
        return TrendData(
            keyword=data["keyword"],
            timeframe=data["timeframe"],
            geo=data["geo"],
            interest_over_time=interest_over_time,
            interest_by_region=interest_by_region,
            related_topics=related_topics,
            related_queries=related_queries,
            suggestions=data.get("suggestions", []),
            last_updated=last_updated
        )
    def get_stats(self) -> Dict[str, Any]:
        """Get client statistics."""
        stats = {
            "rate_limit_delay": self.config.rate_limit_delay,
            "cache_enabled": self.config.enable_caching,
            "geo": self.config.geo,
            "language": self.config.language,
        }
        if self.cache_manager:
            stats["cache_stats"] = self.cache_manager.get_stats()
        return stats
    def __enter__(self):
        """Context manager entry."""
        return self
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        pass
</file>

<file path="src/kdp_strategist/main.py">
#!/usr/bin/env python3
"""Main entry point for the KDP Strategist AI Agent.
This module provides the main entry point for running the KDP Strategist
MCP (Model Context Protocol) agent. It handles:
- Agent initialization and configuration
- MCP server setup and connection
- Tool registration and management
- Error handling and logging
- Graceful shutdown procedures
Usage:
    python -m kdp_strategist.main
Or as a console script (after installation):
    kdp-_trategist
"""
import asyncio
import logging
import signal
import sys
from pathlib import Path
from typing import Optional
import argparse
# Add the src directory to the Python path
src_dir = Path(__file__).parent.parent
sys.path.insert(0, str(src_dir))
from config.settings import Settings, load_settings
from agent.kdp_strategist_agent import KDPStrategistAgent
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('kdp_strategist.log')
    ]
)
logger = logging.getLogger(__name__)
class KDPStrategistServer:
    """Main server class for the KDP Strategist MCP agent."""
    def __init__(self, settings: Settings):
        """Initialize the server with configuration settings."""
        self.settings = settings
        self.agent: Optional[KDPStrategistAgent] = None
        self.running = False
        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals gracefully."""
        logger.info(f"Received signal {signum}, initiating graceful shutdown...")
        self.running = False
    async def start(self) -> None:
        """Start the KDP Strategist MCP agent server."""
        try:
            logger.info("Starting KDP Strategist AI Agent...")
            # Initialize the agent
            self.agent = KDPStrategistAgent(self.settings)
            await self.agent.initialize()
            logger.info("KDP Strategist Agent initialized successfully")
            logger.info(f"Available tools: {', '.join(await self.agent.list_tools())}")
            # Start the main server loop
            self.running = True
            await self._run_server_loop()
        except Exception as e:
            logger.error(f"Failed to start KDP Strategist Agent: {e}")
            raise
        finally:
            await self._cleanup()
    async def _run_server_loop(self) -> None:
        """Main server loop for handling MCP requests."""
        logger.info("KDP Strategist Agent is ready and waiting for requests...")
        logger.info("Press Ctrl+C to stop the agent")
        try:
            while self.running:
                # In a real MCP implementation, this would handle incoming requests
                # For now, we'll just keep the agent alive and ready
                await asyncio.sleep(1)
                # Optionally, perform periodic maintenance tasks
                if hasattr(self.agent, 'perform_maintenance'):
                    await self.agent.perform_maintenance()
        except asyncio.CancelledError:
            logger.info("Server loop cancelled")
        except Exception as e:
            logger.error(f"Error in server loop: {e}")
            raise
    async def _cleanup(self) -> None:
        """Cleanup resources before shutdown."""
        logger.info("Cleaning up resources...")
        if self.agent:
            try:
                await self.agent.cleanup()
                logger.info("Agent cleanup completed")
            except Exception as e:
                logger.error(f"Error during agent cleanup: {e}")
        logger.info("KDP Strategist Agent shutdown complete")
async def run_interactive_mode(agent: KDPStrategistAgent) -> None:
    """Run the agent in interactive mode for testing and development."""
    logger.info("Starting interactive mode...")
    logger.info("Type 'help' for available commands, 'quit' to exit")
    available_tools = await agent.list_tools()
    while True:
        try:
            user_input = input("\nKDP Strategist> ").strip()
            if user_input.lower() in ['quit', 'exit', 'q']:
                break
            elif user_input.lower() == 'help':
                print("\nAvailable commands:")
                print("  help - Show this help message")
                print("  tools - List available tools")
                print("  test <tool_name> - Test a specific tool")
                print("  quit - Exit interactive mode")
                print("\nAvailable tools:")
                for tool in available_tools:
                    print(f"  - {tool}")
            elif user_input.lower() == 'tools':
                print("\nAvailable tools:")
                for tool in available_tools:
                    print(f"  - {tool}")
            elif user_input.lower().startswith('test '):
                tool_name = user_input[5:].strip()
                if tool_name in available_tools:
                    await _test_tool(agent, tool_name)
                else:
                    print(f"Unknown tool: {tool_name}")
            elif user_input:
                print("Unknown command. Type 'help' for available commands.")
        except KeyboardInterrupt:
            break
        except EOFError:
            break
        except Exception as e:
            logger.error(f"Error in interactive mode: {e}")
            print(f"Error: {e}")
    logger.info("Exiting interactive mode")
async def _test_tool(agent: KDPStrategistAgent, tool_name: str) -> None:
    """Test a specific tool with sample data."""
    print(f"\nTesting tool: {tool_name}")
    try:
        # Sample test parameters for each tool
        test_params = {
            "find_profitable_niches": {
                "base_keywords": ["cooking", "fitness"],
                "max_niches": 3,
                "min_profitability": 60
            },
            "analyze_competitor_asin": {
                "asin": "B08EXAMPLE",
                "include_market_analysis": True
            },
            "generate_kdp_listing": {
                "niche_keyword": "healthy cooking",
                "target_audience": "health-conscious adults",
                "book_type": "cookbook"
            },
            "validate_trend": {
                "keyword": "meal prep",
                "timeframe": "today 12-m",
                "include_forecasts": True
            },
            "niche_stress_test": {
                "niche_keyword": "keto recipes",
                "include_all_scenarios": True
            }
        }
        if tool_name in test_params:
            print(f"Running with test parameters: {test_params[tool_name]}")
            result = await agent.call_tool(tool_name, test_params[tool_name])
            if "error" in result:
                print(f"Tool returned error: {result['error']}")
            else:
                print("Tool executed successfully!")
                # Print a summary of the result
                if tool_name == "find_profitable_niches":
                    niches = result.get("discovered_niches", {})
                    print(f"Found {len(niches)} profitable niches")
                elif tool_name == "analyze_competitor_asin":
                    metrics = result.get("competitor_metrics", {})
                    print(f"Competitor analysis completed. Sales rank: {metrics.get('sales_rank', 'N/A')}")
                elif tool_name == "generate_kdp_listing":
                    listing = result.get("generated_listing", {})
                    print(f"Generated listing with title: {listing.get('title', 'N/A')[:50]}...")
                elif tool_name == "validate_trend":
                    validation = result.get("validation_result", {})
                    print(f"Trend validation: {validation.get('is_valid', False)}")
                elif tool_name == "niche_stress_test":
                    summary = result.get("stress_test_summary", {})
                    print(f"Stress test completed. Overall resilience: {summary.get('overall_resilience', 'N/A')}")
        else:
            print(f"No test parameters defined for tool: {tool_name}")
    except Exception as e:
        logger.error(f"Error testing tool {tool_name}: {e}")
        print(f"Error testing tool: {e}")
def parse_arguments() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="KDP Strategist AI Agent - MCP Server",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s                    # Start the MCP server
  %(prog)s --interactive      # Run in interactive mode
  %(prog)s --config custom.env # Use custom config file
  %(prog)s --log-level DEBUG  # Enable debug logging
"""
    )
    parser.add_argument(
        "--interactive", "-i",
        action="store_true",
        help="Run in interactive mode for testing"
    )
    parser.add_argument(
        "--config", "-c",
        type=str,
        help="Path to configuration file (default: .env)"
    )
    parser.add_argument(
        "--log-level", "-l",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        default="INFO",
        help="Set the logging level (default: INFO)"
    )
    parser.add_argument(
        "--version", "-v",
        action="version",
        version="KDP Strategist AI Agent v1.0.0"
    )
    return parser.parse_args()
async def main() -> None:
    """Main entry point for the KDP Strategist AI Agent."""
    args = parse_arguments()
    # Set logging level
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    try:
        # Load configuration
        logger.info("Loading configuration...")
        settings = load_settings(config_file=args.config)
        # Validate critical settings
        if not settings.keepa.api_key and not settings.development_mode:
            logger.warning("Keepa API key not configured. Some features may not work.")
        logger.info(f"Configuration loaded. Development mode: {settings.development_mode}")
        if args.interactive:
            # Run in interactive mode
            logger.info("Starting in interactive mode...")
            agent = KDPStrategistAgent(settings)
            await agent.initialize()
            try:
                await run_interactive_mode(agent)
            finally:
                await agent.cleanup()
        else:
            # Run as MCP server
            server = KDPStrategistServer(settings)
            await server.start()
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt, shutting down...")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)
if __name__ == "__main__":
    # Handle Windows event loop policy
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Application interrupted by user")
    except Exception as e:
        logger.error(f"Application failed: {e}")
        sys.exit(1)
</file>

<file path="src/kdp_strategist/models/__init__.py">
"""Data models for KDP Strategist AI Agent.
This module contains all data structures used throughout the application:
- Niche: Represents a market niche with scoring and metadata
- KDPListing: Represents an optimized book listing for KDP
- TrendAnalysis: Represents trend analysis results and forecasts
All models include validation, serialization, and utility methods.
"""
from .niche_model import Niche, NicheCategory
from .listing_model import KDPListing, ListingCategory
from .trend_model import TrendAnalysis, TrendDirection, SeasonalPattern
__all__ = [
    "Niche",
    "NicheCategory",
    "KDPListing",
    "ListingCategory",
    "TrendAnalysis",
    "TrendDirection",
    "SeasonalPattern",
]
</file>

<file path="src/kdp_strategist/models/listing_model.py">
"""KDP Listing data model for KDP Strategist.
Defines the KDPListing class and related data structures for representing
optimized book listings with titles, descriptions, keywords, and metadata.
"""
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Tuple
from enum import Enum
from datetime import datetime
import json
import re
class ListingCategory(Enum):
    """KDP category classifications for book listings."""
    KINDLE_EBOOKS = "Kindle eBooks"
    PAPERBACK = "Books"
    HARDCOVER = "Books"
    AUDIOBOOK = "Audible Audiobooks"
class ContentType(Enum):
    """Types of content for KDP listings."""
    FICTION = "Fiction"
    NON_FICTION = "Non-Fiction"
    CHILDREN = "Children's"
    EDUCATIONAL = "Educational"
    REFERENCE = "Reference"
    POETRY = "Poetry"
    DRAMA = "Drama"
@dataclass
class KDPListing:
    """Represents an optimized book listing for Amazon KDP.
    This class encapsulates all elements needed for a successful KDP listing,
    including title optimization, keyword strategy, and content planning.
    Attributes:
        title: Main book title (optimized for search)
        subtitle: Optional subtitle for additional context
        description: Book description/blurb for the product page
        keywords: List of 7 keyword phrases for KDP backend
        categories: Primary and secondary category selections
        target_audience: Description of intended readers
        unique_selling_points: Key differentiators from competitors
        estimated_page_count: Estimated number of pages
        suggested_price: Recommended pricing
        content_outline: High-level content structure
        content_type: Type of content (fiction, non-fiction, etc.)
        language: Primary language of the content
        series_info: Information if part of a series
        author_bio: Suggested author biography
        marketing_hooks: Key marketing messages
        competitive_advantages: Advantages over similar books
    """
    # Core listing elements
    title: str
    subtitle: str = ""
    description: str = ""
    # KDP-specific fields
    keywords: List[str] = field(default_factory=list)  # Max 7 keywords
    categories: List[str] = field(default_factory=list)  # Primary + secondary
    # Target market
    target_audience: str = ""
    unique_selling_points: List[str] = field(default_factory=list)
    # Content planning
    estimated_page_count: int = 0
    suggested_price: float = 9.99
    content_outline: List[str] = field(default_factory=list)
    content_type: str = ContentType.NON_FICTION.value
    language: str = "English"
    # Series and branding
    series_info: Optional[Dict[str, Any]] = None
    author_bio: str = ""
    # Marketing elements
    marketing_hooks: List[str] = field(default_factory=list)
    competitive_advantages: List[str] = field(default_factory=list)
    # Metadata
    created_date: datetime = field(default_factory=datetime.now)
    last_updated: datetime = field(default_factory=datetime.now)
    optimization_score: float = 0.0
    additional_data: Dict[str, Any] = field(default_factory=dict)
    def __post_init__(self):
        """Validate data after initialization."""
        self._validate_title()
        self._validate_keywords()
        self._validate_categories()
        self._validate_price()
        self._validate_description()
        self.last_updated = datetime.now()
        self.optimization_score = self._calculate_optimization_score()
    def _validate_title(self) -> None:
        """Validate title meets KDP requirements."""
        if not self.title or not self.title.strip():
            raise ValueError("Title cannot be empty")
        if len(self.title) > 200:
            raise ValueError("Title cannot exceed 200 characters")
        # Check for prohibited characters or patterns
        prohibited_patterns = [
            r'\b(free|\$0\.00)\b',  # Price references
            r'\b(bestseller|#1)\b',  # Ranking claims
            r'[<>{}\[\]]',  # Special characters
        ]
        for pattern in prohibited_patterns:
            if re.search(pattern, self.title, re.IGNORECASE):
                raise ValueError(f"Title contains prohibited content: {pattern}")
    def _validate_keywords(self) -> None:
        """Validate keywords meet KDP requirements."""
        if len(self.keywords) > 7:
            raise ValueError("Cannot have more than 7 keywords")
        for keyword in self.keywords:
            if not isinstance(keyword, str) or not keyword.strip():
                raise ValueError("All keywords must be non-empty strings")
            if len(keyword) > 50:
                raise ValueError(f"Keyword too long (max 50 chars): {keyword}")
    def _validate_categories(self) -> None:
        """Validate category selections."""
        if len(self.categories) > 2:
            raise ValueError("Cannot select more than 2 categories")
        if len(self.categories) == 0:
            raise ValueError("Must select at least one category")
    def _validate_price(self) -> None:
        """Validate pricing meets KDP requirements."""
        if self.suggested_price < 0.99:
            raise ValueError("Price cannot be less than $0.99")
        if self.suggested_price > 200.00:
            raise ValueError("Price cannot exceed $200.00")
    def _validate_description(self) -> None:
        """Validate description meets KDP requirements."""
        if len(self.description) > 4000:
            raise ValueError("Description cannot exceed 4000 characters")
    def _calculate_optimization_score(self) -> float:
        """Calculate optimization score based on listing completeness and quality.
        Returns:
            Score from 0-100 indicating listing optimization level
        """
        score = 0.0
        max_score = 100.0
        # Title optimization (20 points)
        if self.title:
            score += 10
            if len(self.title) >= 30:  # Good length for SEO
                score += 5
            if any(keyword.lower() in self.title.lower() for keyword in self.keywords):
                score += 5
        # Subtitle (10 points)
        if self.subtitle and len(self.subtitle) >= 20:
            score += 10
        # Description quality (25 points)
        if self.description:
            score += 10
            if len(self.description) >= 500:  # Substantial description
                score += 10
            if len(self.description.split()) >= 100:  # Good word count
                score += 5
        # Keywords (20 points)
        keyword_score = (len(self.keywords) / 7) * 20
        score += keyword_score
        # Categories (10 points)
        if len(self.categories) >= 1:
            score += 5
        if len(self.categories) == 2:
            score += 5
        # Content planning (10 points)
        if self.content_outline:
            score += 5
        if self.estimated_page_count > 0:
            score += 5
        # Marketing elements (5 points)
        if self.unique_selling_points:
            score += 2.5
        if self.marketing_hooks:
            score += 2.5
        return round(min(score, max_score), 1)
    @property
    def is_optimized(self) -> bool:
        """Check if listing meets optimization criteria.
        Returns:
            True if listing is well-optimized
        """
        return self.optimization_score >= 80.0
    @property
    def character_counts(self) -> Dict[str, int]:
        """Get character counts for various fields.
        Returns:
            Dictionary with character counts for title, subtitle, description
        """
        return {
            "title": len(self.title),
            "subtitle": len(self.subtitle),
            "description": len(self.description),
            "title_remaining": 200 - len(self.title),
            "description_remaining": 4000 - len(self.description),
        }
    @property
    def seo_strength(self) -> str:
        """Assess SEO strength of the listing.
        Returns:
            SEO strength: 'weak', 'moderate', or 'strong'
        """
        keyword_in_title = any(keyword.lower() in self.title.lower() for keyword in self.keywords)
        has_good_keywords = len(self.keywords) >= 5
        has_substantial_description = len(self.description) >= 500
        strong_factors = sum([keyword_in_title, has_good_keywords, has_substantial_description])
        if strong_factors >= 3:
            return "strong"
        elif strong_factors >= 2:
            return "moderate"
        else:
            return "weak"
    def add_keyword(self, keyword: str) -> bool:
        """Add a keyword to the listing.
        Args:
            keyword: Keyword phrase to add
        Returns:
            True if keyword was added, False if limit reached
        """
        if len(self.keywords) >= 7:
            return False
        if keyword and keyword not in self.keywords:
            self.keywords.append(keyword.strip())
            self.last_updated = datetime.now()
            self.optimization_score = self._calculate_optimization_score()
            return True
        return False
    def add_category(self, category: str) -> bool:
        """Add a category to the listing.
        Args:
            category: Category to add
        Returns:
            True if category was added, False if limit reached
        """
        if len(self.categories) >= 2:
            return False
        if category and category not in self.categories:
            self.categories.append(category)
            self.last_updated = datetime.now()
            return True
        return False
    def add_unique_selling_point(self, usp: str) -> None:
        """Add a unique selling point.
        Args:
            usp: Unique selling point to add
        """
        if usp and usp not in self.unique_selling_points:
            self.unique_selling_points.append(usp)
            self.last_updated = datetime.now()
    def add_marketing_hook(self, hook: str) -> None:
        """Add a marketing hook.
        Args:
            hook: Marketing hook to add
        """
        if hook and hook not in self.marketing_hooks:
            self.marketing_hooks.append(hook)
            self.last_updated = datetime.now()
    def set_series_info(self, series_name: str, book_number: int, total_books: Optional[int] = None) -> None:
        """Set series information for the listing.
        Args:
            series_name: Name of the book series
            book_number: Position in the series
            total_books: Total planned books in series (optional)
        """
        self.series_info = {
            "series_name": series_name,
            "book_number": book_number,
            "total_books": total_books,
        }
        self.last_updated = datetime.now()
    def generate_full_title(self) -> str:
        """Generate the full title including subtitle.
        Returns:
            Complete title with subtitle if present
        """
        if self.subtitle:
            return f"{self.title}: {self.subtitle}"
        return self.title
    def to_dict(self) -> Dict[str, Any]:
        """Convert listing to dictionary for serialization.
        Returns:
            Dictionary representation of the listing
        """
        return {
            "title": self.title,
            "subtitle": self.subtitle,
            "description": self.description,
            "keywords": self.keywords,
            "categories": self.categories,
            "target_audience": self.target_audience,
            "unique_selling_points": self.unique_selling_points,
            "estimated_page_count": self.estimated_page_count,
            "suggested_price": self.suggested_price,
            "content_outline": self.content_outline,
            "content_type": self.content_type,
            "language": self.language,
            "series_info": self.series_info,
            "author_bio": self.author_bio,
            "marketing_hooks": self.marketing_hooks,
            "competitive_advantages": self.competitive_advantages,
            "created_date": self.created_date.isoformat(),
            "last_updated": self.last_updated.isoformat(),
            "optimization_score": self.optimization_score,
            "additional_data": self.additional_data,
            "is_optimized": self.is_optimized,
            "character_counts": self.character_counts,
            "seo_strength": self.seo_strength,
            "full_title": self.generate_full_title(),
        }
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "KDPListing":
        """Create listing from dictionary.
        Args:
            data: Dictionary containing listing data
        Returns:
            KDPListing instance
        """
        # Handle datetime conversion
        for date_field in ["created_date", "last_updated"]:
            if date_field in data and isinstance(data[date_field], str):
                data[date_field] = datetime.fromisoformat(data[date_field])
        # Remove computed properties
        computed_fields = {
            "is_optimized", "character_counts", "seo_strength", "full_title"
        }
        filtered_data = {k: v for k, v in data.items() if k not in computed_fields}
        return cls(**filtered_data)
    def to_json(self) -> str:
        """Convert listing to JSON string.
        Returns:
            JSON representation of the listing
        """
        return json.dumps(self.to_dict(), indent=2)
    @classmethod
    def from_json(cls, json_str: str) -> "KDPListing":
        """Create listing from JSON string.
        Args:
            json_str: JSON string containing listing data
        Returns:
            KDPListing instance
        """
        data = json.loads(json_str)
        return cls.from_dict(data)
    def __str__(self) -> str:
        """String representation of the listing."""
        return f"KDPListing(title='{self.title}', optimization_score={self.optimization_score})"
    def __repr__(self) -> str:
        """Detailed string representation of the listing."""
        return (
            f"KDPListing(title='{self.title}', keywords={len(self.keywords)}, "
            f"categories={len(self.categories)}, price=${self.suggested_price}, "
            f"optimization_score={self.optimization_score})"
        )
</file>

<file path="src/kdp_strategist/models/niche_model.py">
"""Niche data model for KDP Strategist.
Defines the Niche class and related data structures for representing
market niches with scoring, competition analysis, and profitability metrics.
"""
from dataclasses import dataclass, field
from typing import List, Tuple, Dict, Optional, Any
from enum import Enum
from datetime import datetime
import json
class NicheCategory(Enum):
    """Enumeration of major KDP categories."""
    BUSINESS = "Business & Money"
    SELF_HELP = "Self-Help"
    HEALTH_FITNESS = "Health, Fitness & Dieting"
    COOKING = "Cookbooks, Food & Wine"
    CRAFTS_HOBBIES = "Crafts, Hobbies & Home"
    PARENTING = "Parenting & Relationships"
    EDUCATION = "Education & Teaching"
    TECHNOLOGY = "Computers & Technology"
    TRAVEL = "Travel"
    FICTION = "Literature & Fiction"
    ROMANCE = "Romance"
    MYSTERY = "Mystery, Thriller & Suspense"
    FANTASY = "Science Fiction & Fantasy"
    CHILDREN = "Children's Books"
    YOUNG_ADULT = "Teen & Young Adult"
    RELIGION = "Religion & Spirituality"
    HISTORY = "History"
    BIOGRAPHY = "Biographies & Memoirs"
    POLITICS = "Politics & Social Sciences"
    ARTS = "Arts & Photography"
@dataclass
class Niche:
    """Represents a market niche with comprehensive analysis data.
    This class encapsulates all information about a potential publishing niche,
    including market metrics, competition analysis, and profitability scoring.
    Attributes:
        category: Primary category for the niche
        subcategory: More specific subcategory
        keywords: List of relevant keywords for the niche
        competition_score: Competition intensity (0-100, lower is better)
        profitability_score: Profit potential (0-100, higher is better)
        trend_direction: Overall trend direction
        estimated_monthly_searches: Estimated search volume per month
        top_competitors: List of top competitor ASINs
        recommended_price_range: Suggested pricing range (min, max)
        content_gaps: Identified gaps in existing content
        analysis_date: When this analysis was performed
        confidence_score: Confidence in the analysis (0-100)
        market_size_score: Overall market size assessment (0-100)
        seasonal_factors: Seasonal considerations for the niche
    """
    # Core identification
    category: str
    subcategory: str
    keywords: List[str]
    # Scoring metrics (0-100 scale)
    competition_score: float
    profitability_score: float
    confidence_score: float = 0.0
    market_size_score: float = 0.0
    # Trend analysis
    trend_direction: str = "stable"  # 'rising', 'stable', 'declining'
    estimated_monthly_searches: int = 0
    # Competition data
    top_competitors: List[str] = field(default_factory=list)  # ASINs
    recommended_price_range: Tuple[float, float] = (9.99, 19.99)
    # Content analysis
    content_gaps: List[str] = field(default_factory=list)
    seasonal_factors: Dict[str, float] = field(default_factory=dict)
    # Metadata
    analysis_date: datetime = field(default_factory=datetime.now)
    additional_data: Dict[str, Any] = field(default_factory=dict)
    def __post_init__(self):
        """Validate data after initialization."""
        self._validate_scores()
        self._validate_keywords()
        self._validate_price_range()
        self._validate_trend_direction()
    def _validate_scores(self) -> None:
        """Validate that all scores are within valid ranges."""
        scores = {
            "competition_score": self.competition_score,
            "profitability_score": self.profitability_score,
            "confidence_score": self.confidence_score,
            "market_size_score": self.market_size_score,
        }
        for score_name, score_value in scores.items():
            if not 0 <= score_value <= 100:
                raise ValueError(f"{score_name} must be between 0 and 100, got {score_value}")
    def _validate_keywords(self) -> None:
        """Validate keywords list."""
        if not self.keywords:
            raise ValueError("Keywords list cannot be empty")
        if len(self.keywords) > 50:
            raise ValueError("Too many keywords (max 50)")
        for keyword in self.keywords:
            if not isinstance(keyword, str) or not keyword.strip():
                raise ValueError("All keywords must be non-empty strings")
    def _validate_price_range(self) -> None:
        """Validate price range."""
        min_price, max_price = self.recommended_price_range
        if min_price <= 0 or max_price <= 0:
            raise ValueError("Prices must be positive")
        if min_price >= max_price:
            raise ValueError("Minimum price must be less than maximum price")
    def _validate_trend_direction(self) -> None:
        """Validate trend direction."""
        valid_directions = {"rising", "stable", "declining"}
        if self.trend_direction not in valid_directions:
            raise ValueError(f"Trend direction must be one of {valid_directions}")
    @property
    def overall_score(self) -> float:
        """Calculate overall niche score based on weighted metrics.
        Returns:
            Weighted score combining profitability, competition, and market size
        """
        # Weights for different factors
        profitability_weight = 0.4
        competition_weight = 0.3  # Lower competition is better, so invert
        market_size_weight = 0.2
        confidence_weight = 0.1
        # Invert competition score (lower competition = higher score)
        adjusted_competition = 100 - self.competition_score
        weighted_score = (
            self.profitability_score * profitability_weight +
            adjusted_competition * competition_weight +
            self.market_size_score * market_size_weight +
            self.confidence_score * confidence_weight
        )
        return round(weighted_score, 2)
    @property
    def is_profitable(self) -> bool:
        """Check if niche meets profitability criteria.
        Returns:
            True if niche is considered profitable
        """
        return (
            self.profitability_score >= 50 and
            self.competition_score <= 70 and
            self.confidence_score >= 60
        )
    @property
    def risk_level(self) -> str:
        """Assess risk level for this niche.
        Returns:
            Risk level: 'low', 'medium', or 'high'
        """
        if self.competition_score <= 30 and self.confidence_score >= 80:
            return "low"
        elif self.competition_score <= 60 and self.confidence_score >= 60:
            return "medium"
        else:
            return "high"
    def get_primary_keywords(self, limit: int = 5) -> List[str]:
        """Get the most important keywords for this niche.
        Args:
            limit: Maximum number of keywords to return
        Returns:
            List of primary keywords
        """
        return self.keywords[:limit]
    def add_competitor(self, asin: str) -> None:
        """Add a competitor ASIN to the analysis.
        Args:
            asin: Amazon Standard Identification Number
        """
        if asin and asin not in self.top_competitors:
            self.top_competitors.append(asin)
    def add_content_gap(self, gap: str) -> None:
        """Add an identified content gap.
        Args:
            gap: Description of the content gap
        """
        if gap and gap not in self.content_gaps:
            self.content_gaps.append(gap)
    def set_seasonal_factor(self, month: str, factor: float) -> None:
        """Set seasonal factor for a specific month.
        Args:
            month: Month name (e.g., 'January', 'February')
            factor: Seasonal multiplier (1.0 = normal, >1.0 = higher demand)
        """
        if not 0 <= factor <= 5.0:
            raise ValueError("Seasonal factor must be between 0 and 5.0")
        self.seasonal_factors[month] = factor
    def to_dict(self) -> Dict[str, Any]:
        """Convert niche to dictionary for serialization.
        Returns:
            Dictionary representation of the niche
        """
        return {
            "category": self.category,
            "subcategory": self.subcategory,
            "keywords": self.keywords,
            "competition_score": self.competition_score,
            "profitability_score": self.profitability_score,
            "confidence_score": self.confidence_score,
            "market_size_score": self.market_size_score,
            "trend_direction": self.trend_direction,
            "estimated_monthly_searches": self.estimated_monthly_searches,
            "top_competitors": self.top_competitors,
            "recommended_price_range": list(self.recommended_price_range),
            "content_gaps": self.content_gaps,
            "seasonal_factors": self.seasonal_factors,
            "analysis_date": self.analysis_date.isoformat(),
            "additional_data": self.additional_data,
            "overall_score": self.overall_score,
            "is_profitable": self.is_profitable,
            "risk_level": self.risk_level,
        }
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Niche":
        """Create niche from dictionary.
        Args:
            data: Dictionary containing niche data
        Returns:
            Niche instance
        """
        # Handle datetime conversion
        if "analysis_date" in data and isinstance(data["analysis_date"], str):
            data["analysis_date"] = datetime.fromisoformat(data["analysis_date"])
        # Handle tuple conversion for price range
        if "recommended_price_range" in data and isinstance(data["recommended_price_range"], list):
            data["recommended_price_range"] = tuple(data["recommended_price_range"])
        # Remove computed properties
        computed_fields = {"overall_score", "is_profitable", "risk_level"}
        filtered_data = {k: v for k, v in data.items() if k not in computed_fields}
        return cls(**filtered_data)
    def to_json(self) -> str:
        """Convert niche to JSON string.
        Returns:
            JSON representation of the niche
        """
        return json.dumps(self.to_dict(), indent=2)
    @classmethod
    def from_json(cls, json_str: str) -> "Niche":
        """Create niche from JSON string.
        Args:
            json_str: JSON string containing niche data
        Returns:
            Niche instance
        """
        data = json.loads(json_str)
        return cls.from_dict(data)
    def __str__(self) -> str:
        """String representation of the niche."""
        return f"Niche(category='{self.category}', subcategory='{self.subcategory}', score={self.overall_score})"
    def __repr__(self) -> str:
        """Detailed string representation of the niche."""
        return (
            f"Niche(category='{self.category}', subcategory='{self.subcategory}', "
            f"keywords={len(self.keywords)}, profitability={self.profitability_score}, "
            f"competition={self.competition_score}, overall_score={self.overall_score})"
        )
</file>

<file path="src/kdp_strategist/models/trend_model.py">
"""Trend Analysis data model for KDP Strategist.
Defines the TrendAnalysis class and related data structures for representing
Google Trends analysis, forecasting, and seasonal pattern detection.
"""
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Tuple
from enum import Enum
from datetime import datetime, timedelta
import json
import statistics
class TrendDirection(Enum):
    """Enumeration of trend directions."""
    RISING = "rising"
    STABLE = "stable"
    DECLINING = "declining"
    VOLATILE = "volatile"
    SEASONAL = "seasonal"
class SeasonalPattern(Enum):
    """Enumeration of seasonal patterns."""
    NONE = "none"
    SPRING_PEAK = "spring_peak"
    SUMMER_PEAK = "summer_peak"
    FALL_PEAK = "fall_peak"
    WINTER_PEAK = "winter_peak"
    HOLIDAY_DRIVEN = "holiday_driven"
    BACK_TO_SCHOOL = "back_to_school"
    NEW_YEAR = "new_year"
    CUSTOM = "custom"
class TrendStrength(Enum):
    """Enumeration of trend strength levels."""
    VERY_WEAK = "very_weak"  # 0-20
    WEAK = "weak"  # 21-40
    MODERATE = "moderate"  # 41-60
    STRONG = "strong"  # 61-80
    VERY_STRONG = "very_strong"  # 81-100
@dataclass
class TrendAnalysis:
    """Represents comprehensive trend analysis for a keyword or topic.
    This class encapsulates Google Trends data analysis, seasonal patterns,
    forecasting, and confidence metrics for market trend assessment.
    Attributes:
        keyword: The primary keyword or phrase analyzed
        trend_score: Overall trend strength (0-100)
        trend_direction: Direction of the trend
        trend_strength: Categorical strength assessment
        regional_interest: Interest levels by geographic region
        related_queries: Related search queries from Google Trends
        seasonal_patterns: Detected seasonal patterns and factors
        forecast_6_months: 6-month forecast values
        confidence_level: Confidence in the analysis (0-100)
        analysis_period: Time period analyzed
        data_points: Number of data points used in analysis
        volatility_score: Measure of trend volatility (0-100)
        growth_rate: Monthly growth rate percentage
        peak_periods: Identified peak interest periods
        low_periods: Identified low interest periods
    """
    # Core trend data
    keyword: str
    trend_score: float  # 0-100
    trend_direction: str = TrendDirection.STABLE.value
    trend_strength: str = TrendStrength.MODERATE.value
    # Geographic and related data
    regional_interest: Dict[str, float] = field(default_factory=dict)
    related_queries: List[str] = field(default_factory=list)
    # Seasonal analysis
    seasonal_patterns: Dict[str, float] = field(default_factory=dict)
    seasonal_pattern_type: str = SeasonalPattern.NONE.value
    # Forecasting
    forecast_6_months: List[float] = field(default_factory=list)
    confidence_level: float = 0.0
    # Analysis metadata
    analysis_period: str = "12m"  # e.g., "12m", "5y", "today 3-m"
    data_points: int = 0
    volatility_score: float = 0.0
    growth_rate: float = 0.0  # Monthly percentage
    # Peak and trend analysis
    peak_periods: List[Dict[str, Any]] = field(default_factory=list)
    low_periods: List[Dict[str, Any]] = field(default_factory=list)
    trend_breakpoints: List[Dict[str, Any]] = field(default_factory=list)
    # Raw data (optional, for detailed analysis)
    raw_data: Optional[Dict[str, Any]] = None
    # Metadata
    analysis_date: datetime = field(default_factory=datetime.now)
    last_updated: datetime = field(default_factory=datetime.now)
    additional_data: Dict[str, Any] = field(default_factory=dict)
    def __post_init__(self):
        """Validate and process data after initialization."""
        self._validate_scores()
        self._validate_keyword()
        self._validate_forecast()
        self._determine_trend_strength()
        self.last_updated = datetime.now()
    def _validate_scores(self) -> None:
        """Validate that all scores are within valid ranges."""
        scores = {
            "trend_score": self.trend_score,
            "confidence_level": self.confidence_level,
            "volatility_score": self.volatility_score,
        }
        for score_name, score_value in scores.items():
            if not 0 <= score_value <= 100:
                raise ValueError(f"{score_name} must be between 0 and 100, got {score_value}")
    def _validate_keyword(self) -> None:
        """Validate keyword."""
        if not self.keyword or not self.keyword.strip():
            raise ValueError("Keyword cannot be empty")
    def _validate_forecast(self) -> None:
        """Validate forecast data."""
        if self.forecast_6_months:
            if len(self.forecast_6_months) != 6:
                raise ValueError("Forecast must contain exactly 6 monthly values")
            for value in self.forecast_6_months:
                if not 0 <= value <= 100:
                    raise ValueError("Forecast values must be between 0 and 100")
    def _determine_trend_strength(self) -> None:
        """Determine categorical trend strength based on score."""
        if self.trend_score <= 20:
            self.trend_strength = TrendStrength.VERY_WEAK.value
        elif self.trend_score <= 40:
            self.trend_strength = TrendStrength.WEAK.value
        elif self.trend_score <= 60:
            self.trend_strength = TrendStrength.MODERATE.value
        elif self.trend_score <= 80:
            self.trend_strength = TrendStrength.STRONG.value
        else:
            self.trend_strength = TrendStrength.VERY_STRONG.value
    @property
    def is_trending_up(self) -> bool:
        """Check if trend is moving upward.
        Returns:
            True if trend direction is rising
        """
        return self.trend_direction == TrendDirection.RISING.value
    @property
    def is_seasonal(self) -> bool:
        """Check if trend shows seasonal patterns.
        Returns:
            True if seasonal patterns are detected
        """
        return (
            self.seasonal_pattern_type != SeasonalPattern.NONE.value or
            bool(self.seasonal_patterns)
        )
    @property
    def is_reliable(self) -> bool:
        """Check if trend analysis is reliable.
        Returns:
            True if confidence level and data quality are sufficient
        """
        return (
            self.confidence_level >= 70 and
            self.data_points >= 12 and  # At least 12 data points
            self.volatility_score <= 60  # Not too volatile
        )
    @property
    def risk_assessment(self) -> str:
        """Assess investment risk based on trend characteristics.
        Returns:
            Risk level: 'low', 'medium', 'high', or 'very_high'
        """
        risk_factors = 0
        # High volatility increases risk
        if self.volatility_score > 70:
            risk_factors += 2
        elif self.volatility_score > 50:
            risk_factors += 1
        # Declining trends increase risk
        if self.trend_direction == TrendDirection.DECLINING.value:
            risk_factors += 2
        elif self.trend_direction == TrendDirection.VOLATILE.value:
            risk_factors += 1
        # Low confidence increases risk
        if self.confidence_level < 50:
            risk_factors += 2
        elif self.confidence_level < 70:
            risk_factors += 1
        # Low trend score increases risk
        if self.trend_score < 30:
            risk_factors += 1
        if risk_factors >= 5:
            return "very_high"
        elif risk_factors >= 3:
            return "high"
        elif risk_factors >= 1:
            return "medium"
        else:
            return "low"
    @property
    def opportunity_score(self) -> float:
        """Calculate overall opportunity score.
        Returns:
            Weighted opportunity score (0-100)
        """
        # Base score from trend strength
        base_score = self.trend_score * 0.4
        # Bonus for rising trends
        direction_bonus = 0
        if self.trend_direction == TrendDirection.RISING.value:
            direction_bonus = 20
        elif self.trend_direction == TrendDirection.STABLE.value:
            direction_bonus = 10
        # Confidence factor
        confidence_factor = self.confidence_level * 0.3
        # Volatility penalty
        volatility_penalty = self.volatility_score * 0.1
        opportunity = base_score + direction_bonus + confidence_factor - volatility_penalty
        return round(max(0, min(100, opportunity)), 1)
    def add_peak_period(self, start_date: datetime, end_date: datetime, peak_value: float, description: str = "") -> None:
        """Add a peak period to the analysis.
        Args:
            start_date: Start of peak period
            end_date: End of peak period
            peak_value: Peak interest value
            description: Optional description of the peak
        """
        peak_info = {
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "peak_value": peak_value,
            "description": description,
            "duration_days": (end_date - start_date).days,
        }
        self.peak_periods.append(peak_info)
    def add_low_period(self, start_date: datetime, end_date: datetime, low_value: float, description: str = "") -> None:
        """Add a low period to the analysis.
        Args:
            start_date: Start of low period
            end_date: End of low period
            low_value: Low interest value
            description: Optional description of the low period
        """
        low_info = {
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "low_value": low_value,
            "description": description,
            "duration_days": (end_date - start_date).days,
        }
        self.low_periods.append(low_info)
    def set_seasonal_factor(self, month: str, factor: float) -> None:
        """Set seasonal factor for a specific month.
        Args:
            month: Month name (e.g., 'January', 'February')
            factor: Seasonal multiplier (1.0 = normal, >1.0 = higher interest)
        """
        if not 0 <= factor <= 5.0:
            raise ValueError("Seasonal factor must be between 0 and 5.0")
        self.seasonal_patterns[month] = factor
    def get_forecast_trend(self) -> str:
        """Analyze forecast trend direction.
        Returns:
            Forecast trend: 'improving', 'stable', 'declining'
        """
        if not self.forecast_6_months or len(self.forecast_6_months) < 2:
            return "unknown"
        # Calculate trend from first to last forecast value
        first_half = statistics.mean(self.forecast_6_months[:3])
        second_half = statistics.mean(self.forecast_6_months[3:])
        change_percent = ((second_half - first_half) / first_half) * 100 if first_half > 0 else 0
        if change_percent > 5:
            return "improving"
        elif change_percent < -5:
            return "declining"
        else:
            return "stable"
    def get_best_months(self, limit: int = 3) -> List[Tuple[str, float]]:
        """Get months with highest seasonal interest.
        Args:
            limit: Maximum number of months to return
        Returns:
            List of (month, factor) tuples sorted by factor
        """
        if not self.seasonal_patterns:
            return []
        sorted_months = sorted(
            self.seasonal_patterns.items(),
            key=lambda x: x[1],
            reverse=True
        )
        return sorted_months[:limit]
    def get_worst_months(self, limit: int = 3) -> List[Tuple[str, float]]:
        """Get months with lowest seasonal interest.
        Args:
            limit: Maximum number of months to return
        Returns:
            List of (month, factor) tuples sorted by factor (ascending)
        """
        if not self.seasonal_patterns:
            return []
        sorted_months = sorted(
            self.seasonal_patterns.items(),
            key=lambda x: x[1]
        )
        return sorted_months[:limit]
    def to_dict(self) -> Dict[str, Any]:
        """Convert trend analysis to dictionary for serialization.
        Returns:
            Dictionary representation of the trend analysis
        """
        return {
            "keyword": self.keyword,
            "trend_score": self.trend_score,
            "trend_direction": self.trend_direction,
            "trend_strength": self.trend_strength,
            "regional_interest": self.regional_interest,
            "related_queries": self.related_queries,
            "seasonal_patterns": self.seasonal_patterns,
            "seasonal_pattern_type": self.seasonal_pattern_type,
            "forecast_6_months": self.forecast_6_months,
            "confidence_level": self.confidence_level,
            "analysis_period": self.analysis_period,
            "data_points": self.data_points,
            "volatility_score": self.volatility_score,
            "growth_rate": self.growth_rate,
            "peak_periods": self.peak_periods,
            "low_periods": self.low_periods,
            "trend_breakpoints": self.trend_breakpoints,
            "analysis_date": self.analysis_date.isoformat(),
            "last_updated": self.last_updated.isoformat(),
            "additional_data": self.additional_data,
            "is_trending_up": self.is_trending_up,
            "is_seasonal": self.is_seasonal,
            "is_reliable": self.is_reliable,
            "risk_assessment": self.risk_assessment,
            "opportunity_score": self.opportunity_score,
            "forecast_trend": self.get_forecast_trend(),
            "best_months": self.get_best_months(),
            "worst_months": self.get_worst_months(),
        }
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "TrendAnalysis":
        """Create trend analysis from dictionary.
        Args:
            data: Dictionary containing trend analysis data
        Returns:
            TrendAnalysis instance
        """
        # Handle datetime conversion
        for date_field in ["analysis_date", "last_updated"]:
            if date_field in data and isinstance(data[date_field], str):
                data[date_field] = datetime.fromisoformat(data[date_field])
        # Remove computed properties
        computed_fields = {
            "is_trending_up", "is_seasonal", "is_reliable", "risk_assessment",
            "opportunity_score", "forecast_trend", "best_months", "worst_months"
        }
        filtered_data = {k: v for k, v in data.items() if k not in computed_fields}
        return cls(**filtered_data)
    def to_json(self) -> str:
        """Convert trend analysis to JSON string.
        Returns:
            JSON representation of the trend analysis
        """
        return json.dumps(self.to_dict(), indent=2)
    @classmethod
    def from_json(cls, json_str: str) -> "TrendAnalysis":
        """Create trend analysis from JSON string.
        Args:
            json_str: JSON string containing trend analysis data
        Returns:
            TrendAnalysis instance
        """
        data = json.loads(json_str)
        return cls.from_dict(data)
    def __str__(self) -> str:
        """String representation of the trend analysis."""
        return f"TrendAnalysis(keyword='{self.keyword}', score={self.trend_score}, direction='{self.trend_direction}')"
    def __repr__(self) -> str:
        """Detailed string representation of the trend analysis."""
        return (
            f"TrendAnalysis(keyword='{self.keyword}', score={self.trend_score}, "
            f"direction='{self.trend_direction}', confidence={self.confidence_level}, "
            f"opportunity={self.opportunity_score})"
        )
</file>

<file path="start_frontend.py">
#!/usr/bin/env python3
"""
KDP Strategist Frontend Development Server Launcher
This script starts the React development server for the KDP Strategist web UI.
It handles npm/yarn detection, dependency installation, and server startup.
"""
import os
import sys
import subprocess
import shutil
from pathlib import Path
def check_node_installed():
    """Check if Node.js is installed."""
    try:
        result = subprocess.run(['node', '-v'], capture_output=True, text=True, shell=True)
        if result.returncode == 0:
            version = result.stdout.strip()
            print(f"‚úÖ Node.js found: {version}")
            return True
    except FileNotFoundError:
        pass
    print("‚ùå Node.js not found. Please install Node.js from https://nodejs.org/")
    return False
def check_package_manager():
    """Check which package manager is available (npm or yarn)."""
    # Check for yarn first (often faster)
    if shutil.which('yarn'):
        try:
            result = subprocess.run(['yarn', '-v'], capture_output=True, text=True, shell=True)
            if result.returncode == 0:
                version = result.stdout.strip()
                print(f"‚úÖ Yarn found: {version}")
                return 'yarn'
        except FileNotFoundError:
            pass
    # Check for npm - try multiple approaches for Windows compatibility
    npm_commands = ['npm', 'npm.cmd', 'npm.exe']
    for npm_cmd in npm_commands:
        if shutil.which(npm_cmd):
            try:
                result = subprocess.run([npm_cmd, '-v'], capture_output=True, text=True, shell=True)
                if result.returncode == 0:
                    version = result.stdout.strip()
                    print(f"‚úÖ npm found: {version}")
                    return 'npm'
            except FileNotFoundError:
                continue
    # Fallback: try running npm directly without shutil.which check
    try:
        result = subprocess.run(['npm', '-v'], capture_output=True, text=True, shell=True)
        if result.returncode == 0:
            version = result.stdout.strip()
            print(f"‚úÖ npm found: {version}")
            return 'npm'
    except FileNotFoundError:
        pass
    print("‚ùå No package manager found. Please install npm or yarn.")
    return None
def install_dependencies(package_manager, frontend_dir):
    """Install frontend dependencies."""
    print(f"üì¶ Installing dependencies with {package_manager}...")
    try:
        if package_manager == 'yarn':
            cmd = ['yarn', 'install']
        else:
            cmd = ['npm', 'install']
        result = subprocess.run(
            cmd,
            cwd=frontend_dir,
            check=True,
            capture_output=False,
            shell=True
        )
        print("‚úÖ Dependencies installed successfully!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Failed to install dependencies: {e}")
        return False
def check_dependencies_installed(frontend_dir):
    """Check if node_modules exists and has content."""
    node_modules = frontend_dir / 'node_modules'
    return node_modules.exists() and any(node_modules.iterdir())
def start_dev_server(package_manager, frontend_dir):
    """Start the React development server."""
    print(f"üöÄ Starting React development server with {package_manager}...")
    try:
        if package_manager == 'yarn':
            cmd = ['yarn', 'start']
        else:
            cmd = ['npm', 'start']
        # Set environment variables
        env = os.environ.copy()
        env['REACT_APP_API_URL'] = 'http://localhost:8000'
        env['BROWSER'] = 'none'  # Don't auto-open browser
        print("\n" + "="*60)
        print("üåê KDP Strategist Frontend Development Server")
        print("="*60)
        print("üìç Frontend URL: http://localhost:3000")
        print("üîó API Backend: http://localhost:8000")
        print("üìñ Make sure the backend server is running!")
        print("="*60)
        print("Press Ctrl+C to stop the server")
        print("="*60 + "\n")
        # Start the server
        subprocess.run(
            cmd,
            cwd=frontend_dir,
            env=env,
            check=True,
            shell=True
        )
    except KeyboardInterrupt:
        print("\nüõë Development server stopped by user")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Failed to start development server: {e}")
        return False
    return True
def create_env_file(frontend_dir):
    """Create .env file for React app if it doesn't exist."""
    env_file = frontend_dir / '.env'
    if not env_file.exists():
        print("üìù Creating .env file...")
        env_content = """# KDP Strategist Frontend Environment Variables
# API Backend URL
REACT_APP_API_URL=http://localhost:8000
# App Configuration
REACT_APP_NAME=KDP Strategist
REACT_APP_VERSION=1.0.0
# Development Settings
GENERATE_SOURCEMAP=true
REACT_APP_ENV=development
# Disable automatic browser opening
BROWSER=none
# Port configuration (optional)
# PORT=3000
"""
        with open(env_file, 'w') as f:
            f.write(env_content)
        print("‚úÖ .env file created successfully!")
def main():
    """Main entry point."""
    print("üéØ KDP Strategist Frontend Launcher")
    print("="*40)
    # Get frontend directory
    frontend_dir = Path(__file__).parent / 'frontend'
    if not frontend_dir.exists():
        print(f"‚ùå Frontend directory not found: {frontend_dir}")
        sys.exit(1)
    # Check Node.js
    if not check_node_installed():
        sys.exit(1)
    # Check package manager
    package_manager = check_package_manager()
    if not package_manager:
        sys.exit(1)
    # Create .env file
    create_env_file(frontend_dir)
    # Check if dependencies are installed
    if not check_dependencies_installed(frontend_dir):
        print("üì¶ Dependencies not found. Installing...")
        if not install_dependencies(package_manager, frontend_dir):
            sys.exit(1)
    else:
        print("‚úÖ Dependencies already installed")
    # Start development server
    if not start_dev_server(package_manager, frontend_dir):
        sys.exit(1)
if __name__ == "__main__":
    main()
</file>

<file path="start_kdp_strategist.py">
#!/usr/bin/env python3
"""
KDP Strategist Full Stack Launcher
This script provides options to start the KDP Strategist application:
1. Backend only (FastAPI server)
2. Frontend only (React development server)
3. Full stack (both backend and frontend)
4. Production mode
"""
import os
import sys
import time
import signal
import argparse
import subprocess
import threading
from pathlib import Path
from typing import List, Optional
class KDPStrategistLauncher:
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.frontend_dir = self.project_root / 'frontend'
        self.processes: List[subprocess.Popen] = []
        self.shutdown_event = threading.Event()
        # Setup signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals."""
        print("\nüõë Shutting down KDP Strategist...")
        self.shutdown_event.set()
        self._cleanup_processes()
        sys.exit(0)
    def _cleanup_processes(self):
        """Clean up all running processes."""
        for process in self.processes:
            if process.poll() is None:  # Process is still running
                try:
                    process.terminate()
                    process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    process.kill()
                except Exception as e:
                    print(f"Error terminating process: {e}")
        self.processes.clear()
    def check_dependencies(self) -> bool:
        """Check if all required dependencies are available."""
        print("üîç Checking dependencies...")
        # Check Python dependencies
        try:
            import fastapi
            import uvicorn
            print("‚úÖ FastAPI dependencies found")
        except ImportError:
            print("‚ùå FastAPI dependencies missing. Run: pip install -r requirements.txt")
            return False
        # Check Node.js for frontend
        try:
            result = subprocess.run(['node', '--version'], capture_output=True, text=True)
            if result.returncode == 0:
                print(f"‚úÖ Node.js found: {result.stdout.strip()}")
            else:
                print("‚ùå Node.js not found")
                return False
        except FileNotFoundError:
            print("‚ùå Node.js not found. Install from https://nodejs.org/")
            return False
        return True
    def start_backend(self, port: int = 8000, reload: bool = True) -> Optional[subprocess.Popen]:
        """Start the FastAPI backend server."""
        print(f"üöÄ Starting backend server on port {port}...")
        try:
            env = os.environ.copy()
            env['PORT'] = str(port)
            env['RELOAD'] = 'true' if reload else 'false'
            process = subprocess.Popen(
                [sys.executable, 'run_server.py'],
                cwd=self.project_root,
                env=env,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            self.processes.append(process)
            # Wait a moment to check if the process started successfully
            time.sleep(2)
            if process.poll() is not None:
                print("‚ùå Backend server failed to start")
                return None
            print(f"‚úÖ Backend server started (PID: {process.pid})")
            return process
        except Exception as e:
            print(f"‚ùå Failed to start backend: {e}")
            return None
    def start_frontend(self, api_url: str = "http://localhost:8000") -> Optional[subprocess.Popen]:
        """Start the React frontend development server."""
        print("üåê Starting frontend development server...")
        if not self.frontend_dir.exists():
            print(f"‚ùå Frontend directory not found: {self.frontend_dir}")
            return None
        try:
            # Create .env file
            self._create_frontend_env(api_url)
            # Check if dependencies are installed
            if not self._check_frontend_dependencies():
                if not self._install_frontend_dependencies():
                    return None
            # Determine package manager
            package_manager = self._get_package_manager()
            if not package_manager:
                return None
            # Start the development server
            env = os.environ.copy()
            env['REACT_APP_API_URL'] = api_url
            env['BROWSER'] = 'none'
            cmd = ['yarn', 'start'] if package_manager == 'yarn' else ['npm', 'start']
            process = subprocess.Popen(
                cmd,
                cwd=self.frontend_dir,
                env=env,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            self.processes.append(process)
            # Wait for the server to start
            time.sleep(5)
            if process.poll() is not None:
                print("‚ùå Frontend server failed to start")
                return None
            print(f"‚úÖ Frontend server started (PID: {process.pid})")
            return process
        except Exception as e:
            print(f"‚ùå Failed to start frontend: {e}")
            return None
    def _create_frontend_env(self, api_url: str):
        """Create .env file for React app."""
        env_file = self.frontend_dir / '.env'
        env_content = f"""# KDP Strategist Frontend Environment Variables
REACT_APP_API_URL={api_url}
REACT_APP_NAME=KDP Strategist
REACT_APP_VERSION=1.0.0
GENERATE_SOURCEMAP=true
REACT_APP_ENV=development
BROWSER=none
"""
        with open(env_file, 'w') as f:
            f.write(env_content)
    def _check_frontend_dependencies(self) -> bool:
        """Check if frontend dependencies are installed."""
        node_modules = self.frontend_dir / 'node_modules'
        return node_modules.exists() and any(node_modules.iterdir())
    def _install_frontend_dependencies(self) -> bool:
        """Install frontend dependencies."""
        print("üì¶ Installing frontend dependencies...")
        package_manager = self._get_package_manager()
        if not package_manager:
            return False
        try:
            cmd = ['yarn', 'install'] if package_manager == 'yarn' else ['npm', 'install']
            result = subprocess.run(
                cmd,
                cwd=self.frontend_dir,
                check=True,
                capture_output=True,
                text=True
            )
            print("‚úÖ Frontend dependencies installed")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to install frontend dependencies: {e}")
            return False
    def _get_package_manager(self) -> Optional[str]:
        """Get available package manager."""
        try:
            subprocess.run(['yarn', '--version'], capture_output=True, check=True)
            return 'yarn'
        except (FileNotFoundError, subprocess.CalledProcessError):
            pass
        try:
            subprocess.run(['npm', '--version'], capture_output=True, check=True)
            return 'npm'
        except (FileNotFoundError, subprocess.CalledProcessError):
            print("‚ùå No package manager found (npm or yarn required)")
            return None
    def print_status(self, backend_port: int = 8000, frontend_port: int = 3000):
        """Print application status and URLs."""
        print("\n" + "="*70)
        print("üéØ KDP Strategist Application Status")
        print("="*70)
        print(f"üîó Frontend URL: http://localhost:{frontend_port}")
        print(f"üîó Backend API: http://localhost:{backend_port}")
        print(f"üìñ API Documentation: http://localhost:{backend_port}/docs")
        print(f"üîß Interactive API: http://localhost:{backend_port}/redoc")
        print("="*70)
        print("Press Ctrl+C to stop all servers")
        print("="*70 + "\n")
    def monitor_processes(self):
        """Monitor running processes and restart if needed."""
        while not self.shutdown_event.is_set():
            for i, process in enumerate(self.processes[:]):
                if process.poll() is not None:
                    print(f"‚ö†Ô∏è Process {process.pid} has stopped")
                    self.processes.remove(process)
            time.sleep(5)
    def run_fullstack(self, backend_port: int = 8000, frontend_port: int = 3000):
        """Run both backend and frontend servers."""
        print("üöÄ Starting KDP Strategist Full Stack...")
        if not self.check_dependencies():
            return False
        # Start backend
        backend_process = self.start_backend(port=backend_port)
        if not backend_process:
            return False
        # Wait for backend to be ready
        time.sleep(3)
        # Start frontend
        frontend_process = self.start_frontend(f"http://localhost:{backend_port}")
        if not frontend_process:
            self._cleanup_processes()
            return False
        # Print status
        self.print_status(backend_port, frontend_port)
        # Monitor processes
        try:
            self.monitor_processes()
        except KeyboardInterrupt:
            pass
        return True
def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='KDP Strategist Application Launcher')
    parser.add_argument(
        'mode',
        choices=['backend', 'frontend', 'fullstack', 'full'],
        help='Launch mode: backend only, frontend only, or full stack'
    )
    parser.add_argument(
        '--backend-port',
        type=int,
        default=8000,
        help='Backend server port (default: 8000)'
    )
    parser.add_argument(
        '--frontend-port',
        type=int,
        default=3000,
        help='Frontend server port (default: 3000)'
    )
    parser.add_argument(
        '--no-reload',
        action='store_true',
        help='Disable auto-reload for backend server'
    )
    args = parser.parse_args()
    launcher = KDPStrategistLauncher()
    try:
        if args.mode == 'backend':
            if not launcher.check_dependencies():
                sys.exit(1)
            process = launcher.start_backend(
                port=args.backend_port,
                reload=not args.no_reload
            )
            if process:
                print(f"üîó Backend API: http://localhost:{args.backend_port}")
                print(f"üìñ API Docs: http://localhost:{args.backend_port}/docs")
                print("Press Ctrl+C to stop")
                process.wait()
            else:
                sys.exit(1)
        elif args.mode == 'frontend':
            process = launcher.start_frontend(f"http://localhost:{args.backend_port}")
            if process:
                print(f"üîó Frontend: http://localhost:{args.frontend_port}")
                print("Press Ctrl+C to stop")
                process.wait()
            else:
                sys.exit(1)
        elif args.mode in ['fullstack', 'full']:
            success = launcher.run_fullstack(
                backend_port=args.backend_port,
                frontend_port=args.frontend_port
            )
            if not success:
                sys.exit(1)
    except KeyboardInterrupt:
        print("\nüõë Shutting down...")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)
    finally:
        launcher._cleanup_processes()
if __name__ == "__main__":
    main()
</file>

<file path="requirements.txt">
# Core MCP and Agent Framework
mcp>=0.1.0
langchain>=0.1.0
langchain-community>=0.0.1

# FastAPI and Web Server
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
python-multipart>=0.0.6
websockets>=12.0

# Data Analysis and Processing
pandas>=2.0.0
numpy>=1.24.0
scipy==1.16.0

# External API Clients
keepa>=1.3.0
pytrends>=4.9.0
requests>=2.31.0
aiohttp>=3.8.0

# Data Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.15.0

# Machine Learning and Embeddings
scikit-learn>=1.3.0
transformers>=4.30.0
sentence-transformers>=2.2.0

# Caching and Storage
redis>=4.5.0

# Configuration and Environment
python-dotenv>=1.0.0
pydantic>=2.0.0
pydantic-settings>=2.1.0
PyYAML>=6.0
npm

# Logging and Monitoring
structlog>=23.1.0
rich>=13.4.0

# Testing and Development
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0
black>=23.7.0
flake8>=6.0.0
mypy>=1.5.0

# Utilities
click>=8.1.0
tqdm>=4.65.0
python-dateutil>=2.8.0
fuzzy>=1.2.2

# Export and File Handling
openpyxl>=3.1.0
reportlab>=4.0.0
orjson>=3.9.0

# Additional FastAPI Dependencies
httpx>=0.25.0
cachetools>=5.3.0
typing-extensions>=4.8.0
</file>

<file path="src/kdp_strategist/agent/__init__.py">
"""KDP Strategist AI Agent - MCP Integration.
This module provides the MCP (Model Context Protocol) agent implementation
for the KDP Strategist, enabling AI-powered publishing strategy analysis.
Core Components:
- KDPStrategistAgent: Main MCP agent class
- Tool implementations for niche discovery, competitor analysis, etc.
- Integration with external APIs (Keepa, Google Trends)
- Caching and performance optimization
"""
from .kdp_strategist_agent import KDPStrategistAgent
from .tools import (
    find_profitable_niches,
    analyze_competitor_asin,
    generate_kdp_listing,
    validate_trend,
    niche_stress_test
)
__all__ = [
    "KDPStrategistAgent",
    "find_profitable_niches",
    "analyze_competitor_asin", 
    "generate_kdp_listing",
    "validate_trend",
    "niche_stress_test"
]
</file>

</files>
